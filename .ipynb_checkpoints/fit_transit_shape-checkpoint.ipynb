{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit transit shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize as op\n",
    "import scipy.signal as sig\n",
    "from   scipy import stats\n",
    "from   scipy import fftpack\n",
    "from   scipy import ndimage\n",
    "import astropy\n",
    "from   astropy.io import fits as pyfits\n",
    "from   sklearn.cluster import KMeans\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import importlib as imp\n",
    "import glob\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "import progressbar\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import lightkurve as lk\n",
    "import exoplanet as exo\n",
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "import corner\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import *\n",
    "from alderaan.Planet import *\n",
    "from alderaan.LiteCurve import *\n",
    "import alderaan.io as io\n",
    "import alderaan.detrend as detrend\n",
    "import alderaan.noise as noise\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually set I/O parameters\n",
    "#### User should manually set MISSION, TARGET, PRIMARY_DIR,  and CSV_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Simulated\"\n",
    "TARGET  = \"S03260\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "\n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\"\n",
    "    \n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog_eccentric.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where we parse the inputs\n",
    "try:\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for ALDERAAN transit fiting pipeline\")\n",
    "    parser.add_argument(\"--mission\", default=None, type=str, required=True, \\\n",
    "                        help=\"Mission name\")\n",
    "    parser.add_argument(\"--target\", default=None, type=str, required=True, \\\n",
    "                        help=\"Target name; see ALDERAAN documentation for acceptable formats\")\n",
    "    parser.add_argument(\"--primary_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"Primary directory path for accessing lightcurve data and saving outputs\")\n",
    "    parser.add_argument(\"--csv_file\", default=None, type=str, required=True, \\\n",
    "                        help=\"Path to .csv file containing input planetary parameters\")\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    MISSION     = args.mission\n",
    "    TARGET      = args.target\n",
    "    PRIMARY_DIR = args.primary_dir\n",
    "    CSV_FILE    = args.csv_file\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs\n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in planet and stellar parameters from Kepler DR25 & Gaia DR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = TARGET\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "RSTAR = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "RSTAR_ERR1 = np.array(target_dict['rstar_err1'],  dtype='float')[use]\n",
    "RSTAR_ERR2 = np.array(target_dict['rstar_err2'],  dtype='float')[use]\n",
    "\n",
    "U1 = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "U2 = np.array(target_dict['limbdark_2'], dtype='float')[use]\n",
    "\n",
    "PERIODS = np.array(target_dict['period'], dtype='float')[use]\n",
    "EPOCHS  = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "DEPTHS  = np.array(target_dict['depth'], dtype='float')[use]*1e-6          # [ppm] --> []\n",
    "DURS    = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR[0] for r in RSTAR): RSTAR = RSTAR[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == RSTAR_ERR1[0] for r in RSTAR_ERR1): RSTAR_ERR1 = RSTAR_ERR1[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR_ERR1 in the csv input file')\n",
    "    \n",
    "if all(r == RSTAR_ERR2[0] for r in RSTAR_ERR2): RSTAR_ERR2 = RSTAR_ERR2[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR_ERR2 in the csv input file')  \n",
    "    \n",
    "if all(u == U1[0] for u in U1): U1 = U1[0]\n",
    "else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "if all(u == U2[0] for u in U2): U2 = U2[0]\n",
    "else: raise ValueError('There are inconsistencies with U2 in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort planet parameters by period\n",
    "order = np.argsort(PERIODS)\n",
    "\n",
    "PERIODS = PERIODS[order]\n",
    "EPOCHS  = EPOCHS[order]\n",
    "DEPTHS  = DEPTHS[order]\n",
    "DURS    = DURS[order]\n",
    "\n",
    "# initialize and radius arrays\n",
    "RADII = np.sqrt(DEPTHS)*RSTAR\n",
    "IMPACTS = 0.5*np.ones(NPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combline stellar radius uncertainties\n",
    "RSTAR_ERR = np.sqrt(RSTAR_ERR1**2 + RSTAR_ERR2**2)/np.sqrt(2)\n",
    "\n",
    "# Kipping 2013 limb darkening parameterization\n",
    "Q1 = (U1 + U2)**2\n",
    "Q2 = 0.5*U1/(U1+U2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in detrended lightcurves and initial transit time estimates\n",
    "#### The data can be generated by running the script \"detrend_and_estimate_noise.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detrended lightcurves\n",
    "try:\n",
    "    lc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_lc_detrended.fits')\n",
    "except:\n",
    "    lc = None\n",
    "    \n",
    "try:\n",
    "    sc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_sc_detrended.fits')\n",
    "except:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in QuickTTV estimates and calculate linear ephemeris for each planet\n",
    "EPOCHS  = np.zeros(NPL)\n",
    "PERIODS = np.zeros(NPL)\n",
    "\n",
    "transit_inds = []\n",
    "indep_transit_times = []\n",
    "indep_ephemeris = []\n",
    "\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # read in predetermined transit times\n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_quick_ttvs.txt'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    transit_inds.append(data_in[:,0])\n",
    "    indep_transit_times.append(data_in[:,1])\n",
    "    \n",
    "    # do a quick fit to get a linear ephemeris\n",
    "    pfit = np.polyfit(transit_inds[npl], indep_transit_times[npl], 1)\n",
    "    \n",
    "    indep_ephemeris.append(np.polyval(pfit, transit_inds[npl]))\n",
    "    \n",
    "    EPOCHS[npl] = pfit[1]\n",
    "    PERIODS[npl] = pfit[0]\n",
    "    \n",
    "    \n",
    "# make sure transit_inds are zero-indexed\n",
    "for npl in range(NPL):\n",
    "    transit_inds[npl] = np.array(transit_inds[npl] - transit_inds[npl][0], dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine scatter relative to linear ephemeris\n",
    "# this is a deliberate overestimate of the true scatter\n",
    "omc_scatter = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = indep_ephemeris[npl]\n",
    "    yomc  = indep_transit_times[npl] - indep_ephemeris[npl]\n",
    "    \n",
    "    omc_scatter[npl] = np.std(yomc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_ephemeris = np.copy(indep_ephemeris)\n",
    "fixed_transit_times = np.copy(indep_transit_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set time baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the time baseline\n",
    "time_min = []\n",
    "time_max = []\n",
    "\n",
    "try:\n",
    "    time_min.append(sc.time.min())\n",
    "    time_max.append(sc.time.max()) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    time_min.append(lc.time.min())\n",
    "    time_max.append(lc.time.max())     \n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "TIME_START = np.min(time_min)\n",
    "TIME_END   = np.max(time_max)\n",
    "\n",
    "# put epochs in range (TIME_START, TIME_START + PERIOD)\n",
    "for npl in range(NPL):\n",
    "    if EPOCHS[npl] < TIME_START:\n",
    "        adj = 1 + (TIME_START - EPOCHS[npl])//PERIODS[npl]\n",
    "        EPOCHS[npl] += adj*PERIODS[npl]        \n",
    "        \n",
    "    if EPOCHS[npl] > (TIME_START + PERIODS[npl]):\n",
    "        adj = (EPOCHS[npl] - TIME_START)//PERIODS[npl]\n",
    "        EPOCHS[npl] -= adj*PERIODS[npl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up noise GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in noise model GP priors\n",
    "gp_percs = []\n",
    "\n",
    "for z in range(4):\n",
    "    try:\n",
    "        fname_in = NOISE_DIR + TARGET + '_shoterm_gp_priors_{0}.txt'.format(z)\n",
    "\n",
    "        with open(fname_in) as infile:\n",
    "            gp_percs.append(json.load(infile))\n",
    "\n",
    "    except:\n",
    "        gp_percs.append(None)\n",
    "\n",
    "# Read in quarter-by-quarter variances\n",
    "var_by_quarter = np.genfromtxt(NOISE_DIR + TARGET + '_var_by_quarter.txt')[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_priors = []\n",
    "\n",
    "for z in range(4):\n",
    "    if gp_percs[z] is not None:\n",
    "        # set GP priors baed on outputs of alderaan.detrend_and_estimate_noise\n",
    "        # expected for any season with short cadence data\n",
    "        gpz = {}\n",
    "\n",
    "        for k in gp_percs[z].keys():\n",
    "            if k != \"percentiles\":\n",
    "                perc = np.array(gp_percs[z]['percentiles'])\n",
    "\n",
    "                med = np.array(gp_percs[z][k])[perc == 50.0][0]\n",
    "                err1 = np.array(gp_percs[z][k])[perc == 84.1][0]\n",
    "                err2 = np.array(gp_percs[z][k])[perc == 15.9][0]\n",
    "\n",
    "                dev = np.sqrt((err1-med)**2/2 + (err2-med)**2/2)\n",
    "\n",
    "                gpz[k] = (med, dev)\n",
    "\n",
    "        gp_priors.append(gpz)\n",
    "        \n",
    "    else:\n",
    "        # these are dummy values that effectively create a zero-amplitude kernel\n",
    "        gpz = {}\n",
    "        gpz['logw0'] = [np.log(2*pi/(7*DURS.max()))]\n",
    "        gpz['logSw4'] = [-100.]\n",
    "        gpz['logQ'] = [np.log(1/np.sqrt(2))]\n",
    "        \n",
    "        gp_priors.append(gpz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(4):\n",
    "    gpz = gp_priors[z]\n",
    "    \n",
    "    logS = gpz[\"logSw4\"][0] - 4*gpz[\"logw0\"][0]\n",
    "    \n",
    "    if len(gpz[\"logSw4\"]) == 1:\n",
    "        gp_priors[z][\"logS\"] = np.copy(logS)\n",
    "        \n",
    "    if len(gpz[\"logSw4\"]) == 2:\n",
    "        logS_var = gpz[\"logSw4\"][1]**2 + 16*gpz[\"logw0\"][1]**2\n",
    "        gp_priors[z][\"logS\"] = np.array([logS, np.sqrt(logS_var)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the relevant data and starting transit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data near transits for each quarter\n",
    "all_time = []\n",
    "all_flux = []\n",
    "all_error = []\n",
    "all_dtype = []\n",
    "\n",
    "lc_flux = []\n",
    "sc_flux = []\n",
    "\n",
    "if sc is not None:\n",
    "    for q in range(18):\n",
    "        if np.isin(q, sc.quarter)*np.isin(q, lc.quarter):\n",
    "            raise ValueError(\"Double counting data in both short and long cadence\")\n",
    "\n",
    "\n",
    "        elif np.isin(q, sc.quarter):\n",
    "            use = (sc.mask.sum(axis=0) > 0)*(sc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 45:\n",
    "                all_time.append(sc.time[use])\n",
    "                all_flux.append(sc.flux[use])\n",
    "                all_error.append(sc.error[use])\n",
    "                all_dtype.append('short')\n",
    "\n",
    "                sc_flux.append(sc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        elif np.isin(q, lc.quarter):\n",
    "            use = (lc.mask.sum(axis=0) > 0)*(lc.quarter == q)\n",
    "            \n",
    "            if np.sum(use) > 3:\n",
    "                all_time.append(lc.time[use])\n",
    "                all_flux.append(lc.flux[use])\n",
    "                all_error.append(lc.error[use])\n",
    "                all_dtype.append('long')\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_time.append(None)\n",
    "            all_flux.append(None)\n",
    "            all_error.append(None)\n",
    "            all_dtype.append('none')\n",
    "            \n",
    "else:\n",
    "    for q in range(18):\n",
    "        if np.isin(q, lc.quarter):\n",
    "            use = (lc.mask.sum(axis=0) > 0)*(lc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 3:\n",
    "                all_time.append(lc.time[use])\n",
    "                all_flux.append(lc.flux[use])\n",
    "                all_error.append(lc.error[use])\n",
    "                all_dtype.append('long')\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_time.append(None)\n",
    "            all_flux.append(None)\n",
    "            all_error.append(None)\n",
    "            all_dtype.append('none')\n",
    "\n",
    "\n",
    "\n",
    "# check which quarters have data\n",
    "good = (np.array(all_dtype) == 'short') + (np.array(all_dtype) == 'long')\n",
    "quarters = np.arange(18)[good]\n",
    "nq = len(quarters)\n",
    "\n",
    "seasons = np.sort(np.unique(quarters % 4))\n",
    "\n",
    "lc_quarters = np.arange(18)[np.array(all_dtype) == 'long']\n",
    "sc_quarters = np.arange(18)[np.array(all_dtype) == 'short']\n",
    "\n",
    "\n",
    "# expand var_by_quarter to have None for missing quarters\n",
    "vbq_all = [None]*18\n",
    "\n",
    "for j, q in enumerate(quarters):\n",
    "    vbq_all[q] = var_by_quarter[j]\n",
    "    \n",
    "vbq_all = np.asarray(vbq_all, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some linear flux arrays (for convenience use laster)\n",
    "try: sc_flux_lin = np.hstack(sc_flux)\n",
    "except: sc_flux_lin = np.array([])\n",
    "    \n",
    "try: lc_flux_lin = np.hstack(lc_flux)\n",
    "except: lc_flux_lin = np.array([])\n",
    "    \n",
    "try:\n",
    "    good_flux = np.hstack([sc_flux_lin, lc_flux_lin])\n",
    "except:\n",
    "    try:\n",
    "        good_flux = np.hstack(sc_flux)\n",
    "    except:\n",
    "        good_flux = np.hstack(lc_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Legendre polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Legendre polynomials for better orthogonality; \"x\" is in the range (-1,1)\n",
    "Leg0 = []\n",
    "Leg1 = []\n",
    "Leg2 = []\n",
    "Leg3 = []\n",
    "t = []\n",
    "\n",
    "# this assumes a baseline in the range (TIME_START,TIME_END)\n",
    "for npl in range(NPL):    \n",
    "    t.append(fixed_ephemeris[npl])\n",
    "    x = 2*(t[npl]-TIME_START)/(TIME_END-TIME_START) - 1\n",
    "\n",
    "    Leg0.append(np.ones_like(x))\n",
    "    Leg1.append(x.copy())\n",
    "    Leg2.append(0.5*(3*x**2 - 1))\n",
    "    Leg3.append(0.5*(5*x**3 - 3*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a transit model with fixed transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as shape_model:\n",
    "    # stellar parameters (limb darkening using Kipping 2013)\n",
    "    q_limbdark = pm.Uniform(\"q_limbdark\", lower=0, upper=1, testval=np.array([Q1,Q2]), shape=2)\n",
    "    u1 = 2*T.sqrt(q_limbdark[0])*q_limbdark[1]\n",
    "    u2 = T.sqrt(q_limbdark[0])*(1-2*q_limbdark[1])\n",
    "    u  = pm.Deterministic(\"u\", T.stack([u1,u2]))\n",
    "\n",
    "    Rstar  = pm.Bound(pm.Normal, lower=0)('Rstar', mu=RSTAR, sd=RSTAR_ERR)\n",
    "    \n",
    "    # planetary parameters (rho is circular density, see Ford, Quinn, & Veras 2008)\n",
    "    logr = pm.Uniform('logr', lower=np.log(0.0003), upper=np.log(0.3), testval=np.log(RADII), shape=NPL)\n",
    "    rp   = pm.Deterministic('rp', T.exp(logr))\n",
    "    \n",
    "    beta = pm.Exponential('beta', lam=1, testval=-np.log(IMPACTS), shape=NPL)\n",
    "    b    = pm.Deterministic('b', T.exp(-beta))\n",
    "    \n",
    "    logrho = pm.Normal(\"logrho\", mu=0, sd=10, shape=NPL, testval=np.log(RHOSUN_GCM3))\n",
    "    rho    = pm.Deterministic(\"rho\", T.exp(logrho))\n",
    "    \n",
    "    # polynomial TTV parameters (coefficients for Legendre polynomicals)\n",
    "    C0 = pm.Normal('C0', mu=np.zeros(NPL), sd=3*omc_scatter, shape=NPL)\n",
    "    C1 = pm.Normal('C1', mu=np.zeros(NPL), sd=3*omc_scatter, shape=NPL)\n",
    "    \n",
    "    \n",
    "    # transit times\n",
    "    transit_times = []\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        transit_times.append(pm.Deterministic(\"tts_{0}\".format(npl),\n",
    "                                              fixed_transit_times[npl]\n",
    "                                              + C0[npl]*Leg0[npl] + C1[npl]*Leg1[npl]))\n",
    "   \n",
    "    # set up stellar model and planetary orbit\n",
    "    exoSLC = exo.StarryLightCurve(u)\n",
    "    orbit  = exo.orbits.TTVOrbit(transit_times=transit_times, transit_inds=transit_inds, \n",
    "                                 b=b, r_star=Rstar, rho_star=rho)\n",
    "    \n",
    "    # track period and epoch\n",
    "    T0 = pm.Deterministic('T0', orbit.t0)\n",
    "    P  = pm.Deterministic('P', orbit.period)\n",
    "    \n",
    "    \n",
    "    # build the GP kernel using a different noise model for each season\n",
    "    logSw4   = [None]*4\n",
    "    logw0    = [None]*4\n",
    "    logQ_off = [None]*4\n",
    "    logQ     = [None]*4\n",
    "    logS     = [None]*4\n",
    "    \n",
    "    kernel  = [None]*4\n",
    "    \n",
    "    for z in range(4):\n",
    "        gpz = gp_priors[z]\n",
    "        \n",
    "        try:\n",
    "            logSw4[z] = pm.Normal('logSw4_{0}'.format(z), mu=gpz['logSw4'][0], sd=gpz['logSw4'][1])\n",
    "        except:\n",
    "            logSw4[z] = gpz['logSw4'][0]\n",
    "        \n",
    "        try:\n",
    "            logw0[z] = pm.Normal('logw0_{0}'.format(z), mu=gpz['logw0'][0], sd=gpz['logw0'][1])\n",
    "        except:\n",
    "            logw0[z] = gpz['logw0'][0]\n",
    "\n",
    "        try:\n",
    "            logQ_off[z] = pm.Normal('logQ_off_{0}'.format(z), \n",
    "                                    mu=np.log(np.exp(gpz['logQ'][0])-1/np.sqrt(2)), \n",
    "                                    sd=gpz['logQ'][1])\n",
    "            logQ[z] = pm.Deterministic('logQ_{0}'.format(z), T.log(1/T.sqrt(2) + T.exp(logQ_off[z]))) \n",
    "        \n",
    "        except:\n",
    "            logQ[z] = gpz['logQ'][0]\n",
    "            \n",
    "        try:\n",
    "            logS[z] = pm.Deterministic('logS_{0}'.format(z), logSw4[z]-4*logw0[z])\n",
    "            pm.Potential('penalty_{0}'.format(z), -T.exp((logS[z]-gpz['logS'][0])/gpz['logS'][1]))\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "            \n",
    "        if np.isin(z, seasons):\n",
    "            kernel[z] = exo.gp.terms.SHOTerm(log_Sw4=logSw4[z], log_w0=logw0[z], log_Q=logQ[z])\n",
    "        else:\n",
    "            kernel[z] = None\n",
    "        \n",
    "        \n",
    "    # nuissance parameters\n",
    "    flux0 = pm.Normal('flux0', mu=np.mean(good_flux), sd=np.std(good_flux), shape=len(quarters))\n",
    "    logjit = pm.Normal('logjit', mu=np.var(good_flux), sd=10, shape=len(quarters))\n",
    "    \n",
    "    \n",
    "    # now evaluate the model for each quarter\n",
    "    light_curves       = [None]*nq\n",
    "    summed_light_curve = [None]*nq\n",
    "    model_flux         = [None]*nq\n",
    "    \n",
    "    gp      = [None]*nq\n",
    "    gp_pred = [None]*nq\n",
    "    \n",
    "    \n",
    "    for j, q in enumerate(quarters):\n",
    "        # set oversampling factor\n",
    "        if all_dtype[q] == 'short':\n",
    "            oversample = 1\n",
    "        elif all_dtype[q] == 'long':\n",
    "            oversample = 15\n",
    "        else:\n",
    "            raise ValueError(\"Cadence data type must be 'short' or 'long'\")\n",
    "            \n",
    "        # calculate light curves\n",
    "        light_curves[j] = exoSLC.get_light_curve(orbit=orbit, r=rp, t=all_time[q], oversample=oversample)\n",
    "        summed_light_curve[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(all_time[q]))\n",
    "        model_flux[j] = pm.Deterministic('model_flux_{0}'.format(j), summed_light_curve[j])\n",
    "        \n",
    "        # here's the GP (w/ kernel by season)\n",
    "        gp[j] = exo.gp.GP(kernel[q%4], all_time[q], T.exp(logjit[j])*T.ones(len(all_time[q])))\n",
    "            \n",
    "        # add custom potential (log-prob fxn) with the GP likelihood\n",
    "        pm.Potential('obs_{0}'.format(j), gp[j].log_likelihood(all_flux[q] - model_flux[j]))\n",
    "\n",
    "        # track GP prediction\n",
    "        #gp_pred[j] = pm.Deterministic('gp_pred_{0}'.format(j), gp[j].predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shape_model:\n",
    "    shape_map = exo.optimize(start=shape_model.test_point, vars=[flux0, logjit])\n",
    "    shape_map = exo.optimize(start=shape_map, vars=[C0, C1])\n",
    "    shape_map = exo.optimize(start=shape_map, vars=[b, rp, rho])\n",
    "    shape_map = exo.optimize(start=shape_map, vars=[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shape_model:\n",
    "    shape_trace = pm.sample(tune=15000, draws=5000, start=shape_map, chains=2, \n",
    "                            step=exo.get_dense_nuts_step(target_accept=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which variables to save (don't save full GP or model traces or \"under the hood\" variables)\n",
    "shape_map_keys = list(shape_map.keys())\n",
    "shape_varnames = []\n",
    "\n",
    "for i, smk in enumerate(shape_map_keys):\n",
    "    skip = (\"gp_pred\" in smk) + (\"model_flux\" in smk) + (\"__\" in smk)\n",
    "\n",
    "    if skip == False:\n",
    "        shape_varnames.append(smk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_hdulist = io.trace_to_hdulist(shape_trace, shape_varnames, TARGET)\n",
    "shape_hdulist.writeto(TRACE_DIR + TARGET + '_transit_shape.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOTAL RUNTIME = %.2f min' %((timer()-global_start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
