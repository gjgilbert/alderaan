{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "import astropy.stats\n",
    "from   astropy.io import fits as pyfits\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "import corner\n",
    "\n",
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import weighted_percentile\n",
    "from alderaan.utils import bin_data\n",
    "import alderaan.io as io\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K02068-00\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "\n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog.csv\"\n",
    "    TRUE_TTV_DIR = PRIMARY_DIR + \"Simulations/TTVs/\"\n",
    "    \n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANET_NO = int(TARGET[-2:])\n",
    "KOI_ID = TARGET[:6]\n",
    "\n",
    "TARGET, KOI_ID, PLANET_NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPSAMPLE = 10\n",
    "ECC_SCALE = 0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs    \n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + KOI_ID + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + KOI_ID + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + KOI_ID + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + KOI_ID + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + KOI_ID + '/'\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in independently determined stellar parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = KOI_ID\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + KOI_ID[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "RSTAR_TRUE = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "\n",
    "LOGRHO_TRUE = np.array(target_dict['logrho'], dtype='float')[use]\n",
    "LOGRHO_ERR1_TRUE = np.array(target_dict['logrho_err1'], dtype='float')[use]\n",
    "LOGRHO_ERR2_TRUE = np.array(target_dict['logrho_err2'], dtype='float')[use]\n",
    "\n",
    "U1_TRUE = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "U2_TRUE = np.array(target_dict['limbdark_2'], dtype='float')[use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIODS = np.array(target_dict['period'], dtype='float')[use]\n",
    "EPOCHS  = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "DEPTHS  = np.array(target_dict['depth'], dtype='float')[use]*1e-6          # [ppm] --> []\n",
    "DURS    = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]\n",
    "\n",
    "PERIODS = np.array(target_dict['period'], dtype='float')[use]\n",
    "EPOCHS  = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "DEPTHS  = np.array(target_dict['depth'], dtype='float')[use]*1e-6          # [ppm] --> []\n",
    "DURS    = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR_TRUE[0] for r in RSTAR_TRUE): RSTAR_TRUE = RSTAR_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_TRUE[0] for r in LOGRHO_TRUE): LOGRHO_TRUE = LOGRHO_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR1_TRUE[0] for r in LOGRHO_ERR1_TRUE): LOGRHO_ERR1_TRUE = LOGRHO_ERR1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR1 in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR2_TRUE[0] for r in LOGRHO_ERR2_TRUE): LOGRHO_ERR2_TRUE = LOGRHO_ERR2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR2 in the csv input file')\n",
    "\n",
    "if all(u == U1_TRUE[0] for u in U1_TRUE): U1_TRUE = U1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "if all(u == U2_TRUE[0] for u in U2_TRUE): U2_TRUE = U2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U2 in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_TRUE = 10**(LOGRHO_TRUE)\n",
    "RHO_ERR1_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR1_TRUE) - RHO_TRUE\n",
    "RHO_ERR2_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR2_TRUE) - RHO_TRUE\n",
    "RHO_ERR_TRUE  = np.sqrt(RHO_ERR1_TRUE**2 + RHO_ERR2_TRUE**2)/np.sqrt(2)\n",
    "\n",
    "RHO_OBS = (RHO_TRUE, RHO_ERR_TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull trace posteriors\n",
    "\n",
    "Data structure is traces[$umbrella$][$parameter$], e.g. traces[\"G\"][\"LOG_R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_files = np.sort(glob.glob(TRACE_DIR + KOI_ID + \"_{:02d}_shape_*\".format(PLANET_NO)))\n",
    "\n",
    "with pyfits.open(trace_files[0]) as trace:\n",
    "    print(trace.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {}\n",
    "\n",
    "for i, tf in enumerate(trace_files):\n",
    "    UID = tf[-6]\n",
    "    \n",
    "    traces[UID] = {}\n",
    "    \n",
    "    with pyfits.open(tf) as trace_data:\n",
    "        \n",
    "        for j in range(1,len(trace_data)):\n",
    "            key = trace_data[j].header[\"EXTNAME\"]\n",
    "            \n",
    "            traces[UID][key] = np.squeeze(trace_data[j].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check transition umbrella $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,5))\n",
    "bins = np.linspace(0,2,21)\n",
    "\n",
    "npl = PLANET_NO\n",
    "g = traces[\"T\"][\"GAMMA\"]\n",
    "\n",
    "y, x, _ = ax.hist(g, bins=bins, density=True, histtype=\"step\", color=\"grey\".format(npl), lw=3)\n",
    "\n",
    "ax.axvline(1, ls=\":\", color=\"k\")\n",
    "ax.text(0.95,y.max()*1.13, r\"$\\leftarrow$ grazing\", va=\"top\", ha=\"right\", fontsize=14)\n",
    "ax.text(1.05,y.max()*1.13, r\"non-grazing $\\rightarrow$\", va=\"top\", ha=\"left\", fontsize=14)\n",
    "ax.set_xlabel(r\"$\\gamma$\", fontsize=20)\n",
    "ax.set_ylim(0, y.max()*1.15)\n",
    "ax.set_yticks([])\n",
    "\n",
    "#plt.savefig(\"/Users/research/projects/grazing_transits/Figures/transition_umbrella_K01426-02.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "num_G = np.sum(g < 1)\n",
    "num_N = np.sum(g >= 1)\n",
    "\n",
    "print(\"{0} out of {1} samples are consistent with a grazing transit\".format(num_G, len(g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for consistent $r$, $b$, $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(24,5))\n",
    "\n",
    "if num_N > 0: ax[0].hist(traces[\"N\"][\"LOG_R\"], histtype=\"step\", density=True, lw=2, label=\"N\")\n",
    "ax[0].hist(traces[\"T\"][\"LOG_R\"], histtype=\"step\", density=True, lw=2, label=\"T\")\n",
    "if num_G > 0: ax[0].hist(traces[\"G\"][\"LOG_R\"], histtype=\"step\", density=True, lw=2, label=\"G\")\n",
    "ax[0].set_yticks([])\n",
    "ax[0].legend(fontsize=16)\n",
    "ax[0].set_title(\"$\\log r$\", fontsize=24)\n",
    "\n",
    "if num_N > 0: ax[1].hist(traces[\"N\"][\"B\"], histtype=\"step\", density=True, lw=2, label=\"N\")\n",
    "ax[1].hist(traces[\"T\"][\"B\"], histtype=\"step\", density=True, lw=2, label=\"T\")\n",
    "if num_G > 0: ax[1].hist(traces[\"G\"][\"B\"], histtype=\"step\", density=True, lw=2, label=\"G\")\n",
    "ax[1].set_yticks([])\n",
    "ax[1].legend(fontsize=16)\n",
    "ax[1].set_title(\"$b$\", fontsize=24)\n",
    "\n",
    "if num_N > 0: ax[2].hist(traces[\"N\"][\"LOG_DUR\"], histtype=\"step\", density=True, lw=2, label=\"N\")\n",
    "ax[2].hist(traces[\"T\"][\"LOG_DUR\"], histtype=\"step\", density=True, lw=2, label=\"T\")\n",
    "if num_G > 0: ax[2].hist(traces[\"G\"][\"LOG_DUR\"], histtype=\"step\", density=True, lw=2, label=\"G\")\n",
    "ax[2].set_yticks([])\n",
    "ax[2].legend(fontsize=16)\n",
    "ax[2].set_title(\"$\\log T$\", fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "if num_N > 0: plt.hist(traces[\"N\"][\"GAMMA\"], histtype=\"step\", density=True, lw=2, label=\"N\")\n",
    "plt.hist(traces[\"T\"][\"GAMMA\"], histtype=\"step\", density=True, lw=2, label=\"T\")\n",
    "if num_G > 0: plt.hist(traces[\"G\"][\"GAMMA\"], histtype=\"step\", density=True, lw=2, label=\"G\")\n",
    "plt.xlim(-1,5)\n",
    "plt.yticks([])\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(\"$\\gamma$\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define umbrella functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmin, rmax = 1e-4, 0.99\n",
    "\n",
    "def psi_N(x, norm=1/rmin-1.5):\n",
    "    x_ = np.atleast_1d(x)\n",
    "    \n",
    "    psi = np.zeros_like(x_)\n",
    "    psi[(x_ < 2)*(x_ >= 1)] = x_[(x_ < 2)*(x_ >= 1)] - 1\n",
    "    psi[(x_ >= 2)] = 1.0\n",
    "    psi = psi.clip(0,1)\n",
    "    \n",
    "    return psi/norm\n",
    "\n",
    "\n",
    "def psi_T(x, norm=1.0):\n",
    "    x_ = np.atleast_1d(x)\n",
    "    psi = np.zeros_like(x_)\n",
    "    psi[(x_ >= 0)*(x_ < 1)] = x_[(x_ >= 0)*(x_ < 1)]\n",
    "    psi[(x_ >= 1)*(x_ < 2)] = 2 - x_[(x_ >= 1)*(x_ < 2)]\n",
    "    psi = psi.clip(0,1)\n",
    "    \n",
    "    return psi/norm\n",
    "\n",
    "    \n",
    "def psi_G(x, norm=1.5):\n",
    "    x_ = np.atleast_1d(x)\n",
    "    \n",
    "    psi = np.zeros_like(x_)\n",
    "    psi[(x_ < 0)*(x_ >= -1)] = 1. + x_[(x_ < 0)*(x_ >= -1)]\n",
    "    psi[(x_ >= 0)] = 1. - x_[x_ >= 0]\n",
    "    psi = psi.clip(0,1)\n",
    "    \n",
    "    return psi/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_G > 0)*(num_N > 0):\n",
    "    psi_fxns = [psi_N, psi_T, psi_G]\n",
    "    psi_names = [\"N\", \"T\", \"G\"]\n",
    "    \n",
    "elif (num_G == 0)*(num_N > 0):\n",
    "    psi_fxns = [psi_N, psi_T]\n",
    "    psi_names = [\"N\", \"T\"]\n",
    "    \n",
    "elif (num_G > 0)*(num_nongrazN == 0):\n",
    "    psi_fxns = [psi_T, psi_G]\n",
    "    psi_names = [\"T\", \"G\"]    \n",
    "    \n",
    "Nwin = len(psi_fxns)\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-1, 5, 1000)\n",
    "for i in range(Nwin):\n",
    "    plt.plot(x, psi_fxns[i](x)/psi_fxns[i](x).max(), lw=2, label=psi_names[i])\n",
    "plt.xlabel(\"$\\gamma$\", fontsize=20)\n",
    "plt.ylabel(\"$f(\\gamma)$\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc eccentricity weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ecc = []\n",
    "\n",
    "for i, name in enumerate(psi_names):\n",
    "    rho_circ = np.repeat(traces[name][\"RHO\"], UPSAMPLE)\n",
    "    \n",
    "    if ECC_SCALE is None:\n",
    "        ecc = np.random.uniform(0, 1, len(rho_circ))\n",
    "        omega = np.random.uniform(-0.5*np.pi, 1.5*np.pi, len(rho_circ))\n",
    "    \n",
    "    else:\n",
    "        esinw, ecosw = np.random.normal(loc=0, scale=ECC_SCALE, size=2*len(rho_circ)).reshape(2,-1)\n",
    "        ecc = np.sqrt(esinw**2 + ecosw**2)\n",
    "        omega = np.arctan2(esinw, ecosw)\n",
    "\n",
    "        while np.any(ecc >= 1):\n",
    "            print(\"redrawing\", np.sum(ecc>=1))\n",
    "            esinw, ecosw = np.random.normal(loc=0, scale=ECC_SCALE, size=2*np.sum(ecc>=1)).reshape(2,-1)\n",
    "            omega[ecc>=1] = np.arctan2(esinw, ecosw)\n",
    "            ecc[ecc>=1] = np.sqrt(esinw**2 + ecosw**2)\n",
    "            \n",
    "    # calculate weight from photoeccentric effect \n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_circ / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - RHO_OBS[0]) / RHO_OBS[1]) ** 2\n",
    "    weight = np.exp(log_weight - np.max(log_weight))\n",
    "    weight /= np.sum(weight)\n",
    "\n",
    "    tol = 1e-12\n",
    "    weight[weight < tol] = tol\n",
    "\n",
    "    weights_ecc.append(weight/np.sum(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate umbrella weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alderaan import emus\n",
    "import importlib as imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "\n",
    "for i, name in enumerate(psi_names):\n",
    "    coords.append(np.repeat(traces[name][\"GAMMA\"], UPSAMPLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = emus.umbrella_weights(psi_fxns, coords, weights_ecc)\n",
    "\n",
    "weights_umb = []\n",
    "for i in range(Nwin):\n",
    "    wd = 0.\n",
    "    for k in range(Nwin):\n",
    "        wd += psi_fxns[k](coords[i])/z[k]\n",
    "        \n",
    "    weights_umb.append(1./wd)\n",
    "    \n",
    "weights = np.array(weights_umb).flatten()/np.sum(weights_umb) * np.hstack(weights_ecc)/np.sum(weights_ecc)\n",
    "weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_keys = [\"R\", \"B\", \"DUR\", \"P\", \"RHO\"]\n",
    "var_names = [\"r\", \"b\", \"dur\", \"per\", \"rho\"]\n",
    "\n",
    "samples = {}\n",
    "\n",
    "for i, key in enumerate(var_keys):\n",
    "    name = var_names[i]\n",
    "    samples[name] = []\n",
    "    \n",
    "    for i, psi in enumerate(psi_names):\n",
    "        samples[name].append(np.repeat(traces[psi][key], UPSAMPLE))\n",
    "        \n",
    "    samples[name] = np.hstack(samples[name])\n",
    "    \n",
    "\n",
    "samples[\"u1\"] = []\n",
    "samples[\"u2\"] = []\n",
    "for i, psi in enumerate(psi_names):\n",
    "    samples[\"u1\"].append(np.repeat(traces[psi][\"U\"][:,0], UPSAMPLE))\n",
    "    samples[\"u2\"].append(np.repeat(traces[psi][\"U\"][:,1], UPSAMPLE))\n",
    "    \n",
    "samples[\"u1\"] = np.hstack(samples[\"u1\"])\n",
    "samples[\"u2\"] = np.hstack(samples[\"u2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic statsitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in var_names:\n",
    "    x = []\n",
    "\n",
    "    for q in [16, 50, 84]:\n",
    "        x.append(weighted_percentile(samples[var], q, weights))\n",
    "        \n",
    "    if var == \"r\":\n",
    "        x = np.array(x)*RSTAR_TRUE*RSRE\n",
    "    else:\n",
    "        x = np.array(x)\n",
    "\n",
    "    print(\"\\n{0}\".format(var))\n",
    "    print(\"----\")\n",
    "    print(\"{0:.3f} +{1:.3f} -{2:.3f}\".format(x[1], x[2]-x[1], x[1]-x[0]))\n",
    "    print(\"{0:.2f} +/- {1:.2f}\".format(x[1], (x[2]-x[0])/2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now plot the posterior lightcurve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load detrended lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detrended lightcurves\n",
    "try:\n",
    "    lc = io.load_detrended_lightcurve(DLC_DIR + KOI_ID + '_lc_detrended.fits')\n",
    "except:\n",
    "    lc = None\n",
    "    \n",
    "try:\n",
    "    sc = io.load_detrended_lightcurve(DLC_DIR + KOI_ID + '_sc_detrended.fits')\n",
    "except:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in QuickTTV estimates and calculate linear ephemeris for each planet\n",
    "EPOCHS  = np.zeros(NPL)\n",
    "PERIODS = np.zeros(NPL)\n",
    "\n",
    "transit_inds = []\n",
    "quick_transit_times = []\n",
    "quick_ephemeris = []\n",
    "\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # read in predetermined transit times\n",
    "    fname_in = QUICK_TTV_DIR + KOI_ID + '_{:02d}'.format(npl) + '_quick.ttvs'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    transit_inds.append(data_in[:,0])\n",
    "    quick_transit_times.append(data_in[:,2])\n",
    "    \n",
    "    # do a quick fit to get a linear ephemeris\n",
    "    pfit = poly.polyfit(transit_inds[npl], quick_transit_times[npl], 1)\n",
    "    \n",
    "    quick_ephemeris.append(poly.polyval(transit_inds[npl], pfit))\n",
    "    \n",
    "    EPOCHS[npl] = pfit[1]\n",
    "    PERIODS[npl] = pfit[0]\n",
    "    \n",
    "    \n",
    "# make sure transit_inds are zero-indexed\n",
    "for npl in range(NPL):\n",
    "    transit_inds[npl] = np.array(transit_inds[npl] - transit_inds[npl][0], dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "\n",
    "if NPL == 1:\n",
    "    ax = [ax]\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = quick_ephemeris[npl]\n",
    "    yomc = (quick_transit_times[npl] - quick_ephemeris[npl])*24*60\n",
    "    \n",
    "    ax[npl].plot(xtime, yomc, '-', c='C{0}'.format(npl))\n",
    "    ax[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "ax[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify overlapping transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lc is not None:\n",
    "    lc_quarters = np.unique(lc.quarter)\n",
    "else:\n",
    "    lc_quarters = np.array([])\n",
    "    \n",
    "if sc is not None:\n",
    "    sc_quarters = np.unique(sc.quarter)\n",
    "else:\n",
    "    sc_quarters = np.array([])\n",
    "    \n",
    "    \n",
    "quarters = np.sort(np.hstack([lc_quarters, sc_quarters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = []\n",
    "\n",
    "for i in range(NPL):\n",
    "    overlap.append(np.zeros(len(quick_ephemeris[i]), dtype='bool'))\n",
    "    \n",
    "    for j in range(NPL):\n",
    "        if i != j:\n",
    "            for tt in quick_ephemeris[j]:\n",
    "                overlap[i] += np.abs(quick_ephemeris[i] - tt) < (DURS[i] + DURS[j] + lcit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase-folded the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npl = PLANET_NO\n",
    "tts = quick_transit_times[npl][~overlap[npl]]\n",
    "dur = weighted_percentile(samples[\"dur\"], 50, weights)\n",
    "\n",
    "t_folded = []\n",
    "f_folded = []\n",
    "\n",
    "# grab the data\n",
    "for t0 in tts:\n",
    "    if sc is not None:\n",
    "        use = np.abs(sc.time-t0)/dur < 1.5\n",
    "\n",
    "        if np.sum(use) > 0:\n",
    "            t_folded.append(sc.time[use]-t0)\n",
    "            f_folded.append(sc.flux[use])\n",
    "\n",
    "    if lc is not None:\n",
    "        use = np.abs(lc.time-t0)/dur < 1.5\n",
    "\n",
    "        if np.sum(use) > 0:\n",
    "            t_folded.append(lc.time[use]-t0)\n",
    "            f_folded.append(lc.flux[use])\n",
    "\n",
    "# sort the data\n",
    "t_folded = np.hstack(t_folded)\n",
    "f_folded = np.hstack(f_folded)\n",
    "\n",
    "order = np.argsort(t_folded)\n",
    "t_folded = t_folded[order]\n",
    "f_folded = f_folded[order]\n",
    "\n",
    "\n",
    "# bin the data\n",
    "t_binned, f_binned = bin_data(t_folded, f_folded, dur/11)\n",
    "\n",
    "# set undersampling factor and plotting limits\n",
    "inds = np.arange(len(t_folded), dtype=\"int\")\n",
    "inds = np.random.choice(inds, size=np.min([3000,len(inds)]), replace=False)\n",
    "\n",
    "ymin = 1 - 5*np.std(f_folded)\n",
    "ymax = 1 + 3*np.std(f_folded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model lightcurve from posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exoplanet as exo\n",
    "import aesara_theano_fallback.tensor as T\n",
    "from   aesara_theano_fallback import aesara as theano\n",
    "\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDRAW = 50\n",
    "\n",
    "draw = np.random.choice(np.arange(len(weights)), p=weights, size=NDRAW)\n",
    "t_ = np.linspace(t_folded.min(), t_folded.max(), 250)\n",
    "light_curves = [None]*NDRAW\n",
    "\n",
    "u1 = theano.shared(samples[\"u1\"][0], \"u1\")\n",
    "u2 = theano.shared(samples[\"u2\"][0], \"u2\")\n",
    "r  = theano.shared(samples[\"r\"][0], \"r\")\n",
    "b  = theano.shared(samples[\"b\"][0], \"b\")\n",
    "per = theano.shared(samples[\"per\"][0], \"per\")\n",
    "dur = theano.shared(samples[\"dur\"][0], \"dur\")\n",
    "\n",
    "\n",
    "for i, d in enumerate(draw):\n",
    "    print(i, d)\n",
    "    \n",
    "    u1.set_value(samples[\"u1\"][d])\n",
    "    u2.set_value(samples[\"u2\"][d])\n",
    "    r.set_value(samples[\"r\"][d])\n",
    "    b.set_value(samples[\"b\"][d])\n",
    "    per.set_value(samples[\"per\"][d])\n",
    "    dur.set_value(samples[\"dur\"][d])\n",
    "    \n",
    "    starrystar = exo.LimbDarkLightCurve([u1,u2])\n",
    "    orbit = exo.orbits.KeplerianOrbit(period = per, ror=r, b = b, duration =dur)\n",
    "    \n",
    "    light_curves[i] = starrystar.get_light_curve(orbit=orbit, r=r, t=t_, oversample=7).sum(-1).eval()\n",
    "\n",
    "    \n",
    "light_curves = np.array(light_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(t_folded[inds]*24, f_folded[inds], \".\", c=\"lightgrey\", zorder=0)\n",
    "plt.xlim(t_folded.min()*24, t_folded.max()*24)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Time from mid-transit [hrs]\", fontsize=20)\n",
    "plt.ylabel(\"Flux\", fontsize=20)\n",
    "\n",
    "f_16 = 1+np.percentile(light_curves, 16, axis=0)\n",
    "f_50 = 1+np.percentile(light_curves, 50, axis=0)\n",
    "f_84 = 1+np.percentile(light_curves, 84, axis=0)\n",
    "\n",
    "plt.plot(t_*24, f_50, c=\"r\", lw=2)\n",
    "plt.fill_between(t_*24, f_16, f_84, color=\"r\", alpha=0.3)\n",
    "plt.savefig(FIGURE_DIR + KOI_ID + '_{0:02d}_posterior_model.pdf'.format(npl), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_corner = False\n",
    "\n",
    "if do_corner:\n",
    "    filename = FIGURE_DIR + \"corner_{0}.pdf\".format(TARGET)\n",
    "\n",
    "    extents = {}\n",
    "    extents[\"r\"] = (0.015, 0.05)\n",
    "\n",
    "    c = ChainConsumer()\n",
    "    c.add_chain(samples, weights=weights).configure(summary=False)\n",
    "    fig = c.plotter.plot(filename=filename)\n",
    "    fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
