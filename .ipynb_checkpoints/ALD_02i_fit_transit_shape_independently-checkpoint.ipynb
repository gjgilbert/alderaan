{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit transit shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import scipy.stats as stats\n",
    "from   astropy.io import fits as pyfits\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib as imp\n",
    "import warnings\n",
    "import argparse\n",
    "import json\n",
    "from   timeit import default_timer as timer\n",
    "\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "import exoplanet as exo\n",
    "import aesara_theano_fallback.tensor as T\n",
    "from   aesara_theano_fallback import aesara as theano\n",
    "from   celerite2.theano import GaussianProcess\n",
    "from   celerite2.theano import terms as GPterms\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import *\n",
    "from alderaan.Planet import *\n",
    "from alderaan.LiteCurve import *\n",
    "import alderaan.io as io\n",
    "import alderaan.detrend as detrend\n",
    "\n",
    "# these lines attempt to make progressbar work with SLURM \n",
    "from fastprogress.fastprogress import force_console_behavior\n",
    "master_bar, progress_bar = force_console_behavior()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually set I/O parameters\n",
    "<pre>\n",
    "MISSION     : can be \"Kepler\" or \"Simulated\"\n",
    "TARGET      : specifies target star, format should be \"K00000-00\" or \"S00000-00\"\n",
    "UMBRELLA    : can be \"non-grazing\", \"transition\", or \"grazing\"\n",
    "PRIMARY_DIR : primary directory for progject\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K02068-00\"\n",
    "UMBRELLA = \"transition\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "\n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\"\n",
    "    \n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog_eccentric.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where we parse the inputs\n",
    "try:\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for ALDERAAN transit fiting pipeline\")\n",
    "    parser.add_argument(\"--mission\", default=None, type=str, required=True, \\\n",
    "                        help=\"Mission name\")\n",
    "    parser.add_argument(\"--target\", default=None, type=str, required=True, \\\n",
    "                        help=\"Target name; see ALDERAAN documentation for acceptable formats\")\n",
    "    parser.add_argument(\"--umbrella\", default=None, type=str, required=True, \\\n",
    "                        help=\"Umbrella can be 'non-grazing', 'transition', or 'grazing'\")\n",
    "    parser.add_argument(\"--primary_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"Primary directory path for accessing lightcurve data and saving outputs\")\n",
    "    parser.add_argument(\"--csv_file\", default=None, type=str, required=True, \\\n",
    "                        help=\"Path to .csv file containing input planetary parameters\")\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    MISSION     = args.mission\n",
    "    TARGET      = args.target\n",
    "    UMBRELLA    = args.umbrella\n",
    "    PRIMARY_DIR = args.primary_dir\n",
    "    CSV_FILE    = args.csv_file\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANET_NO = int(TARGET[-2:])\n",
    "TARGET = TARGET[:6]\n",
    "\n",
    "TARGET, PLANET_NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs\n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in planet and stellar parameters from Kepler DR25 & Gaia DR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = TARGET\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "PERIODS = np.array(target_dict['period'], dtype='float')[use]\n",
    "EPOCHS  = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "DEPTHS  = np.array(target_dict['depth'], dtype='float')[use]*1e-6          # [ppm] --> []\n",
    "DURS    = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort planet parameters by period\n",
    "order = np.argsort(PERIODS)\n",
    "\n",
    "PERIODS = PERIODS[order]\n",
    "EPOCHS  = EPOCHS[order]\n",
    "DEPTHS  = DEPTHS[order]\n",
    "DURS    = DURS[order]\n",
    "IMPACTS = 0.5*np.ones(NPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in detrended lightcurves and initial transit time estimates\n",
    "#### The data can be generated by running the script \"detrend_and_estimate_noise.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detrended lightcurves\n",
    "try:\n",
    "    lc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_lc_detrended.fits')\n",
    "except:\n",
    "    lc = None\n",
    "    \n",
    "try:\n",
    "    sc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_sc_detrended.fits')\n",
    "except:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in QuickTTV estimates and calculate linear ephemeris for each planet\n",
    "EPOCHS  = np.zeros(NPL)\n",
    "PERIODS = np.zeros(NPL)\n",
    "\n",
    "transit_inds = []\n",
    "quick_transit_times = []\n",
    "quick_ephemeris = []\n",
    "\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # read in predetermined transit times\n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_quick.ttvs'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    transit_inds.append(data_in[:,0])\n",
    "    quick_transit_times.append(data_in[:,2])\n",
    "    \n",
    "    # do a quick fit to get a linear ephemeris\n",
    "    pfit = poly.polyfit(transit_inds[npl], quick_transit_times[npl], 1)\n",
    "    \n",
    "    quick_ephemeris.append(poly.polyval(transit_inds[npl], pfit))\n",
    "    \n",
    "    EPOCHS[npl] = pfit[1]\n",
    "    PERIODS[npl] = pfit[0]\n",
    "    \n",
    "    \n",
    "# make sure transit_inds are zero-indexed\n",
    "for npl in range(NPL):\n",
    "    transit_inds[npl] = np.array(transit_inds[npl] - transit_inds[npl][0], dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "\n",
    "if NPL == 1:\n",
    "    ax = [ax]\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = quick_ephemeris[npl]\n",
    "    yomc = (quick_transit_times[npl] - quick_ephemeris[npl])*24*60\n",
    "    \n",
    "    ax[npl].plot(xtime, yomc, '-', c='C{0}'.format(npl))\n",
    "    ax[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "ax[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine scatter relative to linear ephemeris\n",
    "# this is a deliberate overestimate of the true scatter\n",
    "omc_scatter = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = quick_ephemeris[npl]\n",
    "    yomc  = quick_transit_times[npl] - quick_ephemeris[npl]\n",
    "    \n",
    "    omc_scatter[npl] = np.std(yomc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify overlapping transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lc is not None:\n",
    "    lc_quarters = np.unique(lc.quarter)\n",
    "else:\n",
    "    lc_quarters = np.array([])\n",
    "    \n",
    "if sc is not None:\n",
    "    sc_quarters = np.unique(sc.quarter)\n",
    "else:\n",
    "    sc_quarters = np.array([])\n",
    "    \n",
    "    \n",
    "quarters = np.sort(np.hstack([lc_quarters, sc_quarters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = []\n",
    "\n",
    "for i in range(NPL):\n",
    "    overlap.append(np.zeros(len(quick_ephemeris[i]), dtype='bool'))\n",
    "    \n",
    "    for j in range(NPL):\n",
    "        if i != j:\n",
    "            for tt in quick_ephemeris[j]:\n",
    "                overlap[i] += np.abs(quick_ephemeris[i] - tt) < (DURS[i] + DURS[j] + lcit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set fixed ephemeris and transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_ephemeris = []\n",
    "fixed_transit_times = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    fixed_ephemeris.append(quick_ephemeris[npl][~overlap[npl]])\n",
    "    fixed_transit_times.append(quick_transit_times[npl][~overlap[npl]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set time baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the time baseline\n",
    "time_min = []\n",
    "time_max = []\n",
    "\n",
    "if sc is not None:\n",
    "    time_min.append(sc.time.min())\n",
    "    time_max.append(sc.time.max()) \n",
    "\n",
    "if lc is not None:\n",
    "    time_min.append(lc.time.min())\n",
    "    time_max.append(lc.time.max())     \n",
    "\n",
    "    \n",
    "TIME_START = np.min(time_min)\n",
    "TIME_END   = np.max(time_max)\n",
    "\n",
    "\n",
    "# put epochs in range (TIME_START, TIME_START + PERIOD)\n",
    "for npl in range(NPL):\n",
    "    if EPOCHS[npl] < TIME_START:\n",
    "        adj = 1 + (TIME_START - EPOCHS[npl])//PERIODS[npl]\n",
    "        EPOCHS[npl] += adj*PERIODS[npl]        \n",
    "        \n",
    "    if EPOCHS[npl] > (TIME_START + PERIODS[npl]):\n",
    "        adj = (EPOCHS[npl] - TIME_START)//PERIODS[npl]\n",
    "        EPOCHS[npl] -= adj*PERIODS[npl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track which quarter each transit falls in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.zeros(len(quarters)+1)\n",
    "\n",
    "thresh[0] = TIME_START\n",
    "\n",
    "for j, q in enumerate(quarters):\n",
    "    if np.isin(q, sc_quarters):\n",
    "        thresh[j+1] = sc.time[sc.quarter == q].max()\n",
    "    if np.isin(q, lc_quarters):\n",
    "        thresh[j+1] = lc.time[lc.quarter == q].max()\n",
    "        \n",
    "thresh[0] -= 1.0\n",
    "thresh[-1] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_quarter = [None]*NPL\n",
    "\n",
    "for npl in range(NPL):\n",
    "    tts = fixed_ephemeris[npl]\n",
    "    transit_quarter[npl] = np.zeros(len(tts), dtype='int')\n",
    "\n",
    "    for j, q in enumerate(quarters):\n",
    "        transit_quarter[npl][(tts >= thresh[j])*(tts<thresh[j+1])] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make transit masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sc is not None:\n",
    "    sc_mask = np.zeros((NPL,len(sc.time)), dtype=\"bool\")\n",
    "    for npl in range(NPL):\n",
    "        sc_mask[npl] = detrend.make_transitmask(sc.time, fixed_transit_times[npl], masksize=1.5)\n",
    "        \n",
    "        \n",
    "if lc is not None:\n",
    "    lc_mask = np.zeros((NPL,len(lc.time)), dtype=\"bool\")\n",
    "    for npl in range(NPL):\n",
    "        lc_mask[npl] = detrend.make_transitmask(lc.time, fixed_transit_times[npl], masksize=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data near transits for each quarter\n",
    "all_time = [None]*18\n",
    "all_flux = [None]*18\n",
    "all_mask = [None]*18\n",
    "all_dtype = [\"none\"]*18\n",
    "\n",
    "lc_flux = []\n",
    "sc_flux = []\n",
    "\n",
    "\n",
    "for q in range(18):\n",
    "    if sc is not None:\n",
    "        if np.isin(q, sc.quarter):\n",
    "            use = (sc_mask.sum(0) != 0)*(sc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 45:\n",
    "                all_time[q] = sc.time[use]\n",
    "                all_flux[q] = sc.flux[use]\n",
    "                all_mask[q] = sc_mask[:,use]\n",
    "                all_dtype[q] = \"short\"\n",
    "\n",
    "                sc_flux.append(sc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_dtype[q] = \"short_no_transits\"\n",
    "\n",
    "    \n",
    "    if lc is not None:\n",
    "        if np.isin(q, lc.quarter):\n",
    "            use = (lc_mask.sum(0) != 0)*(lc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 5:\n",
    "                all_time[q] = lc.time[use]\n",
    "                all_flux[q] = lc.flux[use]\n",
    "                all_mask[q] = lc_mask[:,use]\n",
    "                all_dtype[q] = \"long\"\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_dtype[q] = \"long_no_transits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which quarters have data and transits\n",
    "good = (np.array(all_dtype) == \"short\") + (np.array(all_dtype) == \"long\")\n",
    "quarters = np.arange(18)[good]\n",
    "nq = len(quarters)\n",
    "\n",
    "seasons = np.sort(np.unique(quarters % 4))\n",
    "\n",
    "# make some linear flux arrays (for convenience use laster)\n",
    "try: sc_flux_lin = np.hstack(sc_flux)\n",
    "except: sc_flux_lin = np.array([])\n",
    "    \n",
    "try: lc_flux_lin = np.hstack(lc_flux)\n",
    "except: lc_flux_lin = np.array([])\n",
    "    \n",
    "try:\n",
    "    good_flux = np.hstack([sc_flux_lin, lc_flux_lin])\n",
    "except:\n",
    "    try:\n",
    "        good_flux = np.hstack(sc_flux)\n",
    "    except:\n",
    "        good_flux = np.hstack(lc_flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set oversampling factors and exposure times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = np.zeros(18, dtype=\"int\")\n",
    "texp = np.zeros(18)\n",
    "\n",
    "oversample[np.array(all_dtype)==\"short\"] = 1\n",
    "oversample[np.array(all_dtype)==\"long\"] = 15\n",
    "\n",
    "texp[np.array(all_dtype)==\"short\"] = scit\n",
    "texp[np.array(all_dtype)==\"long\"] = lcit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Legendre polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Legendre polynomials for better orthogonality; \"x\" is in the range (-1,1)\n",
    "Leg0 = []\n",
    "Leg1 = []\n",
    "Leg2 = []\n",
    "Leg3 = []\n",
    "t = []\n",
    "\n",
    "# this assumes a baseline in the range (TIME_START,TIME_END)\n",
    "for npl in range(NPL):    \n",
    "    t.append(fixed_ephemeris[npl])\n",
    "    x = 2*(t[npl]-TIME_START)/(TIME_END-TIME_START) - 1\n",
    "\n",
    "    Leg0.append(np.ones_like(x))\n",
    "    Leg1.append(x.copy())\n",
    "    Leg2.append(0.5*(3*x**2 - 1))\n",
    "    Leg3.append(0.5*(5*x**3 - 3*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a transit model with fixed transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    if npl == PLANET_NO:\n",
    "        print(\"PLANET\", npl)\n",
    "\n",
    "        with pm.Model() as shape_model:\n",
    "            # identify which quarters and seasons have data\n",
    "            which_quarters = np.unique(transit_quarter[npl])\n",
    "            which_seasons = np.unique(which_quarters % 4)\n",
    "\n",
    "            # limb darkening\n",
    "            u = exo.distributions.QuadLimbDark(\"u\", testval=[0.40,0.25])\n",
    "\n",
    "            # radius ratio\n",
    "            rmin, rmax = 1e-4, 0.99\n",
    "\n",
    "\n",
    "            if UMBRELLA == \"non-grazing\":\n",
    "                # parameters\n",
    "                log_r = pm.Uniform(\"log_r\", lower=np.log(rmin), upper=np.log(rmax), testval=np.log(0.1))\n",
    "                r = pm.Deterministic(\"r\", T.exp(log_r))\n",
    "                b = pm.Uniform(\"b\", lower=0, upper=1-r)\n",
    "                g = pm.Deterministic(\"gamma\", (1-b)/r)\n",
    "\n",
    "                # this adjustment term makes samples uniform in the (r,b) plane\n",
    "                adj = pm.Potential(\"adj\", T.log(1-r) + T.log(r))\n",
    "                \n",
    "                # umbrella bias\n",
    "                norm = 1/rmin - 1.5\n",
    "                psi = pm.Potential(\"psi\", T.log(T.switch(T.lt(g,2), g-1, 1.0))/norm)\n",
    "                \n",
    "\n",
    "            elif UMBRELLA == \"transition\":\n",
    "                # parameters\n",
    "                log_r = pm.Uniform(\"log_r\", lower=np.log(rmin), upper=np.log(rmax), testval=np.log(0.1))\n",
    "                r = pm.Deterministic(\"r\", T.exp(log_r))\n",
    "                g = pm.Uniform(\"gamma\", lower=0, upper=2, testval=1.0)\n",
    "                b = pm.Deterministic(\"b\", 1-g*r)\n",
    "\n",
    "                # Jacobian for (r,b) --> (r,gamma)\n",
    "                jac = pm.Potential(\"jac\", T.log(1/r))\n",
    "\n",
    "                # this adjustment term makes samples uniform in the (r,b) plane\n",
    "                adj = pm.Potential(\"adj\", 2*T.log(r) + T.switch(r < 0.5, T.log(2*r), 0.0))\n",
    "                \n",
    "                # umbrella bias\n",
    "                norm = 1.0\n",
    "                psi = pm.Potential(\"psi\", T.log(T.switch(T.lt(g,1), g, 2-g))/norm)\n",
    "\n",
    "\n",
    "            elif UMBRELLA == \"grazing\":\n",
    "                # parameters\n",
    "                g = pm.Uniform(\"gamma\", lower=-0.99, upper=1.0, testval=0.0)\n",
    "                log_lam = pm.Uniform(\"log_lam\", lower=np.log((g+1)*rmin**2), upper=np.log((g+1)*rmax**2), testval=np.log(0.01))\n",
    "                lam = pm.Deterministic(\"lam\", T.exp(log_lam))\n",
    "\n",
    "                r = pm.Deterministic(\"r\", pm.math.sqrt(lam/(g+1)))\n",
    "                log_r = pm.Deterministic(\"log_r\", T.log(r))\n",
    "                b = pm.Deterministic(\"b\", 1-g*r)\n",
    "\n",
    "                # Jacobian for (r,b) --> (lambda,gamma)\n",
    "                jac = pm.Potential(\"jac\", T.log(2 + 2*g_G))\n",
    "\n",
    "                # this adjustment term makes samples uniform in the (r,b) plane\n",
    "                adj = pm.Potential(\"adj\", -T.log(2 + 2*g_G) + 2*T.log(r_G))\n",
    "\n",
    "                # umbrella bias\n",
    "                norm = 1.0\n",
    "                psi = pm.Potential(\"psi\", T.log(T.switch(T.lt(g,0), 1+g, 1-g))/norm)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Umbrella must be 'non-grazing', 'transition', or 'grazing'\")\n",
    "\n",
    "\n",
    "            # enforce desired prior on (r,b)\n",
    "            r_marginal = pm.Potential(\"r_marginal\", -T.log(1+r) - T.log(r))\n",
    "                \n",
    "            # transit duration (sd --> assume that to 3-sigma, initial guess is correct within a factor of 2)\n",
    "            log_dur = pm.Normal(\"log_dur\", mu=np.log(DURS[npl]), sd=np.log(2)/3)\n",
    "            dur = pm.Deterministic(\"dur\", T.exp(log_dur))\n",
    "\n",
    "            # polynomial TTV parameters (coefficients for Legendre polynomials)\n",
    "            C0 = pm.Normal(\"C0\", mu=0, sd=3*omc_scatter[npl])\n",
    "            C1 = pm.Normal(\"C1\", mu=0, sd=3*omc_scatter[npl])\n",
    "\n",
    "            # transit times\n",
    "            transit_times = pm.Deterministic(\"tts\", fixed_transit_times[npl] + C0*Leg0[npl] + C1*Leg1[npl])\n",
    "\n",
    "            # set up stellar model and planetary orbit\n",
    "            starrystar = exo.LimbDarkLightCurve(u)\n",
    "            orbit  = exo.orbits.TTVOrbit(transit_times=[transit_times], \n",
    "                                         transit_inds=[transit_inds[npl][~overlap[npl]]], \n",
    "                                         b=b, \n",
    "                                         duration=dur, \n",
    "                                         ror=r)\n",
    "\n",
    "            # track period, epoch, and stellar density\n",
    "            P = pm.Deterministic(\"P\", orbit.period)\n",
    "            T0 = pm.Deterministic(\"T0\", orbit.t0)\n",
    "            rho = pm.Deterministic(\"rho\", orbit.rho_star)\n",
    "\n",
    "            # mean flux and jitter\n",
    "            flux0 = pm.Normal(\"flux0\", mu=np.median(good_flux), sd=np.std(good_flux), shape=len(which_quarters))\n",
    "            logjit = pm.Normal(\"logjit\", mu=np.var(good_flux), sd=5.0, shape=len(which_quarters))\n",
    "\n",
    "            # now evaluate the model for each quarter\n",
    "            light_curves = [None]*nq\n",
    "            model_flux = [None]*nq\n",
    "            log_like = [None]*nq    \n",
    "\n",
    "            for j, q in enumerate(which_quarters):\n",
    "                # grab time and flux\n",
    "                t_ = all_time[q][all_mask[q][npl]]\n",
    "                f_ = all_flux[q][all_mask[q][npl]]\n",
    "\n",
    "                # calculate light curves\n",
    "                light_curves[j] = starrystar.get_light_curve(orbit=orbit, \n",
    "                                                             r=r, \n",
    "                                                             t=t_,\n",
    "                                                             oversample=oversample[j], \n",
    "                                                             texp=texp[j])\n",
    "\n",
    "                model_flux[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(t_))\n",
    "                pm.Deterministic(\"model_flux_{0}\".format(j), model_flux[j])\n",
    "\n",
    "                # here's thelikelihood\n",
    "                log_like[j] = pm.Normal(\"log_like_{0}\".format(j), \n",
    "                                        mu=model_flux[j], \n",
    "                                        sd=T.sqrt(T.exp(logjit[j]))*T.ones(len(f_)), \n",
    "                                        observed=f_)\n",
    "\n",
    "\n",
    "        with shape_model:\n",
    "            shape_map = shape_model.test_point\n",
    "            shape_map = pmx.optimize(start=shape_map, vars=[flux0, logjit])\n",
    "            shape_map = pmx.optimize(start=shape_map, vars=[C0, C1])\n",
    "            shape_map = pmx.optimize(start=shape_map, vars=[r, b, dur])        \n",
    "\n",
    "        with shape_model:\n",
    "            shape_trace = pm.sample(tune=10000, \n",
    "                                    draws=5000, \n",
    "                                    start=shape_map, \n",
    "                                    chains=2, \n",
    "                                    target_accept=0.95,\n",
    "                                    init=\"adapt_full\")\n",
    "\n",
    "        # select which variables to save (don't save full GP or model traces or \"under the hood\" variables)\n",
    "        varnames = []\n",
    "\n",
    "        for i, key in enumerate(shape_map.keys()):\n",
    "            skip = (\"pred\" in key) + (\"model_flux\" in key) + (\"__\" in key)\n",
    "\n",
    "            if skip == False:\n",
    "                varnames.append(key)\n",
    "\n",
    "        filename = TARGET + \"_{:02d}_shape_\".format(npl) + UMBRELLA[0].capitalize() + \".fits\"\n",
    "\n",
    "        hdulist = io.trace_to_hdulist(shape_trace, varnames, TARGET)\n",
    "        hdulist.writeto(TRACE_DIR + filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOTAL RUNTIME = %.2f min' %((timer()-global_start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
