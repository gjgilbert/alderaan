{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import astropy\n",
    "from   astropy.io import fits as pyfits\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "\n",
    "from alderaan.constants import *\n",
    "import alderaan.io as io\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Simulated\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "TARGET_FILE = PRIMARY_DIR + \"Temp/target_list-sim-BIG-eccentric.txt\"\n",
    "OUTPUT_FILE = PRIMARY_DIR + \"Catalogs/injection_and_recovery_results-BIG-ecc-v1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_DIR = PRIMARY_DIR + \"Traces/\"\n",
    "\n",
    "# locations of trace .fits files (IN) and .csv catalog (OUT)\n",
    "if MISSION == \"Simulated\":\n",
    "    trace_files = glob.glob(TRACE_DIR + \"S*/*_transit_shape.fits\")\n",
    "    csv_outfile = OUTPUT_FILE\n",
    "    \n",
    "\n",
    "# list of targets\n",
    "with open(TARGET_FILE) as tfile:\n",
    "    target_list = tfile.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only read results from objects on target_list\n",
    "sim_ids = []\n",
    "\n",
    "for i, tf in enumerate(trace_files):\n",
    "    sim_ids.append(tf[-25:-19])\n",
    "\n",
    "keep = np.isin(sim_ids, target_list)\n",
    "trace_files = list(np.array(trace_files)[keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dictionary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "results[\"koi_id\"] = []\n",
    "results[\"npl\"] = []\n",
    "\n",
    "results[\"limbdark_1\"] = []\n",
    "results[\"limbdark_2\"] = []\n",
    "\n",
    "results[\"epoch\"]  = []\n",
    "results[\"period\"] = []\n",
    "results[\"prad\"]   = []\n",
    "results[\"impact\"] = []\n",
    "results[\"rho\"]    = []\n",
    "\n",
    "for z in range(4):\n",
    "    results[\"logsw4_{0}\".format(z)] = []\n",
    "    results[\"logw0_{0}\".format(z)] = []\n",
    "    results[\"logq_{0}\".format(z)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tf in enumerate(trace_files):\n",
    "    with pyfits.open(tf) as trace:\n",
    "        header  = trace[0].header\n",
    "        hdulist = pyfits.HDUList(trace)\n",
    "        \n",
    "        print(header[\"TARGET\"])\n",
    "        \n",
    "        \n",
    "        # target/sampler info\n",
    "        KOI_ID = \"K\" + header[\"TARGET\"][1:]\n",
    "        NDRAWS, NPL = trace['RP'].shape\n",
    "    \n",
    "        # stellar parameters\n",
    "        U = trace['U'].data\n",
    "        U1, U2 = U[:,0], U[:,1]\n",
    "    \n",
    "        # planetary parameters\n",
    "        T0   = trace['T0'].data\n",
    "        P    = trace['P'].data\n",
    "        RP   = trace['RP'].data * RSRE    # [R_earth]\n",
    "        B    = trace['B'].data\n",
    "        RHO  = trace['RHO'].data\n",
    "        \n",
    "\n",
    "        # GP parameters\n",
    "        LOGSW4 = np.zeros((NDRAWS,4))\n",
    "        LOGW0  = np.zeros((NDRAWS,4))\n",
    "        LOGQ   = np.zeros((NDRAWS,4))\n",
    "\n",
    "        for z in range(4):\n",
    "            try: LOGSW4[:,z] = trace['LOGSW4_{0}'.format(z)].data\n",
    "            except: pass\n",
    "\n",
    "            try: LOGW0[:,z] = trace['LOGW0_{0}'.format(z)].data\n",
    "            except: pass\n",
    "\n",
    "            try: LOGQ[:,z] = trace['LOGQ_{0}'.format(z)].data\n",
    "            except: pass\n",
    "        \n",
    "        \n",
    "        for npl in range(NPL):\n",
    "            results[\"koi_id\"].append(KOI_ID)\n",
    "            results[\"npl\"].append(NPL)\n",
    "            \n",
    "            results[\"limbdark_1\"].append(np.percentile(U1, 50))\n",
    "            results[\"limbdark_2\"].append(np.percentile(U2, 50))\n",
    "\n",
    "            results[\"epoch\"].append(np.percentile(T0[:,npl], 50))\n",
    "            results[\"period\"].append(np.percentile(P[:,npl], 50))\n",
    "            results[\"prad\"].append(np.percentile(RP[:,npl], 50))\n",
    "            results[\"impact\"].append(np.percentile(B[:,npl], 50))\n",
    "            results[\"rho\"].append(np.percentile(RHO[:,npl], 50))\n",
    "           \n",
    "            #results[\"x_err3m\"].append(np.percentile(X[:,npl],  0.135))\n",
    "            #results[\"x_err2m\"].append(np.percentile(X[:,npl],  2.275))\n",
    "            #results[\"x_err1m\"].append(np.percentile(X[:,npl], 15.865))\n",
    "            #results[\"x_err1p\"].append(np.percentile(X[:,npl], 84.135))\n",
    "            #results[\"x_err2p\"].append(np.percentile(X[:,npl], 97.725))\n",
    "            #results[\"x_err3p\"].append(np.percentile(X[:,npl], 99.865))\n",
    "            \n",
    "            \n",
    "            for z in range(4):\n",
    "                results[\"logsw4_{0}\".format(z)].append(np.median(LOGSW4[:,z]))\n",
    "                results[\"logw0_{0}\".format(z)].append(np.median(LOGSW4[:,z]))\n",
    "                results[\"logq_{0}\".format(z)].append(np.median(LOGSW4[:,z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in results.keys():\n",
    "    if k != \"koi_id\":\n",
    "        results[k] = np.array(results[k], dtype=\"float\")\n",
    "\n",
    "\n",
    "for k in results.keys():\n",
    "    if k != \"koi_id\":\n",
    "        results[k] = np.round(results[k], 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out the results catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITENEW = True\n",
    "if WRITENEW:\n",
    "    with open(csv_outfile, \"w\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(results.keys())\n",
    "        writer.writerows(zip(*results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
