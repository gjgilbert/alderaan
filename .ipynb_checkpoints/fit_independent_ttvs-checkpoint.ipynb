{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import astropy.stats\n",
    "from   astropy.timeseries import LombScargle\n",
    "from   astropy.io import fits as pyfits\n",
    "from   scipy.interpolate import UnivariateSpline\n",
    "from   scipy import stats\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import importlib as imp\n",
    "import warnings\n",
    "import argparse\n",
    "import json\n",
    "from   timeit import default_timer as timer\n",
    "\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "import exoplanet as exo\n",
    "import aesara_theano_fallback.tensor as T\n",
    "from   aesara_theano_fallback import aesara as theano\n",
    "from   celerite2.theano import GaussianProcess\n",
    "from   celerite2.theano import terms as GPterms\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import *\n",
    "from alderaan.Planet import *\n",
    "from alderaan.LiteCurve import *\n",
    "import alderaan.io as io\n",
    "\n",
    "# these lines make progressbar work with SLURM \n",
    "from fastprogress.fastprogress import force_console_behavior\n",
    "master_bar, progress_bar = force_console_behavior()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually set I/O parameters\n",
    "#### User should manually set MISSION, TARGET, PRIMARY_DIR,  and CSV_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where you can hard code in mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K04034\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "\n",
    "USE_HBM = False\n",
    "INTERLACE = False\n",
    "MAX_GROUP_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's where we parse the inputs from an sbatch script\n",
    "try:\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for ALDERAAN transit fiting pipeline\")\n",
    "    parser.add_argument(\"--mission\", default=None, type=str, required=True, \\\n",
    "                        help=\"Mission name\")\n",
    "    parser.add_argument(\"--target\", default=None, type=str, required=True, \\\n",
    "                        help=\"Target name; see ALDERAAN documentation for acceptable formats\")\n",
    "    parser.add_argument(\"--primary_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"Primary directory path for accessing lightcurve data and saving outputs\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    MISSION     = args.mission\n",
    "    TARGET      = args.target\n",
    "    PRIMARY_DIR = args.primary_dir\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "#if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "#if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs    \n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get shape model posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_FILE = TRACE_DIR + TARGET + '_transit_shape.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyfits.open(TRACE_FILE) as trace:\n",
    "    print(trace.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in the fits file with saved traces\n",
    "\n",
    "with pyfits.open(TRACE_FILE) as trace:\n",
    "    header  = trace[0].header\n",
    "    hdulist = pyfits.HDUList(trace)\n",
    "    \n",
    "    NDRAWS, NPL = trace['P'].shape\n",
    "    \n",
    "    # limb darkening parameters\n",
    "    U_LIMBDARK = np.array(trace['U'].data, dtype=\"float\")\n",
    "    Q_LIMBDARK = np.array(trace['Q_LIMBDARK'].data, dtype=\"float\")\n",
    "    \n",
    "    # basis parameters\n",
    "    C0 = np.array(trace['C0'].data, dtype=\"float\")\n",
    "    C1 = np.array(trace['C1'].data, dtype=\"float\")\n",
    "    B  = np.array(trace['B'].data, dtype=\"float\")\n",
    "    LOG_ROR = np.array(trace['LOG_ROR'].data, dtype=\"float\")/np.log(10)\n",
    "    LOG_DUR = np.array(trace['LOG_DUR'].data, dtype=\"float\")/np.log(10)\n",
    "    \n",
    "    # physical parameters\n",
    "    P   = np.array(trace['P'].data, dtype=\"float\")\n",
    "    T0  = np.array(trace['T0'].data, dtype=\"float\")\n",
    "    RP  = np.array(trace['RP'].data, dtype=\"float\")*RSRE\n",
    "    RHO = np.array(trace['RHO'].data, dtype=\"float\")\n",
    "    DUR = np.array(trace['DUR'].data, dtype=\"float\")\n",
    "    \n",
    "    # TTV parameters\n",
    "    TTS = [None]*NPL\n",
    "\n",
    "    for npl in range(NPL):    \n",
    "        TTS[npl] = trace['TTS_{0}'.format(npl)].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set fixed values for star and planet parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Fit a \"cannonical\" model rather than pulling a single trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = [C0, C1, B, LOG_ROR, LOG_DUR]\n",
    "\n",
    "# identify which sample is closest to the median for all parameters\n",
    "dist_sq = np.zeros(NDRAWS)\n",
    "\n",
    "\n",
    "for i, var in enumerate(basis):\n",
    "    for npl in range(NPL):\n",
    "        dist_sq += ((var[:,npl] - np.median(var[:,npl]))/np.std(var[:,npl]))**2\n",
    "        \n",
    "loc = np.argmin(dist_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab star and planet parameters for that sample\n",
    "u = U_LIMBDARK[loc]\n",
    "\n",
    "ror = np.exp(np.array(LOG_ROR[loc], dtype=\"float\"))\n",
    "b   = np.array(B[loc], dtype=\"float\")\n",
    "dur = np.array(DUR[loc], dtype=\"float\")\n",
    "\n",
    "periods = np.array(P[loc], dtype=\"float\")\n",
    "epochs  = np.array(T0[loc], dtype=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in detrended lightcurves and QuickTTV estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detrended lightcurves\n",
    "try:\n",
    "    lc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_lc_detrended.fits')\n",
    "except:\n",
    "    lc = None\n",
    "    \n",
    "try:\n",
    "    sc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_sc_detrended.fits')\n",
    "except:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_inds = []\n",
    "ephemeris = []\n",
    "\n",
    "quick_transit_times = []\n",
    "map_transit_times = []\n",
    "\n",
    "\n",
    "# load Quick TTVs\n",
    "for npl in range(NPL):\n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_quick_ttvs.txt'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    quick_transit_times.append(np.array(data_in[:,1], dtype=\"float\"))\n",
    "    \n",
    "    transit_inds.append(np.array(data_in[:,0], dtype=\"int\"))\n",
    "    ephemeris.append(poly.polyval(transit_inds[npl], [epochs[npl], periods[npl]]))    \n",
    "    \n",
    "\n",
    "# load MAP TTVs\n",
    "for npl in range(NPL):\n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_map_ttvs.txt'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    map_transit_times.append(np.array(data_in[:,1], dtype=\"float\"))    \n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = ephemeris[npl]\n",
    "    yomc_q = (quick_transit_times[npl] - ephemeris[npl])*24*60\n",
    "    yomc_m = (map_transit_times[npl] - ephemeris[npl])*24*60\n",
    "    \n",
    "    axes[npl].plot(xtime, yomc_q, '-', c='C{0}'.format(npl))\n",
    "    axes[npl].plot(xtime, yomc_m, '.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get estimate of ttv amplitude and a reasonable buffer\n",
    "ttv_rms_amp = np.zeros(NPL)\n",
    "ttv_buffer  = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # estimate TTV amplitude\n",
    "    ttv_rms_amp[npl] = astropy.stats.mad_std(map_transit_times[npl] - ephemeris[npl])\n",
    "\n",
    "    # based on scatter in independent times, set threshold so not even one outlier is expected\n",
    "    N   = len(transit_inds[npl])\n",
    "    eta = np.max([3., stats.norm.interval((N-1)/N)[1]])\n",
    "\n",
    "    ttv_buffer[npl] = eta*ttv_rms_amp[npl] + lcit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up noise GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in noise model GP priors\n",
    "gp_percs = []\n",
    "\n",
    "for z in range(4):\n",
    "    try:\n",
    "        fname_in = NOISE_DIR + TARGET + '_shoterm_gp_priors_{0}.txt'.format(z)\n",
    "\n",
    "        with open(fname_in) as infile:\n",
    "            gp_percs.append(json.load(infile))\n",
    "\n",
    "    except:\n",
    "        gp_percs.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_priors = []\n",
    "\n",
    "for z in range(4):\n",
    "    if gp_percs[z] is not None:\n",
    "        # set GP priors baed on outputs of alderaan.detrend_and_estimate_noise\n",
    "        # expected for any season with short cadence data\n",
    "        gpz = {}\n",
    "\n",
    "        for k in gp_percs[z].keys():\n",
    "            if k != \"percentiles\":\n",
    "                perc = np.array(gp_percs[z]['percentiles'])\n",
    "\n",
    "                med = np.array(gp_percs[z][k])[perc == 50.0][0]\n",
    "                err1 = np.array(gp_percs[z][k])[perc == 84.135][0]\n",
    "                err2 = np.array(gp_percs[z][k])[perc == 15.865][0]\n",
    "\n",
    "                dev = np.sqrt((err1-med)**2/2 + (err2-med)**2/2)\n",
    "\n",
    "                gpz[k] = (med, dev)\n",
    "\n",
    "        gp_priors.append(gpz)\n",
    "        \n",
    "    else:\n",
    "        # these are dummy values that effectively create a zero-amplitude kernel\n",
    "        gpz = {}\n",
    "        gpz['logw0'] = [np.log(2*pi/(7*DURS.max()))]\n",
    "        gpz['logSw4'] = [-100.]\n",
    "        gpz['logQ'] = [np.log(1/np.sqrt(2))]\n",
    "        \n",
    "        gp_priors.append(gpz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(4):\n",
    "    gpz = gp_priors[z]\n",
    "    \n",
    "    logS = gpz[\"logSw4\"][0] - 4*gpz[\"logw0\"][0]\n",
    "    \n",
    "    if len(gpz[\"logSw4\"]) == 1:\n",
    "        gp_priors[z][\"logS\"] = np.copy(logS)\n",
    "        \n",
    "    if len(gpz[\"logSw4\"]) == 2:\n",
    "        logS_var = gpz[\"logSw4\"][1]**2 + 16*gpz[\"logw0\"][1]**2\n",
    "        gp_priors[z][\"logS\"] = np.array([logS, np.sqrt(logS_var)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine time baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = []\n",
    "time_max = []\n",
    "\n",
    "if lc is not None:\n",
    "    time_min.append(lc.time.min())\n",
    "    time_max.append(lc.time.max())\n",
    "\n",
    "if sc is not None:\n",
    "    time_min.append(sc.time.min())\n",
    "    time_max.append(sc.time.max())\n",
    "\n",
    "TIME_START = np.min(time_min)\n",
    "TIME_END   = np.max(time_max)\n",
    "\n",
    "if TIME_START < 0:\n",
    "    raise ValueError(\"START TIME [BKJD] is negative...this will cause problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify overlapping transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lc is not None:\n",
    "    lc_quarters = np.unique(lc.quarter)\n",
    "else:\n",
    "    lc_quarters = np.array([])\n",
    "    \n",
    "if sc is not None:\n",
    "    sc_quarters = np.unique(sc.quarter)\n",
    "else:\n",
    "    sc_quarters = np.array([])\n",
    "    \n",
    "    \n",
    "quarters = np.sort(np.hstack([lc_quarters, sc_quarters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = []\n",
    "\n",
    "for i in range(NPL):\n",
    "    overlap.append(np.zeros(len(ephemeris[i]), dtype='bool'))\n",
    "    \n",
    "    for j in range(NPL):\n",
    "        if i != j:\n",
    "            for tt in ephemeris[j]:\n",
    "                overlap[i] += np.abs(ephemeris[i] - tt) < (dur[i] + dur[j] + lcit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit only the most pristine transits\n",
    "\n",
    "### TO DO: correct edge effects near data gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "if lc is not None:\n",
    "    for npl in range(NPL):\n",
    "        print(\"\\nPLANET\", npl)\n",
    "        \n",
    "        results.append([])\n",
    "        \n",
    "        \n",
    "        # number of cadences to use for each transit\n",
    "        num_cad = int(np.ceil(3*dur[npl]/lcit))+2\n",
    "        num_cad += (num_cad % 2)+1\n",
    "                \n",
    "\n",
    "        # establish theano shared variables\n",
    "        time_ = theano.shared((np.arange(num_cad)-num_cad//2)*lcit, \"time_\")\n",
    "        flux_ = theano.shared(np.ones(num_cad, dtype=\"float\"), \"flux_\")\n",
    "        t0_ = theano.shared(0.0, \"t0_\")\n",
    "        \n",
    "        \n",
    "        # build the model\n",
    "        with pm.Model() as model:\n",
    "            BoundedNormal = pm.Bound(pm.Normal, lower=-dur[npl], upper=dur[npl])\n",
    "            \n",
    "            # single parameter model\n",
    "            dt = BoundedNormal(\"dt\", mu=0.0, sd=ttv_rms_amp)\n",
    "            \n",
    "            # nuissance parameters\n",
    "            flux0 = pm.Normal(\"flux0\", mu=np.median(lc.flux), sd=np.std(lc.flux))\n",
    "            logjit = pm.Normal(\"logjit\", mu=np.log(np.var(lc.flux)), sd=2.0)\n",
    "            \n",
    "            \n",
    "            # set up the star, orbit, and light curve\n",
    "            starrystar = exo.LimbDarkLightCurve(u)\n",
    "            \n",
    "            orbit = exo.orbits.KeplerianOrbit(t0 = t0_+dt,\n",
    "                                              period = periods[npl], \n",
    "                                              b = b[npl], \n",
    "                                              ror = ror[npl], \n",
    "                                              duration = dur[npl])\n",
    "            \n",
    "            light_curve = starrystar.get_light_curve(orbit=orbit, \n",
    "                                                     r=ror[npl], \n",
    "                                                     t=time_, \n",
    "                                                     oversample=15, \n",
    "                                                     texp=lcit)\n",
    "            \n",
    "            model_flux = pm.Deterministic(\"model_flux\", pm.math.sum(light_curve, axis=-1) + flux0)\n",
    "            \n",
    "            obs = pm.Normal(\"obs\", mu=model_flux, sd=T.exp(logjit)*T.ones(num_cad), observed=flux_)\n",
    "        \n",
    "        \n",
    "        for i, t0 in enumerate(ephemeris[npl][:20]):\n",
    "            if (t0 > lc.time.min())*(t0 < lc.time.max())*~overlap[npl][i]:\n",
    "                \n",
    "                # find the transit in the data\n",
    "                loc   = np.argmin(np.abs(lc.time-t0))     \n",
    "                start = loc - num_cad//2\n",
    "                end   = loc + num_cad//2 + 1\n",
    "                \n",
    "                # theano shared variables need to have the same shape\n",
    "                if len(lc.time[start:end]) == (num_cad):\n",
    "                    print(i, t0)\n",
    "                    \n",
    "                    time_.set_value(lc.time[start:end])\n",
    "                    flux_.set_value(lc.flux[start:end])\n",
    "                    t0_.set_value(t0)\n",
    "                    \n",
    "                    \n",
    "                    with model:\n",
    "                        trace = pmx.sample(tune=1000, draws=1000, chains=2, target_accept=0.9)\n",
    "                        \n",
    "                    results[npl].append(trace[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for npl in range(NPL):\n",
    "    yomc = []\n",
    "    \n",
    "    for r in results:\n",
    "        yomc.append(np.median(r), np.std(r))\n",
    "\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(yomc, \".\", c=\"C{0}\".format(npl))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting transit times\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tts_chains = []\n",
    "offset_chains = []\n",
    "pop_sd_chains = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    for ng in range(1,tts_group[npl].max()+1):\n",
    "        print(\"\\nPLANET {0}, GROUP {1}\".format(npl,ng))\n",
    "        \n",
    "        # identify which quarters and which_seasons have data\n",
    "        use = tts_group[npl] == ng\n",
    "        \n",
    "        group_quarters = np.unique(transit_quarter[npl][use])\n",
    "        group_seasons = np.unique(group_quarters % 4)\n",
    "                        \n",
    "        print(np.sum(use), \"transits\")\n",
    "        print(\"quarters:\", group_quarters)\n",
    "        \n",
    "        # grab transit times\n",
    "        fixed_inds = np.copy(transit_inds[npl][use])\n",
    "        fixed_inds -= fixed_inds[0]\n",
    "        \n",
    "        fixed_ephem = ephemeris[npl][use]\n",
    "        quick_tts = quick_transit_times[npl][use]\n",
    "          \n",
    "        \n",
    "        # define Legendre polynomials\n",
    "        #x = 2*(fixed_ephem-np.min(fixed_ephem))/(np.max(fixed_ephem)-np.min(fixed_ephem)) - 1\n",
    "                \n",
    "        \n",
    "        # now build the model\n",
    "        with pm.Model() as hbm_model:\n",
    "            # hierarchical (hyper)parameters\n",
    "            if USE_HBM:\n",
    "                raise ValueError(\"Not set up for HBM yet\")\n",
    "            else:\n",
    "                pop_sd = ttv_rms_amp[npl]\n",
    "            \n",
    "            \n",
    "            # transit times\n",
    "            tt_offset = pm.StudentT(\"tt_offset\", nu=2, shape=len(fixed_ephem))\n",
    "            transit_times = pm.Deterministic(\"tts\", fixed_ephem + tt_offset*pop_sd)\n",
    "            \n",
    "                        \n",
    "            # set up stellar model and planetary orbit\n",
    "            starrystar = exo.LimbDarkLightCurve(u)\n",
    "            orbit = exo.orbits.TTVOrbit(transit_times = [transit_times], \n",
    "                                        transit_inds = [fixed_inds], \n",
    "                                        period = [periods[npl]],\n",
    "                                        b = b[npl],\n",
    "                                        ror = ror[npl], \n",
    "                                        duration = dur[npl])\n",
    "\n",
    "            # track period and epoch\n",
    "            T0 = pm.Deterministic(\"T0\", orbit.t0)\n",
    "            P  = pm.Deterministic(\"P\", orbit.period)\n",
    "            \n",
    "            # nuissance parameters (one mean flux; variance by quarter)\n",
    "            flux0 = pm.Normal(\"flux0\", mu=np.median(good_flux), sd=np.std(good_flux), shape=len(group_quarters))\n",
    "            logjit = pm.Normal(\"logjit\", mu=np.var(good_flux), sd=5.0, shape=len(group_quarters))\n",
    "            \n",
    "            # build the GP kernel using a different noise model for each season\n",
    "            kernel = [None]*4\n",
    "\n",
    "            for z in range(4):\n",
    "                gpz = gp_priors[z]\n",
    "                if np.isin(z, group_seasons):\n",
    "                    kernel[z] = GPterms.SHOTerm(S0=T.exp(gpz[\"logSw4\"][0]-4*gpz[\"logw0\"][0]), \n",
    "                                                w0=T.exp(gpz[\"logw0\"][0]), \n",
    "                                                Q=T.exp(gpz[\"logQ\"][0]))\n",
    "            \n",
    "            \n",
    "            # now evaluate the model for each quarter\n",
    "            light_curves = [None]*len(group_quarters)\n",
    "            model_flux = [None]*len(group_quarters)\n",
    "            gp = [None]*len(group_quarters)\n",
    "\n",
    "            \n",
    "            for j, q in enumerate(group_quarters):\n",
    "                # here's the data\n",
    "                t_ = all_time[q][all_mask[q][npl] == ng]\n",
    "                f_ = all_flux[q][all_mask[q][npl] == ng]\n",
    "                \n",
    "                if all_dtype[q] == \"short\":\n",
    "                    oversample = 1\n",
    "                    texp = 1.0*scit\n",
    "                if all_dtype[q] == \"long\":\n",
    "                    oversample = 15\n",
    "                    texp = 1.0*lcit\n",
    "                \n",
    "\n",
    "                # calculate light curves\n",
    "                light_curves[j] = starrystar.get_light_curve(orbit=orbit, r=ror[npl], t=t_, \n",
    "                                                             oversample=oversample, texp=texp)\n",
    "\n",
    "                model_flux[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(t_))\n",
    "                pm.Deterministic(\"model_flux_{0}\".format(j), model_flux[j])\n",
    "\n",
    "                # here's the GP (w/ kernel by season)\n",
    "                gp[j] = GaussianProcess(kernel[q%4], \n",
    "                                        t=t_, \n",
    "                                        diag=T.exp(logjit[j])*T.ones(len(t_)), \n",
    "                                        mean=0.0)\n",
    "\n",
    "                gp[j].marginal(\"gp_{0}\".format(j), observed=f_-model_flux[j])\n",
    "            \n",
    "\n",
    "        with hbm_model:\n",
    "            hbm_map = hbm_model.test_point\n",
    "            hbm_map = pmx.optimize(start=hbm_map, vars=[flux0, logjit])\n",
    "            hbm_map = pmx.optimize(start=hbm_map)\n",
    "  \n",
    "            \n",
    "        # sample from the posterior\n",
    "        with hbm_model:\n",
    "            hbm_trace = pmx.sample(tune=5, draws=3, start=hbm_map, chains=2, target_accept=0.9)\n",
    "        \n",
    "        \n",
    "        # save the results\n",
    "        tts_chains.append(np.copy(np.array(hbm_trace[\"tts\"])))\n",
    "        offset_chains.append(np.copy(np.array(hbm_trace[\"tt_offset\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit overlapping transits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: fit overlapping transits in batches of < MAX_GROUP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_count = 0\n",
    "\n",
    "for npl in range(NPL):\n",
    "    overlap_count += np.sum(tts_group[npl]==-99)\n",
    "    \n",
    "print(overlap_count, \"overlapping transits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overlap_count > 0:\n",
    "    # grab the relevant data\n",
    "    fixed_inds  = []\n",
    "    fixed_ephem = []\n",
    "    quick_tts   = []\n",
    "\n",
    "    overlap_planets = []\n",
    "    overlap_quarters = []\n",
    "\n",
    "    for npl in range(NPL):\n",
    "        use = tts_group[npl] == -99\n",
    "\n",
    "        if np.sum(use) > 0:\n",
    "            fixed_inds.append(transit_inds[npl][use])\n",
    "            fixed_inds[npl] -= fixed_inds[npl][0]\n",
    "\n",
    "            fixed_ephem.append(ephemeris[npl][use])\n",
    "            quick_tts.append(quick_transit_times[npl][use])\n",
    "\n",
    "            overlap_planets.append(npl)\n",
    "            overlap_quarters.append(transit_quarter[npl][use])\n",
    "    \n",
    "    overlap_quarters = np.unique(np.hstack(overlap_quarters))\n",
    "    overlap_seasons = np.unique(overlap_quarters % 4)\n",
    "    \n",
    "    \n",
    "    # now build the model\n",
    "    with pm.Model() as overlap_model:\n",
    "        # hierarchical (hyper)parameters\n",
    "        if USE_HBM:\n",
    "            raise ValueError(\"Not set up for HBM yet\")\n",
    "        else:\n",
    "            pop_sd = ttv_rms_amp\n",
    "            \n",
    "            \n",
    "        # transit times\n",
    "        tt_offset = []\n",
    "        transit_times = []\n",
    "\n",
    "        for i, npl in enumerate(overlap_planets):\n",
    "            tt_offset.append(pm.StudentT('tt_offset_{0}'.format(npl), nu=2, shape=len(fixed_ephem[i])))\n",
    "            transit_times.append(pm.Deterministic('tts_{0}'.format(npl), \n",
    "                                                  fixed_ephem[i] + tt_offset[i]*pop_sd[npl]))\n",
    "\n",
    "\n",
    "        # set up stellar model and planetary orbit\n",
    "        starrystar = exo.LimbDarkLightCurve(u)\n",
    "        orbit = exo.orbits.TTVOrbit(transit_times = transit_times, \n",
    "                                    transit_inds = fixed_inds, \n",
    "                                    period = np.atleast_1d(periods[overlap_planets]),\n",
    "                                    b = b[overlap_planets], \n",
    "                                    ror = ror[overlap_planets], \n",
    "                                    duration = dur[overlap_planets])\n",
    "\n",
    "\n",
    "        # track period and epoch\n",
    "        T0 = pm.Deterministic(\"T0\", orbit.t0)\n",
    "        P  = pm.Deterministic(\"P\", orbit.period)\n",
    "\n",
    "        # nuissance parameters (one mean flux; variance by quarter)\n",
    "        flux0 = pm.Normal(\"flux0\", mu=np.median(good_flux), sd=np.std(good_flux), shape=len(overlap_quarters))\n",
    "        logjit = pm.Normal(\"logjit\", mu=np.var(good_flux), sd=5.0, shape=len(overlap_quarters))\n",
    "\n",
    "\n",
    "        # build the GP kernel using a different noise model for each season\n",
    "        kernel = [None]*4\n",
    "\n",
    "        for z in range(4):\n",
    "            gpz = gp_priors[z]\n",
    "            if np.isin(z, overlap_seasons):\n",
    "                kernel[z] = GPterms.SHOTerm(S0=T.exp(gpz[\"logSw4\"][0]-4*gpz[\"logw0\"][0]), \n",
    "                                            w0=T.exp(gpz[\"logw0\"][0]), \n",
    "                                            Q=T.exp(gpz[\"logQ\"][0]))\n",
    "\n",
    "\n",
    "\n",
    "        # now evaluate the model for each quarter\n",
    "        light_curves = [None]*len(overlap_quarters)\n",
    "        model_flux = [None]*len(overlap_quarters)\n",
    "        gp = [None]*len(overlap_quarters)\n",
    "\n",
    "\n",
    "        for j, q in enumerate(overlap_quarters):\n",
    "            # here's the data\n",
    "            t_ = all_time[q][(all_mask[q] == -99).sum(0) > 0]\n",
    "            f_ = all_flux[q][(all_mask[q] == -99).sum(0) > 0]\n",
    "\n",
    "\n",
    "            if all_dtype[q] == \"short\":\n",
    "                oversample = 1\n",
    "                texp = 1.0*scit\n",
    "            if all_dtype[q] == \"long\":\n",
    "                oversample = 15\n",
    "                texp = 1.0*lcit\n",
    "\n",
    "\n",
    "            # calculate light curves\n",
    "            light_curves[j] = starrystar.get_light_curve(orbit=orbit, r=ror[overlap_planets], t=t_, \n",
    "                                                         oversample=oversample, texp=texp)\n",
    "\n",
    "            model_flux[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(t_))\n",
    "            pm.Deterministic(\"model_flux_{0}\".format(j), model_flux[j])\n",
    "\n",
    "            # here's the GP (w/ kernel by season)\n",
    "            gp[j] = GaussianProcess(kernel[q%4], \n",
    "                                    t=t_, \n",
    "                                    diag=T.exp(logjit[j])*T.ones(len(t_)), \n",
    "                                    mean=0.0)\n",
    "\n",
    "            gp[j].marginal(\"gp_{0}\".format(j), observed=f_-model_flux[j])\n",
    "        \n",
    "        \n",
    "    # optimize the MAP solution\n",
    "    with overlap_model:\n",
    "        overlap_map = pmx.optimize()\n",
    "\n",
    "\n",
    "    # sample from the posterior\n",
    "    with overlap_model:\n",
    "        overlap_trace = pmx.sample(tune=5, draws=3, start=overlap_map, chains=2, target_accept=0.9)\n",
    "\n",
    "\n",
    "    # save the results\n",
    "    for i, npl in enumerate(overlap_planets):\n",
    "        tts_chains.append(np.copy(np.array(overlap_trace['tts_{0}'.format(npl)])))\n",
    "        offset_chains.append(np.copy(np.array(overlap_trace['tt_offset_{0}'.format(npl)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save traces to fits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of ordered pairs (npl,group) to help organize the chains\n",
    "chain_organizer = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    for ng in range(1, 1+tts_group[npl].max()):\n",
    "        chain_organizer.append((npl,ng))\n",
    "\n",
    "if overlap_count > 0:\n",
    "    for i, npl in enumerate(overlap_planets):\n",
    "        chain_organizer.append((npl,-99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make primary HDU\n",
    "primary_hdu = pyfits.PrimaryHDU()\n",
    "header = primary_hdu.header\n",
    "header[\"TARGET\"] = TARGET\n",
    "header[\"U1\"] = u[0]\n",
    "header[\"U2\"] = u[1]\n",
    "\n",
    "for npl in range(NPL):\n",
    "    header[\"ROR_{0}\".format(npl)] = ror[npl]\n",
    "    header[\"B_{0}\".format(npl)]   = b[npl]\n",
    "    header[\"DUR_{0}\".format(npl)] = dur[npl]   \n",
    "    \n",
    "primary_hdu.header = header\n",
    "    \n",
    "# add it to HDU list\n",
    "hbm_hdulist = []\n",
    "hbm_hdulist.append(primary_hdu)\n",
    "\n",
    "\n",
    "# grab all samples from trace\n",
    "for npl in range(NPL):\n",
    "    combo_tts = []\n",
    "    combo_offset = []\n",
    "    combo_groupno = []\n",
    "    \n",
    "    for i, chorg in enumerate(chain_organizer):\n",
    "        if chorg[0] == npl:\n",
    "            combo_tts.append(tts_chains[i])\n",
    "            combo_offset.append(offset_chains[i])\n",
    "            combo_groupno.append(chorg[1]*np.ones(tts_chains[i].shape[1], dtype=\"int\"))\n",
    "        \n",
    "    combo_tts = np.hstack(combo_tts)\n",
    "    combo_offset = np.hstack(combo_offset)\n",
    "    combo_groupno = np.hstack(combo_groupno)\n",
    "\n",
    "    order = np.argsort(np.nanmedian(combo_tts,0))\n",
    "\n",
    "    combo_tts = combo_tts[:,order]\n",
    "    combo_offset = combo_offset[:,order]\n",
    "    combo_groupno = combo_groupno[order]\n",
    "    \n",
    "    \n",
    "    # add to HDUList\n",
    "    hbm_hdulist.append(pyfits.ImageHDU(combo_tts, name='TTS_{0}'.format(npl)))\n",
    "    hbm_hdulist.append(pyfits.ImageHDU(combo_offset, name='OFFSET_{0}'.format(npl)))\n",
    "    hbm_hdulist.append(pyfits.ImageHDU(combo_groupno, name='GROUP_{0}'.format(npl)))\n",
    "\n",
    "    \n",
    "hbm_hdulist = pyfits.HDUList(hbm_hdulist)\n",
    "hbm_hdulist.writeto(TRACE_DIR + TARGET + '_hbm_ttvs.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOTAL RUNTIME = %.2f min' %((timer()-global_start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
