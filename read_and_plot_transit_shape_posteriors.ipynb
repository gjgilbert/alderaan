{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import astropy.stats\n",
    "from   astropy.io import fits as pyfits\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "import corner\n",
    "\n",
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "from alderaan.constants import *\n",
    "import alderaan.io as io\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K01426-01\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "\n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog.csv\"\n",
    "    TRUE_TTV_DIR = PRIMARY_DIR + \"Simulations/TTVs/\"\n",
    "    \n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('K01426-01', 'K01426', 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLANET_NO = int(TARGET[-2:])\n",
    "KOI_ID = TARGET[:6]\n",
    "\n",
    "TARGET, KOI_ID, PLANET_NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs    \n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + KOI_ID + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + KOI_ID + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + KOI_ID + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + KOI_ID + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + KOI_ID + '/'\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in pre-constrained stellar parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data from csv file\n"
     ]
    }
   ],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = KOI_ID\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + KOI_ID[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "RSTAR_TRUE = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "\n",
    "LOGRHO_TRUE = np.array(target_dict['logrho'], dtype='float')[use]\n",
    "LOGRHO_ERR1_TRUE = np.array(target_dict['logrho_err1'], dtype='float')[use]\n",
    "LOGRHO_ERR2_TRUE = np.array(target_dict['logrho_err2'], dtype='float')[use]\n",
    "\n",
    "U1_TRUE = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "U2_TRUE = np.array(target_dict['limbdark_2'], dtype='float')[use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR_TRUE[0] for r in RSTAR_TRUE): RSTAR_TRUE = RSTAR_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_TRUE[0] for r in LOGRHO_TRUE): LOGRHO_TRUE = LOGRHO_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR1_TRUE[0] for r in LOGRHO_ERR1_TRUE): LOGRHO_ERR1_TRUE = LOGRHO_ERR1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR1 in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR2_TRUE[0] for r in LOGRHO_ERR2_TRUE): LOGRHO_ERR2_TRUE = LOGRHO_ERR2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR2 in the csv input file')\n",
    "\n",
    "if all(u == U1_TRUE[0] for u in U1_TRUE): U1_TRUE = U1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "if all(u == U2_TRUE[0] for u in U2_TRUE): U2_TRUE = U2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U2 in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_TRUE = 10**(LOGRHO_TRUE)\n",
    "RHO_ERR1_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR1_TRUE) - RHO_TRUE\n",
    "RHO_ERR2_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR2_TRUE) - RHO_TRUE\n",
    "\n",
    "RHO_ERR_TRUE = np.sqrt(RHO_ERR1_TRUE**2 + RHO_ERR2_TRUE**2)/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get shape model posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_file  = TRACE_DIR + KOI_ID + \"_{:02d}_shape_C.fits\".format(PLANET_NO)\n",
    "\n",
    "trace = {}\n",
    "\n",
    "with pyfits.open(trace_file) as trace_data:\n",
    "    for i in range(1,len(trace_data)):\n",
    "        key = trace_data[i].header[\"EXTNAME\"]\n",
    "\n",
    "        trace[key] = np.squeeze(trace_data[i].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npl = PLANET_NO\n",
    "\n",
    "per = (np.median(trace[\"P\"]), astropy.stats.mad_std(trace[\"P\"]))\n",
    "r   = (np.median(trace[\"R\"]), astropy.stats.mad_std(trace[\"R\"]))\n",
    "b   = (np.median(trace[\"B\"]), astropy.stats.mad_std(trace[\"B\"]))\n",
    "\n",
    "print(\"\\nPLANET {0}\".format(npl))\n",
    "print(\"  period = {:.3f} +/- {:.3f}\\t[days]\".format(per[0],per[1]))\n",
    "print(\"  rp/Rs  = {:.3f} +/- {:.3f}\".format(r[0],r[1]))\n",
    "print(\"  impact = {:.3f} +/- {:.3f}\".format(b[0],b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For simulated data, read in ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MISSION == \"Simulated\":\n",
    "    # Read in the data from csv file\n",
    "    print('Reading in simulated \"ground truth\" data from csv file\\n')\n",
    "\n",
    "    # read in a csv file containing info on targets\n",
    "    csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "    # put these csv data into a dictionary\n",
    "    target_dict = {}\n",
    "    for k in csv_keys: \n",
    "        target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "\n",
    "    # pull relevant quantities and establish GLOBAL variables\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "    use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "    KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "\n",
    "    u1_true = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "    u2_true = np.array(target_dict['limbdark_2'], dtype='float')[use]\n",
    "\n",
    "    P_true  = np.array(target_dict['period'], dtype='float')[use]\n",
    "    T0_true = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "    rp_true = np.array(target_dict['prad'], dtype='float')[use]\n",
    "    b_true  = np.array(target_dict['impact'], dtype='float')[use]\n",
    "    \n",
    "    \n",
    "    # do some consistency checks\n",
    "    if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "    else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "    if all(u == u1_true[0] for u in u1_true): u1_true = u1_true[0]\n",
    "    else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "    if all(u == u2_true[0] for u in u2_true): u2_true = u2_true[0]\n",
    "    else: raise ValueError('There are inconsistencies with U2 in the csv input file')\n",
    "        \n",
    "        \n",
    "    # sort planet truths by period\n",
    "    order = np.argsort(P_true)\n",
    "\n",
    "    P_true  = P_true[order]\n",
    "    T0_true = T0_true[order]\n",
    "    rp_true = rp_true[order]\n",
    "    b_true  = b_true[order]\n",
    "    \n",
    "    \n",
    "    print(\"true radii:\", rp_true)\n",
    "    print(\"true impact:\", b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make corner plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([trace[\"LOG_R\"], trace[\"B\"], trace[\"LOG_DUR\"]]).T\n",
    "labels = [r'$\\log r$', r'$b$', r'$\\log T$']\n",
    "\n",
    "c = ChainConsumer()\n",
    "c.add_chain(data, weights=trace[\"WEIGHTS\"], parameters=labels)\n",
    "fig = c.plotter.plot()\n",
    "fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate chains by this amount\n",
    "upsample = 100\n",
    "\n",
    "# preconstrained density (e.g. from GaiaDR2/Berger+2020 isochrones)\n",
    "rho_obs = (RHO_TRUE, RHO_ERR_TRUE)\n",
    "\n",
    "\n",
    "\n",
    "# draw samples in (e,w) and calculate log(weight) for a given Rayleigh scale\n",
    "rho_circ = np.repeat(trace[\"RHO\"], upsample)\n",
    "    \n",
    "sig_e = 0.0355\n",
    "esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*len(rho_circ)).reshape(2,-1)\n",
    "ecc = np.sqrt(esinw**2 + ecosw**2)\n",
    "omega = np.arctan2(esinw, ecosw)\n",
    "    \n",
    "while np.any(ecc >= 1):\n",
    "    print(\"redrawing\", np.sum(ecc>=1))\n",
    "    esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*np.sum(ecc>=1)).reshape(2,-1)\n",
    "    omega[ecc>=1] = np.arctan2(esinw, ecosw)\n",
    "    ecc[ecc>=1] = np.sqrt(esinw**2 + ecosw**2)\n",
    "    \n",
    "    \n",
    "g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "rho = rho_circ / g ** 3\n",
    "\n",
    "log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "w_rho = np.exp(log_weight - np.max(log_weight))\n",
    "w_rho /= np.sum(w_rho)\n",
    "\n",
    "\n",
    "log_r = np.repeat(trace[\"LOG_R\"], upsample)\n",
    "b     = np.repeat(trace[\"B\"], upsample)\n",
    "log_T = np.repeat(trace[\"LOG_DUR\"], upsample)\n",
    "w_umb = np.repeat(trace[\"WEIGHTS\"], upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([log_r, b, log_T]).T\n",
    "labels = [r'$\\log r$', r'$b$', r'$\\log T$']\n",
    "\n",
    "c = ChainConsumer()\n",
    "c.add_chain(data, weights=w_rho*w_umb, parameters=labels)\n",
    "fig = c.plotter.plot()\n",
    "fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try reweighting for eccentricity posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "# For some reason, ChainConsumer can't find Latex unless I make a matplotlib plot first\n",
    "# This is a workaround -- I'll find a real solution later\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.plot(np.linspace(0,1,2), \"k:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate chains by this amount\n",
    "upsample = 1000 \n",
    "\n",
    "# preconstrained density (e.g. from GaiaDR2/Berger+2020 isochrones)\n",
    "rho_obs = (RHO_TRUE, RHO_ERR_TRUE)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    \n",
    "    # draw samples in (e,w) and calculate log(weight)\n",
    "    rho_circ = np.repeat(RHO[:,npl], upsample)\n",
    "    ecc = np.random.uniform(0, 1, len(rho_circ))\n",
    "    omega = np.random.uniform(-0.5*np.pi, 1.5*np.pi, len(rho_circ))\n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_circ / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "    weight = np.exp(log_weight - np.max(log_weight))\n",
    "    \n",
    "    \n",
    "    # now plot it\n",
    "    c = ChainConsumer()\n",
    "    c.add_chain(np.vstack((ecc,omega*180/pi)).T, weights=weight,parameters=[r'$e$',r'$\\omega$'])\n",
    "    fig = c.plotter.plot()\n",
    "    fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we set priors on eccentricity to recover inclination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate chains by this amount\n",
    "upsample = 1000 \n",
    "\n",
    "# preconstrained density (e.g. from GaiaDR2/Berger+2020 isochrones)\n",
    "rho_obs = (RHO_TRUE, RHO_ERR_TRUE)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    \n",
    "    # draw samples in (e,w) and calculate log(weight) for a given Rayleigh scale\n",
    "    rho_circ = np.repeat(RHO[:,npl], upsample)\n",
    "    \n",
    "    sig_e = 0.0355\n",
    "    esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*len(rho_circ)).reshape(2,-1)\n",
    "    ecc = np.sqrt(esinw**2 + ecosw**2)\n",
    "    omega = np.arctan2(esinw, ecosw)\n",
    "    \n",
    "    while np.any(ecc >= 1):\n",
    "        print(\"redrawing\", np.sum(ecc>=1))\n",
    "        esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*np.sum(ecc>=1)).reshape(2,-1)\n",
    "        omega[ecc>=1] = np.arctan2(esinw, ecosw)\n",
    "        ecc[ecc>=1] = np.sqrt(esinw**2 + ecosw**2)\n",
    "    \n",
    "    \n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_circ / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "    weight = np.exp(log_weight - np.max(log_weight))\n",
    "\n",
    "    # upsample impact parameter\n",
    "    b = np.repeat(B[:,npl], upsample)\n",
    "    \n",
    "    # now plot it\n",
    "    c = ChainConsumer()\n",
    "    c.add_chain(np.vstack((ecc,b)).T, weights=weight,parameters=[r'$e$',r'$b$'])\n",
    "    fig = c.plotter.plot()\n",
    "    fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
