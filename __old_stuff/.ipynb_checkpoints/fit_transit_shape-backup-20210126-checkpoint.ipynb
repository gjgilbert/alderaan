{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit transit shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize as op\n",
    "import scipy.signal as sig\n",
    "from   scipy import stats\n",
    "from   scipy import fftpack\n",
    "from   scipy import ndimage\n",
    "import astropy\n",
    "from   astropy.io import fits as pyfits\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import importlib as imp\n",
    "import glob\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "import progressbar\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import lightkurve as lk\n",
    "import exoplanet as exo\n",
    "import theano.tensor as T\n",
    "import pymc3 as pm\n",
    "import corner\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import *\n",
    "from alderaan.Planet import *\n",
    "from alderaan.LiteCurve import *\n",
    "import alderaan.io as io\n",
    "import alderaan.detrend as detrend\n",
    "import alderaan.noise as noise\n",
    "from alderaan.omc import omc_model\n",
    "from alderaan.omc import choose_omc_model\n",
    "\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually set I/O parameters\n",
    "#### User should manually set MISSION, TARGET, PRIMARY_DIR,  and CSV_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K02086\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "CSV_FILE    = '/Users/research/projects/alderaan/Catalogs/clean_kepler_catalog.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --mission MISSION --target TARGET\n",
      "                             --primary_dir PRIMARY_DIR --csv_file CSV_FILE\n",
      "ipykernel_launcher.py: error: the following arguments are required: --mission, --target, --primary_dir, --csv_file\n"
     ]
    }
   ],
   "source": [
    "# here's where we parse the inputs\n",
    "try:\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for ALDERAAN transit fiting pipeline\")\n",
    "    parser.add_argument(\"--mission\", default=None, type=str, required=True, \\\n",
    "                        help=\"Mission name\")\n",
    "    parser.add_argument(\"--target\", default=None, type=str, required=True, \\\n",
    "                        help=\"Target name; see ALDERAAN documentation for acceptable formats\")\n",
    "    parser.add_argument(\"--primary_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"Primary directory path for accessing lightcurve data and saving outputs\")\n",
    "    parser.add_argument(\"--csv_file\", default=None, type=str, required=True, \\\n",
    "                        help=\"Path to .csv file containing input planetary parameters\")\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    MISSION     = args.mission\n",
    "    TARGET      = args.target\n",
    "    PRIMARY_DIR = args.primary_dir\n",
    "    CSV_FILE    = args.csv_file\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs\n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in planet and stellar parameters from Kepler DR25 & Gaia DR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data from csv file\n"
     ]
    }
   ],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    # pull relevant quantities and establish GLOBAL variables\n",
    "    use = np.array(target_dict['koi_id']) == TARGET\n",
    "\n",
    "    KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "    NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "    \n",
    "    RSTAR = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "    RSTAR_ERR1 = np.array(target_dict['rstar_err1'],  dtype='float')[use]\n",
    "    RSTAR_ERR2 = np.array(target_dict['rstar_err2'],  dtype='float')[use]\n",
    "    \n",
    "    MSTAR  = np.array(target_dict['mstar'], dtype='float')[use]\n",
    "    MSTAR_ERR1 = np.array(target_dict['mstar_err1'],  dtype='float')[use]\n",
    "    MSTAR_ERR2 = np.array(target_dict['mstar_err2'],  dtype='float')[use]\n",
    "\n",
    "    DEPTHS = np.array(target_dict['depth'], dtype='float')[use]*1e-6    # [ppm] --> []\n",
    "    DURS   = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]\n",
    "\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    # pull relevant quantities and establish GLOBAL variables\n",
    "    use = np.array(target_dict['id_sim']) == TARGET\n",
    "\n",
    "    KIC    = np.array(target_dict['kic'], dtype='int')[use]\n",
    "    NPL    = np.array(target_dict['npl'], dtype='int')[use]\n",
    "    RSTAR  = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "    MSTAR  = np.array(target_dict['mstar'], dtype='float')[use]\n",
    "\n",
    "    DEPTHS = np.array(target_dict['depth'], dtype='float')[use]*1e-6     # [ppm] --> []\n",
    "    DURS   = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]\n",
    "        \n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR[0] for r in RSTAR): RSTAR = RSTAR[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == RSTAR_ERR1[0] for r in RSTAR_ERR1): RSTAR_ERR1 = RSTAR_ERR1[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR_ERR1 in the csv input file')\n",
    "    \n",
    "if all(r == RSTAR_ERR2[0] for r in RSTAR_ERR2): RSTAR_ERR2 = RSTAR_ERR2[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR_ERR2 in the csv input file')  \n",
    "    \n",
    "if all(m == MSTAR[0] for m in MSTAR): MSTAR = MSTAR[0]\n",
    "else: raise ValueError('There are inconsistencies with MSTAR in the csv input file')\n",
    "\n",
    "if all(m == MSTAR_ERR1[0] for m in MSTAR_ERR1): MSTAR_ERR1 = MSTAR_ERR1[0]\n",
    "else: raise ValueError('There are inconsistencies with MSTAR_ERR1 in the csv input file')\n",
    "    \n",
    "if all(m == MSTAR_ERR2[0] for m in MSTAR_ERR2): MSTAR_ERR2 = MSTAR_ERR2[0]\n",
    "else: raise ValueError('There are inconsistencies with MSTAR_ERR2 in the csv input file')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combline stellar radius/mass uncertainties\n",
    "MSTAR_ERR = np.sqrt(MSTAR_ERR1**2 + MSTAR_ERR2**2)/np.sqrt(2)\n",
    "RSTAR_ERR = np.sqrt(RSTAR_ERR1**2 + RSTAR_ERR2**2)/np.sqrt(2)\n",
    "\n",
    "#initialize with solar limb darkening coefficients from EXOFAST\n",
    "U1 = 0.39940842\n",
    "U2 = 0.26477268\n",
    "UCOEFFS = [U1, U2]\n",
    "\n",
    "# initialize impact parameter and radius arrays\n",
    "RADII = np.sqrt(DEPTHS)*RSTAR\n",
    "IMPACTS = 0.67*np.ones(NPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in detrended lightcurves and initial transit time estimates\n",
    "#### The data can be generated by running the script \"detrend_and_estimate_noise.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detrended lightcurves\n",
    "try:\n",
    "    lc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_lc_detrended.fits')\n",
    "except:\n",
    "    lc = None\n",
    "    \n",
    "try:\n",
    "    sc = io.load_detrended_lightcurve(DLC_DIR + TARGET + '_sc_detrended.fits')\n",
    "except:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in QuickTTV estimates and calculate linear ephemeris for each planet\n",
    "EPOCHS  = np.zeros(NPL)\n",
    "PERIODS = np.zeros(NPL)\n",
    "\n",
    "transit_inds = []\n",
    "indep_transit_times = []\n",
    "indep_ephemeris = []\n",
    "\n",
    "FIXED_EPHEMERIS = []\n",
    "\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # read in predetermined transit times\n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_map_indep_ttvs.txt'\n",
    "    data_in  = np.genfromtxt(fname_in)\n",
    "    \n",
    "    transit_inds.append(data_in[:,0])\n",
    "    indep_transit_times.append(data_in[:,1])\n",
    "    \n",
    "    fname_in = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_fixed_ephemeris.txt'\n",
    "    data_in  = np.genfromtxt(fname_in)  \n",
    "\n",
    "    # do a quick fit to get a linear ephemeris\n",
    "    pfit = np.polyfit(transit_inds[npl], indep_transit_times[npl], 1)\n",
    "    \n",
    "    indep_ephemeris.append(np.polyval(pfit, transit_inds[npl]))\n",
    "    FIXED_EPHEMERIS.append(np.polyval(pfit, transit_inds[npl]))\n",
    "    \n",
    "    EPOCHS[npl] = pfit[1]\n",
    "    PERIODS[npl] = pfit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a polynomial + sinusoid to the OMC to determine best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_omc = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    print(\"PLANET\", npl)\n",
    "    \n",
    "    # calculate OMC\n",
    "    xtime = np.copy(indep_ephemeris[npl])\n",
    "    yomc = indep_transit_times[npl] - indep_ephemeris[npl]\n",
    "\n",
    "    # select best-fit parameters for OMC model; ordered as [N, K, c0, c1,...c_n, A1, B1, f1, A2, B2, f2]\n",
    "    best_omc.append(choose_omc_model(xtime, yomc))\n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(NPL, figsize=(12,8))\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = indep_transit_times[npl]\n",
    "    yomc  = (indep_transit_times[npl] - indep_ephemeris[npl])\n",
    "    \n",
    "    N = int(best_omc[npl][0])\n",
    "    K = int(best_omc[npl][1])\n",
    "    theta = best_omc[npl][2:]\n",
    "\n",
    "    axes[npl].plot(xtime, yomc*24*60, '.', c='grey')\n",
    "    axes[npl].plot(xtime, omc_model(theta, xtime, N, K)*24*60, color=\"C{0}\".format(npl), lw=3)\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + \"_ttvs_initial.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine TTV initialization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get estimate of ttv amplitdue and a reasonable buffer\n",
    "ttv_amps   = np.zeros(NPL)\n",
    "ttv_buffer = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # estimate TTV amplitude\n",
    "    ttv_amps[npl] = astropy.stats.mad_std(indep_transit_times[npl] - FIXED_EPHEMERIS[npl])\n",
    "\n",
    "    # based on scatter in independent times, set threshold so not even one outlier is expected\n",
    "    N   = len(transit_inds[npl])\n",
    "    eta = np.max([3., stats.norm.interval((N-1)/N)[1]])\n",
    "\n",
    "    ttv_buffer[npl] = eta*ttv_amps[npl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the time baseline\n",
    "time_min = []\n",
    "time_max = []\n",
    "\n",
    "try:\n",
    "    time_min.append(sc.time.min())\n",
    "    time_max.append(sc.time.max()) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    time_min.append(lc.time.min())\n",
    "    time_max.append(lc.time.max())     \n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "TIME_START = np.min(time_min) - 0.5*PERIODS.max()\n",
    "TIME_END   = np.max(time_max) + 0.5*PERIODS.max()\n",
    "\n",
    "\n",
    "# put epochs in range (TIME_START, TIME_START + PERIOD)\n",
    "for npl in range(NPL):\n",
    "    if EPOCHS[npl] < TIME_START:\n",
    "        adj = 1 + (TIME_START - EPOCHS[npl])//PERIODS[npl]\n",
    "        EPOCHS[npl] += adj*PERIODS[npl]        \n",
    "        \n",
    "    if EPOCHS[npl] > (TIME_START + PERIODS[npl]):\n",
    "        adj = (EPOCHS[npl] - TIME_START)//PERIODS[npl]\n",
    "        EPOCHS[npl] -= adj*PERIODS[npl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the limits of the sinusoidal frequencies\n",
    "fttv_min = (3/2)/(TIME_END-TIME_START)*np.ones(NPL)\n",
    "fttv_max = 1/(4*PERIODS)\n",
    "\n",
    "# here's the beat frequency (minimum delta_f detectable)\n",
    "fbeat = 1/(TIME_END-TIME_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab TTV frequencies from OMC best fit model\n",
    "fttv1_ls = np.zeros(NPL)\n",
    "fttv2_ls = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    K = int(best_omc[npl][1])\n",
    "    \n",
    "    if K == 0:\n",
    "        fttv1_ls[npl] = None\n",
    "        fttv2_ls[npl] = None\n",
    "        \n",
    "    elif K == 1:\n",
    "        fttv1_ls[npl] = best_omc[npl][-1]\n",
    "        fttv2_ls[npl] = None\n",
    "        \n",
    "    elif K == 2:\n",
    "        fttv1_ls[npl] = best_omc[npl][-4]\n",
    "        fttv2_ls[npl] = best_omc[npl][-1]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model is expected to have 0-2 TTV frequencies\")\n",
    "        \n",
    "        \n",
    "# scale up ttv_amps to make it modestly less informative\n",
    "ttv_amps *= np.sqrt(2)\n",
    "\n",
    "\n",
    "# set initialization points for {A1, B1, A2, B2}\n",
    "A1_testval = np.zeros(NPL)\n",
    "B1_testval = np.zeros(NPL)\n",
    "A2_testval = np.zeros(NPL)\n",
    "B2_testval = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    K = int(best_omc[npl][1])\n",
    "\n",
    "    if K == 1:\n",
    "        A1_testval[npl] = best_omc[npl][-3]/ttv_amps[npl]\n",
    "        B1_testval[npl] = best_omc[npl][-2]/ttv_amps[npl]\n",
    "        \n",
    "    elif K == 2:\n",
    "        A1_testval[npl] = best_omc[npl][-6]/ttv_amps[npl]\n",
    "        B1_testval[npl] = best_omc[npl][-5]/ttv_amps[npl]\n",
    "        A2_testval[npl] = best_omc[npl][-3]/ttv_amps[npl]\n",
    "        B2_testval[npl] = best_omc[npl][-2]/ttv_amps[npl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_f for one TTV frequency\n",
    "log_df_mu = np.ones(NPL)*np.nan\n",
    "log_df_sd = np.ones(NPL)*np.nan\n",
    "\n",
    "\n",
    "# fplus, fminus for two TTV frequencies\n",
    "fplus_init = np.ones(NPL)*np.nan\n",
    "fminus_init = np.ones(NPL)*np.nan\n",
    "\n",
    "fplus_lim  = np.ones((NPL,2))*np.nan\n",
    "fminus_lim = np.ones((NPL,2))*np.nan\n",
    "\n",
    "log_df_plus_mu = np.ones(NPL)*np.nan\n",
    "log_df_plus_sd = np.ones(NPL)*np.nan\n",
    "\n",
    "log_df_minus_mu = np.ones(NPL)*np.nan\n",
    "log_df_minus_sd = np.ones(NPL)*np.nan\n",
    "\n",
    "\n",
    "for npl in range(NPL):\n",
    "    n_ttv_terms = 2 - (int(np.isnan(fttv1_ls[npl]))+int(np.isnan(fttv2_ls[npl])))\n",
    "    \n",
    "    if n_ttv_terms == 1:\n",
    "        # transform to log_delta_f space\n",
    "        df_loc = fttv1_ls[npl] - fttv_min[npl]\n",
    "        df_scale = (fttv_max[npl]/fttv_min[npl])**0.25\n",
    "        \n",
    "        log_df_mu[npl] = np.log(df_loc)\n",
    "        log_df_sd[npl] = np.log(df_scale)\n",
    "        \n",
    "        \n",
    "    if n_ttv_terms == 2:\n",
    "        # set limits and initial guesses for fplus & fminus\n",
    "        fplus_lim[npl]  = 2*fttv_min[npl], 2*fttv_max[npl]\n",
    "        fminus_lim[npl] = fbeat, fttv_max[npl]-fttv_min[npl] \n",
    "        \n",
    "        fplus_init[npl]  = fttv1_ls[npl] + fttv2_ls[npl]\n",
    "        fminus_init[npl] = np.abs(fttv1_ls[npl] - fttv2_ls[npl])\n",
    "        \n",
    "        \n",
    "        # transform to log_delta_f space\n",
    "        df_plus_loc = fplus_init[npl] - fplus_lim[npl,0]\n",
    "        df_plus_scale = (fplus_lim[npl,1]/fplus_lim[npl,0])**0.25\n",
    "        \n",
    "        df_minus_loc = fminus_init[npl] - fminus_lim[npl,0]\n",
    "        df_minus_scale = (fminus_lim[npl,1]/fminus_lim[npl,0])**0.25\n",
    "        \n",
    "        log_df_plus_mu[npl] = np.log(df_plus_loc)\n",
    "        log_df_plus_sd[npl] = np.log(df_plus_scale)\n",
    "        \n",
    "        log_df_minus_mu[npl] = np.log(df_minus_loc)\n",
    "        log_df_minus_sd[npl] = np.log(df_minus_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyorder = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    polyorder.append(int(best_omc[npl][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in noise model GP priors\n",
    "gp_percs = []\n",
    "\n",
    "for z in range(4):\n",
    "    try:\n",
    "        fname_in = NOISE_DIR + TARGET + '_shoterm_gp_priors_{0}.txt'.format(z)\n",
    "\n",
    "        with open(fname_in) as infile:\n",
    "            gp_percs.append(json.load(infile))\n",
    "\n",
    "    except:\n",
    "        gp_percs.append(None)\n",
    "\n",
    "# Read in quarter-by-quarter variances\n",
    "var_by_quarter = np.genfromtxt(NOISE_DIR + TARGET + '_var_by_quarter.txt')[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logSw4 = []\n",
    "for z in range(4):\n",
    "    try:\n",
    "        perc = np.array(gp_percs[z][\"percentiles\"])\n",
    "        \n",
    "        all_logSw4.append(np.array(gp_percs[z][\"logSw4\"])[perc == 50.0][0])\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_priors = []\n",
    "\n",
    "for z in range(4):\n",
    "    if gp_percs[z] is not None:\n",
    "        # set GP priors baed on outputs of alderaan.detrend_and_estimate_noise\n",
    "        # expected for any season with short cadence data\n",
    "        gpi = {}\n",
    "\n",
    "        for k in gp_percs[z].keys():\n",
    "            if k != \"percentiles\":\n",
    "                perc = np.array(gp_percs[z]['percentiles'])\n",
    "\n",
    "                med = np.array(gp_percs[z][k])[perc == 50.0][0]\n",
    "                err1 = np.array(gp_percs[z][k])[perc == 84.1][0]\n",
    "                err2 = np.array(gp_percs[z][k])[perc == 15.9][0]\n",
    "\n",
    "                dev = np.sqrt((err1-med)**2/2 + (err2-med)**2/2)\n",
    "\n",
    "                gpi[k] = (med, dev)\n",
    "\n",
    "        gp_priors.append(gpi)\n",
    "        \n",
    "    else:\n",
    "        # these are dummy values that effectively create a zero-amplitude kernel\n",
    "        # expected for any season with only long cadence data\n",
    "        gpi = {}\n",
    "        gpi['logw0'] = [np.log(2*pi/(5*DURS.max()))]\n",
    "        gpi['logSw4'] = [-40.]\n",
    "        gpi['logQ'] = [1/np.sqrt(2)]\n",
    "        \n",
    "        gp_priors.append(gpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the relevant data and starting transit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data near transits for each quarter\n",
    "all_time = []\n",
    "all_flux = []\n",
    "all_error = []\n",
    "all_dtype = []\n",
    "\n",
    "lc_flux = []\n",
    "sc_flux = []\n",
    "\n",
    "if sc is not None:\n",
    "    for q in range(18):\n",
    "        if np.isin(q, sc.quarter)*np.isin(q, lc.quarter):\n",
    "            raise ValueError(\"Double counting data in both short and long cadence\")\n",
    "\n",
    "\n",
    "        elif np.isin(q, sc.quarter):\n",
    "            use = (sc.mask.sum(axis=0) > 0)*(sc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 45:\n",
    "                all_time.append(sc.time[use])\n",
    "                all_flux.append(sc.flux[use])\n",
    "                all_error.append(sc.error[use])\n",
    "                all_dtype.append('short')\n",
    "\n",
    "                sc_flux.append(sc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        elif np.isin(q, lc.quarter):\n",
    "            use = (lc.mask.sum(axis=0) > 0)*(lc.quarter == q)\n",
    "            \n",
    "            if np.sum(use) > 3:\n",
    "                all_time.append(lc.time[use])\n",
    "                all_flux.append(lc.flux[use])\n",
    "                all_error.append(lc.error[use])\n",
    "                all_dtype.append('long')\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_time.append(None)\n",
    "            all_flux.append(None)\n",
    "            all_error.append(None)\n",
    "            all_dtype.append('none')\n",
    "            \n",
    "else:\n",
    "    for q in range(18):\n",
    "        if np.isin(q, lc.quarter):\n",
    "            use = (lc.mask.sum(axis=0) > 0)*(lc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 3:\n",
    "                all_time.append(lc.time[use])\n",
    "                all_flux.append(lc.flux[use])\n",
    "                all_error.append(lc.error[use])\n",
    "                all_dtype.append('long')\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_time.append(None)\n",
    "                all_flux.append(None)\n",
    "                all_error.append(None)\n",
    "                all_dtype.append('none')\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_time.append(None)\n",
    "            all_flux.append(None)\n",
    "            all_error.append(None)\n",
    "            all_dtype.append('none')\n",
    "\n",
    "\n",
    "\n",
    "# check which quarters have data\n",
    "good = (np.array(all_dtype) == 'short') + (np.array(all_dtype) == 'long')\n",
    "quarters = np.arange(18)[good]\n",
    "nq = len(quarters)\n",
    "\n",
    "\n",
    "# make some linear flux arrays (for convenience use laster)\n",
    "try: sc_flux_lin = np.hstack(sc_flux)\n",
    "except: sc_flux_lin = np.array([])\n",
    "    \n",
    "try: lc_flux_lin = np.hstack(lc_flux)\n",
    "except: lc_flux_lin = np.array([])\n",
    "    \n",
    "try:\n",
    "    good_flux = np.hstack([sc_flux_lin, lc_flux_lin])\n",
    "except:\n",
    "    try:\n",
    "        good_flux = np.hstack(sc_flux)\n",
    "    except:\n",
    "        good_flux = np.hstack(lc_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Legendre polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Legendre polynomials for better orthogonality; \"x\" is in the range (-1,1)\n",
    "Leg0 = []\n",
    "Leg1 = []\n",
    "Leg2 = []\n",
    "Leg3 = []\n",
    "Leg4 = []\n",
    "Leg5 = []\n",
    "t = []\n",
    "\n",
    "# this assumes a baseline in the range (TIME_START,TIME_END)\n",
    "for npl in range(NPL):    \n",
    "    t.append(FIXED_EPHEMERIS[npl])\n",
    "    x = 2*(t[npl]-TIME_START)/(TIME_END-TIME_START) - 1\n",
    "\n",
    "    Leg0.append(np.ones_like(x))\n",
    "    Leg1.append(x.copy())\n",
    "    Leg2.append(0.5*(3*x**2 - 1))\n",
    "    Leg3.append(0.5*(5*x**3 - 3*x))\n",
    "    Leg4.append(0.125*(35*x**4 - 30*x**2 + 3))\n",
    "    Leg5.append(0.125*(63*x**5 - 70*x**3 + 15*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Fit a transit model with both polynomial and sinusoidal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n(3) Fitting SINUSOIDAL TTV model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sin_model:\n",
    "    # stellar parameters (limb darkening using Kipping 2013)\n",
    "    u = exo.distributions.QuadLimbDark('u', testval=np.array([U1,U2]))\n",
    "\n",
    "    Rstar = pm.Bound(pm.Normal, lower=RSTAR-3*RSTAR_ERR, upper=RSTAR+3*RSTAR_ERR)('Rstar', mu=RSTAR, sd=RSTAR_ERR)\n",
    "    Mstar = pm.Bound(pm.Normal, lower=MSTAR-3*MSTAR_ERR, upper=MSTAR+3*MSTAR_ERR)('Mstar', mu=MSTAR, sd=MSTAR_ERR)\n",
    "    \n",
    "    # planetary parameters (impact parameter using Espinoza 2018)\n",
    "    logr = pm.Uniform('logr', lower=np.log(0.0003), upper=np.log(0.3), testval=np.log(RADII), shape=NPL)\n",
    "    rp   = pm.Deterministic('rp', T.exp(logr))\n",
    "    b    = exo.distributions.ImpactParameter('b', ror=rp/Rstar, testval=IMPACTS, shape=NPL)\n",
    "\n",
    "    \n",
    "    # polynomial TTV parameters    \n",
    "    C0 = pm.Normal('C0', mu=0.0, sd=ttv_amps, shape=NPL)\n",
    "    C1 = pm.Normal('C1', mu=0.0, sd=ttv_amps, shape=NPL)\n",
    "    C2 = []\n",
    "    C3 = []\n",
    "    C4 = []\n",
    "    C5 = []\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        if polyorder[npl] >= 2:\n",
    "            C2.append(pm.Normal('C2_{0}'.format(npl), mu=0.0, sd=ttv_amps[npl], testval=0))\n",
    "        else:\n",
    "            C2.append(0.)\n",
    "            \n",
    "        if polyorder[npl] >= 3:\n",
    "            C3.append(pm.Normal('C3_{0}'.format(npl), mu=0.0, sd=ttv_amps[npl], testval=0))\n",
    "        else:\n",
    "            C3.append(0.)\n",
    "    \n",
    "        if polyorder[npl] >= 4:\n",
    "            C4.append(pm.Normal('C4_{0}'.format(npl), mu=0.0, sd=ttv_amps[npl], testval=0))\n",
    "        else:\n",
    "            C4.append(0.)\n",
    "            \n",
    "        if polyorder[npl] >= 5:\n",
    "            C5.append(pm.Normal('C5_{0}'.format(npl), mu=0.0, sd=ttv_amps[npl], testval=0))\n",
    "        else:\n",
    "            C5.append(0.)\n",
    "            \n",
    "    # sinusoidal TTV parameters\n",
    "    log_df = []\n",
    "    log_df_plus  = []\n",
    "    log_df_minus = []\n",
    "    \n",
    "    fplus  = []\n",
    "    fminus = []\n",
    "    fttv1  = []\n",
    "    fttv2  = []\n",
    "    \n",
    "    A1 = []\n",
    "    B1 = []\n",
    "    A2 = []\n",
    "    B2 = []\n",
    "    Amp = []\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        # number of TTV frequencies to model\n",
    "        n_ttv_terms = 2 - (int(np.isnan(fttv1_ls[npl]))+int(np.isnan(fttv2_ls[npl])))\n",
    "        \n",
    "        # zero frequencies\n",
    "        if n_ttv_terms == 0:\n",
    "            log_df.append(None)\n",
    "            log_df_plus.append(None)\n",
    "            log_df_minus.append(None)\n",
    "            fplus.append(None)\n",
    "            fminus.append(None)\n",
    "            fttv1.append(0.)\n",
    "            fttv2.append(0.)\n",
    "            A1.append(0.)\n",
    "            B1.append(0.)\n",
    "            A2.append(0.)\n",
    "            B2.append(0.)\n",
    "            Amp.append(1.)\n",
    "        \n",
    "        # one frequency\n",
    "        elif n_ttv_terms == 1:\n",
    "            log_df_plus.append(None)\n",
    "            log_df_minus.append(None)\n",
    "            fplus.append(None)\n",
    "            fminus.append(None)\n",
    "            \n",
    "            log_df.append(pm.Normal('log_df_{0}'.format(npl), mu=log_df_mu[npl], sd=log_df_sd[npl], testval=log_df_mu[npl]))\n",
    "            fttv1.append(pm.Deterministic('fttv1_{0}'.format(npl), fttv_min[npl]+T.exp(log_df[npl])))\n",
    "            fttv2.append(0.)\n",
    "            A1.append(pm.Normal('A1_{0}'.format(npl), mu=0, sd=1, testval=A1_testval[npl]))\n",
    "            B1.append(pm.Normal('B1_{0}'.format(npl), mu=0, sd=1, testval=B1_testval[npl]))\n",
    "            A2.append(0.)\n",
    "            B2.append(0.)\n",
    "            \n",
    "            Amp.append(ttv_amps[npl])\n",
    "            \n",
    "        # two frequencies\n",
    "        if n_ttv_terms == 2:\n",
    "            log_df.append(None)\n",
    "            \n",
    "            log_df_plus.append(pm.Normal('log_df_plus_{0}'.format(npl), mu=log_df_plus_mu[npl], \\\n",
    "                                         sd=log_df_plus_sd[npl], testval=log_df_plus_mu[npl]))\n",
    "            \n",
    "            log_df_minus.append(pm.Normal('log_df_minus_{0}'.format(npl), mu=log_df_minus_mu[npl], \\\n",
    "                                         sd=log_df_minus_sd[npl], testval=log_df_minus_mu[npl]))\n",
    "            \n",
    "            fplus.append(pm.Deterministic('fplus_{0}'.format(npl), fplus_lim[npl,0]+T.exp(log_df_plus[npl])))\n",
    "            fminus.append(pm.Deterministic('fminus_{0}'.format(npl), fminus_lim[npl,0]+T.exp(log_df_minus[npl])))\n",
    "\n",
    "            \n",
    "            fttv1.append(pm.Deterministic('fttv1_{0}'.format(npl), 0.5*(fplus[npl]+fminus[npl])))\n",
    "            fttv2.append(pm.Deterministic('fttv2_{0}'.format(npl), 0.5*(fplus[npl]-fminus[npl])))            \n",
    "            \n",
    "            A1.append(pm.Normal('A1_{0}'.format(npl), mu=0, sd=1, testval=A1_testval[npl]))\n",
    "            B1.append(pm.Normal('B1_{0}'.format(npl), mu=0, sd=1, testval=B1_testval[npl]))\n",
    "            A2.append(pm.Normal('A2_{0}'.format(npl), mu=0, sd=1, testval=A2_testval[npl]))\n",
    "            B2.append(pm.Normal('B2_{0}'.format(npl), mu=0, sd=1, testval=B2_testval[npl]))\n",
    "            \n",
    "            Amp.append(ttv_amps[npl])\n",
    "            \n",
    "    \n",
    "    # transit times\n",
    "    transit_times = []\n",
    "    for npl in range(NPL):\n",
    "        transit_times.append(pm.Deterministic('tts_{0}'.format(npl), \\\n",
    "                                              FIXED_EPHEMERIS[npl] + \\\n",
    "                                              C0[npl]*Leg0[npl] + C1[npl]*Leg1[npl] + \\\n",
    "                                              C2[npl]*Leg2[npl] + C3[npl]*Leg3[npl] + \\\n",
    "                                              C4[npl]*Leg4[npl] + C5[npl]*Leg5[npl] + \\\n",
    "                                              Amp[npl]*A1[npl]*T.sin(2*pi*fttv1[npl]*t[npl]) + \\\n",
    "                                              Amp[npl]*B1[npl]*T.cos(2*pi*fttv1[npl]*t[npl]) + \\\n",
    "                                              Amp[npl]*A2[npl]*T.sin(2*pi*fttv2[npl]*t[npl]) + \\\n",
    "                                              Amp[npl]*B2[npl]*T.cos(2*pi*fttv2[npl]*t[npl])))\n",
    "                \n",
    "\n",
    "    # set up stellar model and planetary orbit\n",
    "    exoSLC = exo.StarryLightCurve(u)\n",
    "    orbit  = exo.orbits.TTVOrbit(transit_times=transit_times, transit_inds=transit_inds, \\\n",
    "                                 b=b, r_star=Rstar, m_star=Mstar)\n",
    "\n",
    "    \n",
    "    # track period and epoch\n",
    "    T0 = pm.Deterministic('T0', orbit.t0)\n",
    "    P  = pm.Deterministic('P', orbit.period)\n",
    "    \n",
    "    \n",
    "    # build the GP kernel using a different noise model for each season\n",
    "    logSw4 = [None]*4\n",
    "    logw0  = [None]*4\n",
    "    logQ   = [None]*4\n",
    "    kernel = [None]*4\n",
    "    \n",
    "    for i in range(4):\n",
    "        gpi = gp_priors[i]\n",
    "\n",
    "        # here's the single low-frequency term\n",
    "        try:\n",
    "            logSw4[i] = pm.Normal('logSw4_{0}'.format(i), mu=gpi['logSw4'][0], sd=gpi['logSw4'][1])\n",
    "        except:\n",
    "            logSw4[i] = gpi['logSw4'][0]\n",
    "\n",
    "        try:\n",
    "            logw0[i] = pm.Normal('logw0_{0}'.format(i), mu=gpi['logw0'][0], sd=gpi['logw0'][1])\n",
    "        except:\n",
    "            logw0[i] = gpi['logw0'][0]\n",
    "\n",
    "        try:\n",
    "            logQ[i] = pm.Normal('logQ_{0}'.format(i), mu=gpi['logQ'][0], sd=gpi['logQ'][1])\n",
    "        except:\n",
    "            logQ[i] = gpi['logQ'][0]\n",
    "\n",
    "        kernel[i] = exo.gp.terms.SHOTerm(log_Sw4=logSw4[i], log_w0=logw0[i], log_Q=logQ[i])\n",
    "    \n",
    "    \n",
    "    # nuissance parameters (one mean flux; variance by quarter)\n",
    "    flux0 = pm.Normal('flux0', mu=np.ones(nq), sd=np.sqrt(var_by_quarter)/4, shape=nq)\n",
    "    logvar = pm.Normal('logvar', mu=np.log(var_by_quarter), sd=np.log(4)*np.ones(nq), shape=nq)\n",
    "   \n",
    "\n",
    "    # now evaluate the model for each quarter\n",
    "    light_curves       = [None]*nq\n",
    "    summed_light_curve = [None]*nq\n",
    "    model_flux         = [None]*nq\n",
    "    \n",
    "    gp      = [None]*nq\n",
    "    gp_pred = [None]*nq\n",
    "    \n",
    "    \n",
    "    for j, q in enumerate(quarters):\n",
    "        # set oversampling factor\n",
    "        if all_dtype[q] == 'short':\n",
    "            oversample = 1\n",
    "        elif all_dtype[q] == 'long':\n",
    "            oversample = 15\n",
    "            \n",
    "        # calculate light curves\n",
    "        light_curves[j] = exoSLC.get_light_curve(orbit=orbit, r=rp, t=all_time[q], oversample=oversample)\n",
    "        summed_light_curve[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(all_time[q]))\n",
    "        model_flux[j] = pm.Deterministic('model_flux_{0}'.format(j), summed_light_curve[j])\n",
    "        \n",
    "        # here's the GP (w/ kernel by season)\n",
    "        gp[j] = exo.gp.GP(kernel[q%4], all_time[q], T.exp(logvar[j])*T.ones(len(all_time[q])))\n",
    "\n",
    "\n",
    "        # add custom potential (log-prob fxn) with the GP likelihood\n",
    "        pm.Potential('obs_{0}'.format(j), gp[j].log_likelihood(all_flux[q] - model_flux[j]))\n",
    "\n",
    "\n",
    "        # track GP prediction\n",
    "        #gp_pred[j] = pm.Deterministic('gp_pred_{0}'.format(j), gp[j].predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sin_model:\n",
    "    # Keplerian orbit parameters\n",
    "    sin_map = exo.optimize(start=sin_model.test_point, vars=[flux0, logvar])   \n",
    "    sin_map = exo.optimize(start=sin_map, vars=[C0, C1])    \n",
    "    sin_map = exo.optimize(start=sin_map, vars=[b])\n",
    "    sin_map = exo.optimize(start=sin_map, vars=[u, Mstar])\n",
    "    \n",
    "    # polynomial TTV parameters\n",
    "    for npl in range(NPL):\n",
    "        if np.isnan(fttv1_ls[npl]):\n",
    "            if polyorder[npl] == 2:\n",
    "                sin_map = exo.optimize(start=sin_map, vars=[C2[npl]])\n",
    "            elif polyorder[npl] == 3:\n",
    "                sin_map = exo.optimize(start=sin_map, vars=[C3[npl], C2[npl]])\n",
    "            elif polyorder[npl] == 4:\n",
    "                sin_map = exo.optimize(start=sin_map, vars=[C4[npl], C3[npl], C2[npl]])\n",
    "            elif polyorder[npl] == 5:\n",
    "                sin_map = exo.optimize(start=sin_map, vars=[C5[npl], C4[npl], C3[npl], C2[npl]])\n",
    "    \n",
    "    # sinusoidal TTV parameters\n",
    "    for npl in range(NPL):\n",
    "        try:\n",
    "            sin_map = exo.optimize(start=sin_map, vars=[A1[npl], B1[npl], A2[npl], B2[npl]])\n",
    "        except:\n",
    "            try:\n",
    "                sin_map = exo.optimize(start=sin_map, vars=[A1[npl], B1[npl]])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "\n",
    "    # all parameters\n",
    "    #sin_map = exo.optimize(start=sin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_transit_times = []\n",
    "sin_ephemeris = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    sin_transit_times.append(sin_map['tts_{0}'.format(npl)])\n",
    "    sin_ephemeris.append(sin_map['P'][npl]*transit_inds[npl] + sin_map['T0'][npl])\n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(NPL, figsize=(12,8))\n",
    "\n",
    "for npl in range(NPL):\n",
    "    xtime = sin_transit_times[npl]\n",
    "    yomc  = (sin_transit_times[npl] - sin_ephemeris[npl])*24*60\n",
    "    \n",
    "    axes[npl].plot(xtime, yomc, '.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + '_ttvs_sin.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Sample from posteriors for POLYNOMIAL + SINUSOID transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sin_model:\n",
    "    sin_trace = pm.sample(tune=1000, draws=500, start=sin_map, chains=2, \\\n",
    "                      step=exo.get_dense_nuts_step(target_accept=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which variables to save (don't save full GP or model traces or \"under the hood\" variables)\n",
    "sin_map_keys = list(sin_map.keys())\n",
    "sin_varnames = []\n",
    "\n",
    "for i, smk in enumerate(sin_map_keys):\n",
    "    skip = (\"gp_pred\" in smk) + (\"model_flux\" in smk) + (\"__\" in smk)\n",
    "\n",
    "    if skip == False:\n",
    "        sin_varnames.append(smk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_hdulist = io.trace_to_hdulist(sin_trace, sin_varnames, TARGET)\n",
    "sin_hdulist.writeto(TRACE_DIR + TARGET + '_transit_shape.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOTAL RUNTIME = %.2f min' %((timer()-global_start_time)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
