{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrend and Estimate Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import scipy.optimize as op\n",
    "import scipy.signal as sig\n",
    "from   scipy import stats\n",
    "from   scipy import fftpack\n",
    "from   scipy import ndimage\n",
    "from   scipy.interpolate import UnivariateSpline\n",
    "from   scipy.interpolate import interp1d\n",
    "import astropy\n",
    "from   astropy.io import fits as pyfits\n",
    "from   astropy.timeseries import LombScargle\n",
    "from   sklearn.cluster import KMeans\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import argparse\n",
    "import json\n",
    "import importlib as imp\n",
    "from   copy import deepcopy\n",
    "from   timeit import default_timer as timer\n",
    "\n",
    "import lightkurve as lk\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "import exoplanet as exo\n",
    "import aesara_theano_fallback.tensor as T\n",
    "from   aesara_theano_fallback import aesara as theano\n",
    "from   celerite2.theano import GaussianProcess\n",
    "from   celerite2.theano import terms as GPterms\n",
    "\n",
    "from alderaan.constants import *\n",
    "from alderaan.utils import *\n",
    "from alderaan.Planet import *\n",
    "from alderaan.LiteCurve import *\n",
    "import alderaan.io as io\n",
    "import alderaan.detrend as detrend\n",
    "import alderaan.noise as noise\n",
    "\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually set I/O parameters\n",
    "#### User should manually set MISSION, TARGET, PRIMARY_DIR,  and CSV_FILE\n",
    "#### Note that if using DR25, you will need to manually correct epochs from BJD to BJKD with an offset of 2454833.0 days; the cumulative exoplanet archive catalog has already converted epochs to BJKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K00137\"\n",
    "PRIMARY_DIR  = '/Users/research/projects/alderaan/'\n",
    "\n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\"\n",
    "    \n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog_eccentric.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here's where we parse the inputs\n",
    "try:\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for ALDERAAN transit fiting pipeline\")\n",
    "    parser.add_argument(\"--mission\", default=None, type=str, required=True, \\\n",
    "                        help=\"Mission name\")\n",
    "    parser.add_argument(\"--target\", default=None, type=str, required=True, \\\n",
    "                        help=\"Target name; see ALDERAAN documentation for acceptable formats\")\n",
    "    parser.add_argument(\"--primary_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"Primary directory path for accessing lightcurve data and saving outputs\")\n",
    "    parser.add_argument(\"--csv_file\", default=None, type=str, required=True, \\\n",
    "                        help=\"Path to .csv file containing input planetary parameters\")\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    MISSION      = args.mission\n",
    "    TARGET       = args.target\n",
    "    PRIMARY_DIR  = args.primary_dir\n",
    "    CSV_FILE     = args.csv_file    \n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs\n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in planet and stellar parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = TARGET\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "RSTAR = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "LOGRHO = np.array(target_dict['logrho'],  dtype='float')[use]\n",
    "LOGRHO_ERR1 = np.array(target_dict['logrho_err1'],  dtype='float')[use]\n",
    "LOGRHO_ERR2 = np.array(target_dict['logrho_err2'],  dtype='float')[use]\n",
    "\n",
    "U1 = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "U2 = np.array(target_dict['limbdark_2'], dtype='float')[use]\n",
    "\n",
    "PERIODS = np.array(target_dict['period'], dtype='float')[use]\n",
    "EPOCHS  = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "DEPTHS  = np.array(target_dict['depth'], dtype='float')[use]*1e-6          # [ppm] --> []\n",
    "DURS    = np.array(target_dict['duration'], dtype='float')[use]/24         # [hrs] --> [days]\n",
    "IMPACTS = np.array(target_dict['impact'], dtype='float')[use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR[0] for r in RSTAR): RSTAR = RSTAR[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO[0] for r in LOGRHO): LOGRHO = LOGRHO[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR1[0] for r in LOGRHO_ERR1): LOGRHO_ERR1 = LOGRHO_ERR1[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR1 in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR2[0] for r in LOGRHO_ERR2): LOGRHO_ERR2 = LOGRHO_ERR2[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR2 in the csv input file')\n",
    "\n",
    "if all(u == U1[0] for u in U1): U1 = U1[0]\n",
    "else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "if all(u == U2[0] for u in U2): U2 = U2[0]\n",
    "else: raise ValueError('There are inconsistencies with U2 in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# limb darkening coefficients\n",
    "UCOEFFS = [U1, U2]\n",
    "\n",
    "# stellar density\n",
    "RHO = 10**(LOGRHO)\n",
    "RHO_ERR1 = 10**(LOGRHO+LOGRHO_ERR1) - RHO\n",
    "RHO_ERR2 = 10**(LOGRHO+LOGRHO_ERR2) - RHO\n",
    "RHO_ERR = np.sqrt(RHO_ERR1**2 + RHO_ERR2**2)/np.sqrt(2)\n",
    "\n",
    "LOGRHO_ERR = np.sqrt(LOGRHO_ERR1**2 + LOGRHO_ERR2**2)/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in pre-downloaded lightcurve data\n",
    "#### Kepler data can be retrieved by running the script \"download_from_MAST.py\"\n",
    "#### Simulated data can be produced by running the script \"simulate_lightcurve.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short cadence\n",
    "try:\n",
    "    if MISSION == 'Kepler':\n",
    "        sc_path  = glob.glob(DOWNLOAD_DIR + 'mastDownload/Kepler/kplr' + '{0:09d}'.format(KIC) + '*_sc*/')[0]\n",
    "        sc_files = glob.glob(sc_path + '*')\n",
    "\n",
    "        sc_rawdata_list = []\n",
    "        for i, scf in enumerate(sc_files):\n",
    "            sc_rawdata_list.append(lk.read(sc_files[i]))\n",
    "\n",
    "        sc_raw_collection = lk.LightCurveCollection(sc_rawdata_list)\n",
    "        sc_data = detrend.cleanup_lkfc(sc_raw_collection, KIC)\n",
    "\n",
    "\n",
    "    elif MISSION == 'Simulated':\n",
    "        sc_path = DOWNLOAD_DIR + 'Lightcurves/Kepler/simkplr' + '{0:09d}'.format(KIC) + '_sc/'\n",
    "        sc_files = glob.glob(sc_path + '*')\n",
    "\n",
    "        sc_rawdata_list = []\n",
    "        for i, scf in enumerate(sc_files):\n",
    "            sc_rawdata_list.append(io.load_sim_fits(scf))\n",
    "\n",
    "        quarters = []\n",
    "        for i, scrd in enumerate(sc_rawdata_list):\n",
    "            quarters.append(scrd.quarter)\n",
    "\n",
    "        order = np.argsort(quarters)\n",
    "        sc_rawdata_list = [sc_rawdata_list[j] for j in order]\n",
    "\n",
    "        sc_raw_collection = lk.LightCurveCollection(sc_rawdata_list)\n",
    "        sc_data = detrend.cleanup_lkfc(sc_raw_collection)\n",
    "\n",
    "\n",
    "except:\n",
    "    sc_data = lk.LightCurveCollection([])\n",
    "\n",
    "    \n",
    "sc_quarters = []\n",
    "for i, scd in enumerate(sc_data):\n",
    "    sc_quarters.append(scd.quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# long cadence\n",
    "try:\n",
    "    if MISSION == 'Kepler':\n",
    "        lc_path  = glob.glob(DOWNLOAD_DIR + 'mastDownload/Kepler/kplr' + '{0:09d}'.format(KIC) + '*_lc*/')[0]\n",
    "        lc_files = glob.glob(lc_path + '*')\n",
    "\n",
    "        lc_rawdata_list = []\n",
    "        for i, lcf in enumerate(lc_files):\n",
    "            lkread = lk.read(lc_files[i])\n",
    "\n",
    "            if ~np.isin(lkread.quarter, sc_quarters):\n",
    "                lc_rawdata_list.append(lkread)\n",
    "\n",
    "        lc_raw_collection = lk.LightCurveCollection(lc_rawdata_list)\n",
    "        lc_data = detrend.cleanup_lkfc(lc_raw_collection, KIC)\n",
    "\n",
    "\n",
    "    elif MISSION == 'Simulated':\n",
    "        lc_path = DOWNLOAD_DIR + 'Lightcurves/Kepler/simkplr' + '{0:09d}'.format(KIC) + '_lc/'\n",
    "        lc_files = glob.glob(lc_path + '*')\n",
    "\n",
    "        lc_rawdata_list = []\n",
    "        for i, lcf in enumerate(lc_files):\n",
    "            lc_rawdata_list.append(io.load_sim_fits(lcf))\n",
    "\n",
    "\n",
    "        quarters = []\n",
    "        for i, lcrd in enumerate(lc_rawdata_list):\n",
    "            quarters.append(lcrd.quarter)\n",
    "\n",
    "        order = np.argsort(quarters)\n",
    "        lc_rawdata_list = [lc_rawdata_list[j] for j in order]\n",
    "\n",
    "        lc_raw_collection = lk.LightCurveCollection(lc_rawdata_list)\n",
    "        lc_data = detrend.cleanup_lkfc(lc_raw_collection, KIC)\n",
    "\n",
    "\n",
    "except:\n",
    "    lc_data = LightCurveCollection([])\n",
    "\n",
    "    \n",
    "lc_quarters = []\n",
    "for i, lcd in enumerate(lc_data):\n",
    "    lc_quarters.append(lcd.quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert lk.LightCurve to LiteCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LightKurve_to_LiteCurve(lklc):\n",
    "    return LiteCurve(time  = np.array(lklc.time.value, dtype=\"float\"),\n",
    "                     flux  = np.array(lklc.flux.value, dtype=\"float\"),\n",
    "                     error = np.array(lklc.flux_err.value, dtype=\"float\"),\n",
    "                     cadno = np.array(lklc.cadenceno.value, dtype=\"int\"),\n",
    "                     quarter = lklc.quarter*np.ones(len(lklc.time), dtype=\"int\"),\n",
    "                     season  = (lklc.quarter%4)*np.ones(len(lklc.time), dtype=\"int\"),\n",
    "                     channel = lklc.channel*np.ones(len(lklc.time), dtype=\"int\"),\n",
    "                     quality = lklc.quality.value\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_lite = []\n",
    "lc_lite = []\n",
    "\n",
    "for i, scd in enumerate(sc_data):\n",
    "    sc_lite.append(LightKurve_to_LiteCurve(scd))\n",
    "    \n",
    "for i, lcd in enumerate(lc_data):\n",
    "    lc_lite.append(LightKurve_to_LiteCurve(lcd))\n",
    "    \n",
    "    \n",
    "# renaming here is an artifact of updates to my pipeline and dependencies\n",
    "# this is easier than going through and fixing all variable names\n",
    "sc_data = sc_lite\n",
    "lc_data = lc_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine time baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_min = []\n",
    "time_max = []\n",
    "\n",
    "for i, scd in enumerate(sc_data):\n",
    "    time_min.append(scd.time.min())\n",
    "    time_max.append(scd.time.max())\n",
    "\n",
    "for i, lcd in enumerate(lc_data):\n",
    "    time_min.append(lcd.time.min())\n",
    "    time_max.append(lcd.time.max())\n",
    "\n",
    "TIME_START = np.min(time_min)\n",
    "TIME_END   = np.max(time_max)\n",
    "\n",
    "if TIME_START < 0:\n",
    "    raise ValueError(\"START TIME [BKJD] is negative...this will cause problems\")\n",
    "\n",
    "\n",
    "# put epochs in range (TIME_START, TIME_START + PERIOD)\n",
    "for npl in range(NPL):\n",
    "    if EPOCHS[npl] < TIME_START:\n",
    "        adj = 1 + (TIME_START - EPOCHS[npl])//PERIODS[npl]\n",
    "        EPOCHS[npl] += adj*PERIODS[npl]        \n",
    "        \n",
    "    if EPOCHS[npl] > (TIME_START + PERIODS[npl]):\n",
    "        adj = (EPOCHS[npl] - TIME_START)//PERIODS[npl]\n",
    "        EPOCHS[npl] -= adj*PERIODS[npl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Planet objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing %d Planet objects' %NPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "planets = []\n",
    "for npl in range(NPL):\n",
    "    p = Planet()\n",
    "    \n",
    "    # put in some basic transit parameters\n",
    "    p.epoch    = EPOCHS[npl]\n",
    "    p.period   = PERIODS[npl]\n",
    "    p.depth    = DEPTHS[npl]\n",
    "    p.duration = DURS[npl]\n",
    "    p.impact   = IMPACTS[npl]\n",
    "    \n",
    "    if p.impact > 1 - np.sqrt(p.depth):\n",
    "        p.impact = (1 - np.sqrt(p.depth))**2\n",
    "        \n",
    "    # estimate transit times from linear ephemeris\n",
    "    p.tts = np.arange(p.epoch, TIME_END, p.period)\n",
    "\n",
    "    # make transit indexes\n",
    "    p.index = np.array(np.round((p.tts-p.epoch)/p.period),dtype='int')\n",
    "    \n",
    "    # add to list\n",
    "    planets.append(p)\n",
    "\n",
    "\n",
    "# put planets in order by period\n",
    "order = np.argsort(PERIODS)\n",
    "\n",
    "sorted_planets = []\n",
    "for npl in range(NPL):\n",
    "    sorted_planets.append(planets[order[npl]])\n",
    "\n",
    "planets = np.copy(sorted_planets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Holczer+ 2016 TTVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building initial TTV model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HOLCZER_FILE = PRIMARY_DIR + \"Catalogs/holczer_2016_kepler_ttvs.txt\"\n",
    "\n",
    "if MISSION == \"Kepler\":\n",
    "    holczer_data = np.loadtxt(HOLCZER_FILE, usecols=[0,1,2,3])\n",
    "\n",
    "    holczer_inds = []\n",
    "    holczer_tts  = []\n",
    "    holczer_pers = []\n",
    "\n",
    "    for npl in range(NPL):\n",
    "        koi = int(TARGET[1:]) + 0.01*(1+npl)\n",
    "        use = np.isclose(holczer_data[:,0], koi, rtol=1e-10, atol=1e-10)\n",
    "        \n",
    "        # Holczer uses BJD -24548900; BJKD = BJD - 2454833\n",
    "        if np.sum(use) > 0:\n",
    "            holczer_inds.append(np.array(holczer_data[use,1], dtype=\"int\"))\n",
    "            holczer_tts.append(holczer_data[use,2] + holczer_data[use,3]/24/60 + 67)\n",
    "            holczer_pers.append(np.median(holczer_tts[npl][1:] - holczer_tts[npl][:-1]))\n",
    "            \n",
    "        else:\n",
    "            holczer_inds.append(None)\n",
    "            holczer_tts.append(None)\n",
    "            holczer_pers.append(np.nan)\n",
    "            \n",
    "    holczer_pers = np.asarray(holczer_pers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For simulated data, initilize with true TTVs + noise\n",
    "#### Synthetic \"Holczer\" TTVs are approximated as ground truth + Student-t2 noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MISSION == \"Simulated\":\n",
    "    holczer_inds = []\n",
    "    holczer_tts  = []\n",
    "    holczer_pers = []\n",
    "    \n",
    "    for npl, p in enumerate(planets):\n",
    "        # read in the \"ground truth\" TTVs\n",
    "        fname_in = DOWNLOAD_DIR + \"TTVs/\" + TARGET + \"_0{0}_sim_ttvs.txt\".format(npl)\n",
    "        data_in  = np.loadtxt(fname_in).swapaxes(0,1)\n",
    "    \n",
    "        inds = np.array(data_in[0], dtype=\"int\")\n",
    "        tts_true  = np.array(data_in[1], dtype=\"float\")\n",
    "        \n",
    "        # add some noise and reject transits without photometry cover\n",
    "        if len(tts_true) > 20:\n",
    "            tts_noisy = tts_true + stats.t.rvs(df=2, size=len(tts_true))*p.duration/3\n",
    "        else:\n",
    "            tts_noisy = tts_true + np.random.normal(size=len(tts_true))*p.duration/3\n",
    "        \n",
    "        keep = np.zeros(len(tts_noisy), dtype=\"bool\")\n",
    "        \n",
    "        for i, t0 in enumerate(tts_noisy):\n",
    "            for j, scd in enumerate(sc_data):\n",
    "                if np.min(np.abs(scd.time - t0)) < p.duration:\n",
    "                    keep[i] = True\n",
    "            for j, lcd in enumerate(lc_data):\n",
    "                if np.min(np.abs(lcd.time - t0)) < p.duration:\n",
    "                    keep[i] = True\n",
    "        \n",
    "        holczer_inds.append(inds[keep])\n",
    "        holczer_tts.append(tts_noisy[keep])\n",
    "        holczer_pers.append(p.period)\n",
    "        \n",
    "        \n",
    "    holczer_pers = np.array(holczer_pers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth and interpolate Holczer TTVs where they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    if np.isfinite(holczer_pers[npl]):\n",
    "        # fit a linear ephemeris \n",
    "        pfit  = poly.polyfit(holczer_inds[npl], holczer_tts[npl], 1)\n",
    "        ephem = poly.polyval(holczer_inds[npl], pfit)\n",
    "        \n",
    "        \n",
    "        # put fitted epoch in range (TIME_START, TIME_START + PERIOD)\n",
    "        hepoch, hper = pfit\n",
    "\n",
    "        if hepoch < TIME_START:\n",
    "            adj = 1 + (TIME_START - hepoch)//hper\n",
    "            hepoch += adj*hper       \n",
    "\n",
    "        if hepoch > (TIME_START + hper):\n",
    "            adj = (hepoch - TIME_START)//hper\n",
    "            hepoch -= adj*hper      \n",
    "\n",
    "        hephem = np.arange(hepoch, TIME_END, hper)        \n",
    "        hinds  = np.array(np.round((hephem-hepoch)/hper),dtype='int')\n",
    "        \n",
    "        \n",
    "        # calculate OMC and flag outliers\n",
    "        xtime = np.copy(holczer_tts[npl])\n",
    "        yomc  = (holczer_tts[npl] - ephem)\n",
    "\n",
    "        ymed = ndimage.median_filter(yomc, size=5, mode=\"mirror\")\n",
    "        out  = np.abs(yomc-ymed)/astropy.stats.mad_std(yomc-ymed) > 3.0\n",
    "                \n",
    "        # set up a GP using a Matern-3/2 kernel\n",
    "        with pm.Model() as holczer_model:\n",
    "\n",
    "            dx = np.mean(np.diff(xtime))\n",
    "            \n",
    "            # build the kernel\n",
    "            log_sigma = pm.Normal(\"log_sigma\", mu=np.log(np.std(yomc)), sd=5)\n",
    "            log_rho_off = pm.Normal(\"log_rho_off\", mu=np.log(4*dx), sd=5)\n",
    "            rho = pm.Deterministic(\"rho\", 4*dx + T.exp(log_rho_off))\n",
    "            \n",
    "            kernel = GPterms.Matern32Term(sigma=T.exp(log_sigma), rho=rho)\n",
    "\n",
    "            # here's the GP likelihood\n",
    "            mean = pm.Normal(\"mean\", mu=np.mean(yomc), sd=np.std(yomc))\n",
    "            logjit = pm.Normal(\"logjit\", mu=np.var(yomc), sd=5)\n",
    "            \n",
    "            gp = GaussianProcess(kernel, t=xtime[~out], diag=T.exp(logjit)*T.ones(np.sum(~out)), mean=mean)\n",
    "\n",
    "            gp.marginal(\"gp\", observed=yomc[~out])\n",
    "\n",
    "            # track GP prediction\n",
    "            pred = pm.Deterministic(\"pred\", gp.predict(yomc[~out], hephem))\n",
    "                                    \n",
    "\n",
    "        # find the MAP solution\n",
    "        with holczer_model:\n",
    "            holczer_map = pmx.optimize()\n",
    "            \n",
    "        htts = hephem + holczer_map[\"pred\"]\n",
    "\n",
    "        holczer_inds[npl] = np.copy(hinds)\n",
    "        holczer_tts[npl] = np.copy(htts)\n",
    "            \n",
    "            \n",
    "        # plot the results\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(xtime[~out], yomc[~out]*24*60, 'o', c=\"grey\", label=\"Holczer\")\n",
    "        plt.plot(xtime[out], yomc[out]*24*60, \"rx\")\n",
    "        plt.plot(hephem, (htts-hephem)*24*60, \"k+\", label=\"Interpolation\")\n",
    "        plt.xlabel(\"Time [BJKD]\", fontsize=20)\n",
    "        plt.ylabel(\"O-C [min]\", fontsize=20)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Holczer TTVs exist, and if so, replace the linear ephemeris\n",
    "holczer_transit_times = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    match = np.isclose(holczer_pers, p.period, rtol=0.1, atol=DURS.max())\n",
    "    \n",
    "    if np.sum(match) > 1:\n",
    "        raise ValueError(\"Something has gone wrong matching periods between DR25 and Holczer+ 2016\")\n",
    "        \n",
    "    if np.sum(match) == 1:\n",
    "        loc = np.squeeze(np.where(match))\n",
    "    \n",
    "        hinds = holczer_inds[loc]\n",
    "        htts  = holczer_tts[loc]\n",
    "        \n",
    "        for i, t0 in enumerate(p.tts):\n",
    "            for j, tH in enumerate(htts):\n",
    "                if np.abs(t0-tH)/p.period < 0.25:\n",
    "                    p.tts[i] = tH\n",
    "                    \n",
    "        holczer_transit_times.append(np.copy(p.tts))\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        holczer_transit_times.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the OMC TTVs\n",
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "if NPL == 1: axes = [axes]\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    xtime = poly.polyval(p.index, poly.polyfit(p.index, p.tts, 1))\n",
    "    yomc  = (p.tts - xtime)*24*60\n",
    "    \n",
    "    axes[npl].plot(xtime, yomc, '.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "axes[0].set_title(TARGET, fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + '_ttvs_initial.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FULL_FIXED_EPHEMERIS = []\n",
    "FULL_FIXED_INDS = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    FULL_FIXED_EPHEMERIS.append(poly.polyval(p.index, poly.polyfit(p.index, p.tts, 1)))\n",
    "    FULL_FIXED_INDS.append(np.copy(p.index - p.index.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrend the lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set break tolerance and nominal minimum oscillation period\n",
    "break_tolerance = np.max([int(DURS.min()/(LCIT/60/24)*5/2), 13])\n",
    "min_period = 1.0\n",
    "\n",
    "\n",
    "for i, lcd in enumerate(lc_data):\n",
    "    print(\"QUARTER {}\".format(lcd.quarter[0]))\n",
    "    \n",
    "    qmask = lk.KeplerQualityFlags.create_quality_mask(lcd.quality, bitmask=\"default\")\n",
    "    lcd.remove_flagged_cadences(qmask)\n",
    "    \n",
    "    # make transit mask\n",
    "    lcd.mask = np.zeros(len(lcd.time), dtype=\"bool\")\n",
    "    for npl, p in enumerate(planets):\n",
    "        lcd.mask += detrend.make_transitmask(lcd.time, p.tts, np.max([1/24,1.5*p.duration]))\n",
    "    \n",
    "    lcd.clip_outliers(kernel_size=13, sigma_upper=5, sigma_lower=5, mask=lcd.mask)\n",
    "    lcd.clip_outliers(kernel_size=13, sigma_upper=5, sigma_lower=1000, mask=None)\n",
    "    \n",
    "    try:\n",
    "        lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period)\n",
    "    except:\n",
    "        warnings.warn(\"Initial detrending model failed...attempting to refit without exponential ramp component\")\n",
    "        try:\n",
    "            lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period, correct_ramp=False)\n",
    "        except:\n",
    "            warnings.warn(\"Detrending with RotationTerm failed...attempting to detrend with SHOTerm\")\n",
    "            lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period, kterm=\"SHOTerm\", correct_ramp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lc_data) > 0:\n",
    "    lc = detrend.stitch(lc_data)\n",
    "else:\n",
    "    lc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set break tolerance and nominal minimum oscillation period\n",
    "break_tolerance = np.max([int(DURS.min()/(SCIT/3600/24)*5/2), 91])\n",
    "min_period = 1.0\n",
    "\n",
    "\n",
    "for i, scd in enumerate(sc_data):\n",
    "    print(\"QUARTER {}\".format(scd.quarter[0]))\n",
    "    \n",
    "    qmask = lk.KeplerQualityFlags.create_quality_mask(scd.quality, bitmask=\"default\")\n",
    "    scd.remove_flagged_cadences(qmask)\n",
    "    \n",
    "    # make transit mask\n",
    "    scd.mask = np.zeros(len(scd.time), dtype=\"bool\")\n",
    "    for npl, p in enumerate(planets):\n",
    "        scd.mask += detrend.make_transitmask(scd.time, p.tts, np.max([1/24,1.5*p.duration]))\n",
    "    \n",
    "    scd.clip_outliers(kernel_size=13, sigma_upper=5, sigma_lower=5, mask=scd.mask)\n",
    "    scd.clip_outliers(kernel_size=13, sigma_upper=5, sigma_lower=1000, mask=None)\n",
    "    \n",
    "    try:\n",
    "        scd = detrend.flatten_with_gp(scd, break_tolerance, min_period)\n",
    "    except:\n",
    "        warnings.warn(\"Initial detrending model failed...attempting to refit without exponential ramp component\")\n",
    "        try:\n",
    "            scd = detrend.flatten_with_gp(scd, break_tolerance, min_period, correct_ramp=False)\n",
    "        except:\n",
    "            warnings.warn(\"Detrending with RotationTerm failed...attempting to detrend with SHOTerm\")\n",
    "            scd = detrend.flatten_with_gp(scd, break_tolerance, min_period, kterm=\"SHOTerm\", correct_ramp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sc_data) > 0:\n",
    "    sc = detrend.stitch(sc_data)\n",
    "else:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make wide masks that track each planet individually\n",
    "### These masks have width 2.5 transit durations, which is probably wider than the masks used for detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if sc is not None:\n",
    "    sc_mask = np.zeros((NPL,len(sc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        sc_mask[npl] = detrend.make_transitmask(sc.time, p.tts, np.max([2/24,2.5*p.duration]))\n",
    "        \n",
    "    sc.mask = sc_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    sc_mask = None\n",
    "\n",
    "    \n",
    "if lc is not None:\n",
    "    lc_mask = np.zeros((NPL,len(lc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        lc_mask[npl] = detrend.make_transitmask(lc.time, p.tts, np.max([2/24,2.5*p.duration]))\n",
    "        \n",
    "    lc.mask = lc_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    lc_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag high quality transits (quality = 1)\n",
    "\n",
    "### Good transits must have  at least 50% photometry coverage in/near transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl, p in enumerate(planets):\n",
    "    count_expect_lc = int(np.ceil(p.duration/lcit))\n",
    "    count_expect_sc = int(np.ceil(p.duration/scit))\n",
    "        \n",
    "    quality = np.zeros(len(p.tts), dtype=\"bool\")\n",
    "    \n",
    "    for i, t0 in enumerate(p.tts):\n",
    "        \n",
    "        if sc is not None:\n",
    "            in_sc = np.abs(sc.time - t0)/p.duration < 0.5\n",
    "            near_sc = np.abs(sc.time - t0)/p.duration < 1.5\n",
    "            \n",
    "            qual_in = np.sum(in_sc) > 0.5*count_expect_sc\n",
    "            qual_near = np.sum(near_sc) > 1.5*count_expect_sc\n",
    "            \n",
    "            quality[i] += qual_in*qual_near\n",
    "        \n",
    "        \n",
    "        if lc is not None:\n",
    "            in_lc = np.abs(lc.time - t0)/p.duration < 0.5\n",
    "            near_lc = np.abs(lc.time - t0)/p.duration < 1.5\n",
    "            \n",
    "            qual_in = np.sum(in_lc) > 0.5*count_expect_lc\n",
    "            qual_near = np.sum(near_lc) > 1.5*count_expect_lc\n",
    "            \n",
    "            quality[i] += qual_in*qual_near\n",
    "            \n",
    "    \n",
    "    p.quality = np.copy(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag which transits overlap (overlap = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify overlapping transits\n",
    "dur_max = np.max(DURS)\n",
    "overlap = []\n",
    "\n",
    "for i in range(NPL):\n",
    "    overlap.append(np.zeros(len(planets[i].tts), dtype='bool'))\n",
    "    \n",
    "    for j in range(NPL):\n",
    "        if i != j:\n",
    "            for ttj in planets[j].tts:\n",
    "                overlap[i] += np.abs(planets[i].tts - ttj)/dur_max < 1.5\n",
    "                \n",
    "    planets[i].overlap = np.copy(overlap[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count up transits and calculate initial fixed transit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_transits = np.zeros(NPL)\n",
    "transit_inds = []\n",
    "fixed_tts = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    transit_inds.append(np.array((p.index - p.index.min())[p.quality], dtype=\"int\"))\n",
    "    fixed_tts.append(np.copy(p.tts)[p.quality])\n",
    "    \n",
    "    num_transits[npl] = len(transit_inds[npl])\n",
    "    transit_inds[npl] -= transit_inds[npl].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data near transits for each quarter\n",
    "all_time = [None]*18\n",
    "all_flux = [None]*18\n",
    "all_error = [None]*18\n",
    "all_dtype = [\"none\"]*18\n",
    "\n",
    "lc_flux = []\n",
    "sc_flux = []\n",
    "\n",
    "\n",
    "for q in range(18):\n",
    "    if sc is not None:\n",
    "        if np.isin(q, sc.quarter):\n",
    "            use = (sc.mask)*(sc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 45:\n",
    "                all_time[q] = sc.time[use]\n",
    "                all_flux[q] = sc.flux[use]\n",
    "                all_error[q] = sc.error[use]\n",
    "                all_dtype[q] = \"short\"\n",
    "\n",
    "                sc_flux.append(sc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_dtype[q] = \"short_no_transits\"\n",
    "\n",
    "    \n",
    "    if lc is not None:\n",
    "        if np.isin(q, lc.quarter):\n",
    "            use = (lc.mask)*(lc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 5:\n",
    "                all_time[q] = lc.time[use]\n",
    "                all_flux[q] = lc.flux[use]\n",
    "                all_error[q] = lc.error[use]\n",
    "                all_dtype[q] = \"long\"\n",
    "\n",
    "                lc_flux.append(lc.flux[use])\n",
    "                \n",
    "            else:\n",
    "                all_dtype[q] = \"long_no_transits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which quarters have data and transits\n",
    "good = (np.array(all_dtype) == \"short\") + (np.array(all_dtype) == \"long\")\n",
    "quarters = np.arange(18)[good]\n",
    "nq = len(quarters)\n",
    "\n",
    "\n",
    "# make some linear flux arrays (for convenience use laster)\n",
    "try: sc_flux_lin = np.hstack(sc_flux)\n",
    "except: sc_flux_lin = np.array([])\n",
    "    \n",
    "try: lc_flux_lin = np.hstack(lc_flux)\n",
    "except: lc_flux_lin = np.array([])\n",
    "    \n",
    "try:\n",
    "    good_flux = np.hstack([sc_flux_lin, lc_flux_lin])\n",
    "except:\n",
    "    try:\n",
    "        good_flux = np.hstack(sc_flux)\n",
    "    except:\n",
    "        good_flux = np.hstack(lc_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set oversampling factors and expoure times\n",
    "oversample = np.zeros(18, dtype=\"int\")\n",
    "texp = np.zeros(18)\n",
    "\n",
    "oversample[np.array(all_dtype)==\"short\"] = 1\n",
    "oversample[np.array(all_dtype)==\"long\"] = 15\n",
    "\n",
    "texp[np.array(all_dtype)==\"short\"] = scit\n",
    "texp[np.array(all_dtype)==\"long\"] = lcit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull basic transit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "periods   = np.zeros(NPL)\n",
    "epochs    = np.zeros(NPL)\n",
    "depths    = np.zeros(NPL)\n",
    "durations = np.zeros(NPL)\n",
    "impacts   = np.zeros(NPL)\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    periods[npl]   = p.period\n",
    "    epochs[npl]    = p.epoch\n",
    "    depths[npl]    = p.depth\n",
    "    durations[npl] = p.duration\n",
    "    impacts[npl]   = p.impact\n",
    "\n",
    "radii = np.sqrt(depths)*RSTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Fit transit SHAPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n(1) Fitting transit SHAPE model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use Legendre polynomials for better orthogonality; \"x\" is in the range (-1,1)\n",
    "Leg0 = []\n",
    "Leg1 = []\n",
    "Leg2 = []\n",
    "Leg3 = []\n",
    "t = []\n",
    "\n",
    "# this assumes a baseline in the range (TIME_START,TIME_END)\n",
    "for npl, p in enumerate(planets):    \n",
    "    t.append(p.epoch + transit_inds[npl]*p.period)\n",
    "    x = 2*(t[npl]-TIME_START)/(TIME_END-TIME_START) - 1\n",
    "\n",
    "    Leg0.append(np.ones_like(x))\n",
    "    Leg1.append(x.copy())\n",
    "    Leg2.append(0.5*(3*x**2 - 1))\n",
    "    Leg3.append(0.5*(5*x**3 - 3*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as shape_model:\n",
    "    # place informative priors on stellar density\n",
    "    logrho = pm.Normal(\"logrho\", mu=LOGRHO, sd=LOGRHO_ERR)\n",
    "    rho = pm.Deterministic(\"rho\", 10**logrho)\n",
    "    \n",
    "    # planetary parameters\n",
    "    log_ror = pm.Uniform(\"logror\", lower=np.log(1e-5), upper=np.log(1.0), shape=NPL)\n",
    "    ror = pm.Deterministic(\"ror\", T.exp(log_ror))\n",
    "    rp = pm.Deterministic(\"rp\", ror*RSTAR)\n",
    "        \n",
    "    beta = pm.Exponential('beta', lam=1, testval=-np.log(impacts), shape=NPL)\n",
    "    b    = pm.Deterministic('b', T.exp(-beta))\n",
    "    \n",
    "    # polynomial TTV parameters    \n",
    "    C0 = pm.Normal('C0', mu=0.0, sd=durations/2, shape=NPL)\n",
    "    C1 = pm.Normal('C1', mu=0.0, sd=durations/2, shape=NPL)\n",
    "    \n",
    "    transit_times = []\n",
    "    for npl in range(NPL):\n",
    "        transit_times.append(pm.Deterministic('tts_{0}'.format(npl), \n",
    "                                              fixed_tts[npl] + C0[npl]*Leg0[npl] + C1[npl]*Leg1[npl]))\n",
    "    \n",
    "    \n",
    "    # set up stellar model and planetary orbit\n",
    "    starrystar = exo.LimbDarkLightCurve(UCOEFFS)\n",
    "    orbit = exo.orbits.TTVOrbit(transit_times=transit_times, transit_inds=transit_inds, \n",
    "                                 b=b, r_star=RSTAR, rho_star=rho)\n",
    "    \n",
    "    # track period and epoch\n",
    "    T0 = pm.Deterministic('T0', orbit.t0)\n",
    "    P  = pm.Deterministic('P', orbit.period)\n",
    "    \n",
    "    \n",
    "    # nuissance parameters\n",
    "    flux0 = pm.Normal('flux0', mu=np.mean(good_flux), sd=np.std(good_flux), shape=len(quarters))\n",
    "    logjit = pm.Normal('logjit', mu=np.var(good_flux), sd=10, shape=len(quarters))\n",
    "\n",
    "    \n",
    "    # now evaluate the model for each quarter\n",
    "    light_curves = [None]*nq\n",
    "    model_flux = [None]*nq\n",
    "    obs = [None]*nq\n",
    "    \n",
    "    for j, q in enumerate(quarters):\n",
    "        # calculate light curves\n",
    "        light_curves[j] = starrystar.get_light_curve(orbit=orbit, r=rp, t=all_time[q], \n",
    "                                                     oversample=oversample[j], texp=texp[j])\n",
    "        \n",
    "        model_flux[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(all_time[q]))\n",
    "        pm.Deterministic('model_flux_{0}'.format(j), model_flux[j])\n",
    "        \n",
    "        obs[j] = pm.Normal(\"obs_{0}\".format(j), \n",
    "                           mu=model_flux[j], \n",
    "                           sd=T.sqrt(T.exp(logjit[j])), \n",
    "                           observed=all_flux[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with shape_model:\n",
    "    shape_map = shape_model.test_point\n",
    "    shape_map = pmx.optimize(start=shape_map, vars=[flux0, logjit])\n",
    "    shape_map = pmx.optimize(start=shape_map, vars=[b, rp, rho])\n",
    "    shape_map = pmx.optimize(start=shape_map, vars=[C0, C1])\n",
    "    shape_map = pmx.optimize(start=shape_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grab transit times and ephemeris\n",
    "shape_transit_times = []\n",
    "shape_ephemeris = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    shape_transit_times.append(shape_map['tts_{0}'.format(npl)])\n",
    "    shape_ephemeris.append(shape_map['P'][npl]*transit_inds[npl] + shape_map['T0'][npl])\n",
    "\n",
    "    \n",
    "# plot the OMC TTVs\n",
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "if NPL == 1: axes = [axes]\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    xtime = shape_transit_times[npl]\n",
    "    yomc  = (shape_transit_times[npl] - shape_ephemeris[npl])*24*60\n",
    "    \n",
    "    axes[npl].plot(xtime, yomc, '.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "axes[0].set_title(TARGET, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameter values\n",
    "Mstar   = shape_map[\"rho\"]/RHOSUN_GCM3 * RSTAR**3\n",
    "rho     = shape_map[\"rho\"]\n",
    "\n",
    "periods = shape_map['P']\n",
    "epochs  = shape_map['T0']\n",
    "rp      = shape_map['rp']\n",
    "b       = shape_map['b']\n",
    "\n",
    "sma     = get_sma(periods, Mstar)\n",
    "durs    = get_dur_tot(periods, rp, RSTAR, b, sma)\n",
    "\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    p.period   = periods[npl]\n",
    "    p.epoch    = epochs[npl]\n",
    "    p.depth    = get_transit_depth(rp[npl]/RSTAR, b[npl])\n",
    "    p.duration = durs[npl]\n",
    "    p.impact   = b[npl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Fit slide TTVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n(2) Fitting SLIDE TTVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_transit_times = []\n",
    "slide_error = []\n",
    "\n",
    "t_all = np.array(np.hstack(all_time), dtype=\"float\")\n",
    "f_all = np.array(np.hstack(all_flux), dtype=\"float\")\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    print(\"\\nPLANET\", npl)\n",
    "    \n",
    "    slide_transit_times.append([])\n",
    "    slide_error.append([])\n",
    "    \n",
    "    # create template transit\n",
    "    starrystar = exo.LimbDarkLightCurve(UCOEFFS)\n",
    "    orbit  = exo.orbits.KeplerianOrbit(t0=0, period=p.period, b=b[npl], r_star=RSTAR, m_star=Mstar)\n",
    "\n",
    "    gridstep     = scit/2\n",
    "    slide_offset = 1.0\n",
    "    delta_chisq  = 2.0\n",
    "\n",
    "    template_time = np.arange(-(0.02+p.duration)*(slide_offset+1.6), (0.02+p.duration)*(slide_offset+1.6), gridstep)\n",
    "    template_flux = 1.0 + starrystar.get_light_curve(orbit=orbit, r=rp[npl], t=template_time).sum(axis=-1).eval()\n",
    "    \n",
    "    \n",
    "    # empty lists to hold new transit time and uncertainties\n",
    "    tts = -99*np.ones_like(shape_transit_times[npl])\n",
    "    err = -99*np.ones_like(shape_transit_times[npl])\n",
    "    \n",
    "    for i, t0 in enumerate(shape_transit_times[npl]):\n",
    "        #print(i, np.round(t0,2))\n",
    "        if ~p.overlap[p.quality][i]:\n",
    "        \n",
    "            # grab flux near each non-overlapping transit\n",
    "            use = np.abs(t_all - t0)/p.duration < 2.5\n",
    "            mask = np.abs(t_all - t0)/p.duration < 1.0\n",
    "\n",
    "            t_ = t_all[use]\n",
    "            f_ = f_all[use]\n",
    "            m_ = mask[use]\n",
    "            \n",
    "            \n",
    "            # remove any residual out-of-transit trend\n",
    "            try:\n",
    "                trend = poly.polyval(t_, poly.polyfit(t_[~m_], f_[~m_], 1))\n",
    "            \n",
    "                f_ /= trend\n",
    "                e_ = np.ones_like(f_)*np.std(f_[~m_])\n",
    "                \n",
    "            except:\n",
    "                e_ = np.ones_like(f_)*np.std(f_)\n",
    "            \n",
    "\n",
    "            # slide along transit time vector and calculate chisq\n",
    "            tc_vector = t0 + np.arange(-p.duration*slide_offset, p.duration*slide_offset, gridstep)\n",
    "            chisq_vector = np.zeros_like(tc_vector)\n",
    "\n",
    "            for j, tc in enumerate(tc_vector):\n",
    "                y_ = np.interp(t_-tc, template_time, template_flux)\n",
    "                chisq_vector[j] = np.sum((f_ - y_)**2/e_**2)\n",
    "\n",
    "            chisq_vector = boxcar_smooth(chisq_vector, winsize=7)\n",
    "\n",
    "\n",
    "            # grab points near minimum chisq\n",
    "            delta_chisq = 1\n",
    "            \n",
    "            loop = True\n",
    "            while loop:\n",
    "                # incrememnt delta_chisq and find minimum\n",
    "                delta_chisq += 1\n",
    "                min_chisq = chisq_vector.min()\n",
    "                \n",
    "                # grab the points near minimum\n",
    "                tcfit = tc_vector[chisq_vector < min_chisq+delta_chisq]\n",
    "                x2fit = chisq_vector[chisq_vector < min_chisq+delta_chisq]\n",
    "\n",
    "                # eliminate points far from the local minimum\n",
    "                spacing = np.median(tcfit[1:]-tcfit[:-1])\n",
    "                faraway = np.abs(tcfit-np.median(tcfit))/spacing > 1 + len(tcfit)/2\n",
    "                \n",
    "                tcfit = tcfit[~faraway]\n",
    "                x2fit = x2fit[~faraway]\n",
    "                \n",
    "                # check for stopping conditions\n",
    "                if len(x2fit) >= 3:\n",
    "                    loop = False\n",
    "                    \n",
    "                if delta_chisq >= 9:\n",
    "                    loop = False\n",
    "                    \n",
    "                    \n",
    "            # fit a parabola around the minimum (need at least 3 pts)\n",
    "            if len(tcfit) < 3:\n",
    "                #print(\"TOO FEW POINTS\")\n",
    "                tts[i] = np.nan\n",
    "                err[i] = np.nan\n",
    "\n",
    "            else:\n",
    "                quad_coeffs = np.polyfit(tcfit, x2fit, 2)\n",
    "                quadfit = np.polyval(quad_coeffs, tcfit)\n",
    "                qtc_min = -quad_coeffs[1]/(2*quad_coeffs[0])\n",
    "                qx2_min = np.polyval(quad_coeffs, qtc_min)\n",
    "                qtc_err = np.sqrt(1/quad_coeffs[0])\n",
    "\n",
    "                # here's the fitted transit time\n",
    "                tts[i] = np.mean([qtc_min,np.median(tcfit)])\n",
    "                err[i] = qtc_err*1.0\n",
    "\n",
    "                # check that the fit is well-conditioned (ie. a negative t**2 coefficient)\n",
    "                if quad_coeffs[0] <= 0.0:\n",
    "                    #print(\"INVERTED PARABOLA\")\n",
    "                    tts[i] = np.nan\n",
    "                    err[i] = np.nan\n",
    "\n",
    "                # check that the recovered transit time is within the expected range\n",
    "                if (tts[i] < tcfit.min()) or (tts[i] > tcfit.max()):\n",
    "                    #print(\"T0 OUT OF BOUNDS\")\n",
    "                    tts[i] = np.nan\n",
    "                    err[i] = np.nan\n",
    "\n",
    "\n",
    "            # show plots\n",
    "            if ~np.isnan(tts[i]):\n",
    "                do_plots = False\n",
    "                    \n",
    "                if do_plots:\n",
    "                    fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
    "\n",
    "                    ax[0].plot(t_-tts[i], f_, \"ko\")\n",
    "                    ax[0].plot((t_-tts[i])[m_], f_[m_], \"o\", c=\"C{0}\".format(npl))\n",
    "                    ax[0].plot(template_time, template_flux, c=\"C{0}\".format(npl), lw=2)\n",
    "\n",
    "                    ax[1].plot(tcfit, x2fit, \"ko\")\n",
    "                    ax[1].plot(tcfit, quadfit, c=\"C{0}\".format(npl), lw=3)\n",
    "                    ax[1].axvline(tts[i], color=\"k\", ls=\"--\", lw=2)\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "        else:\n",
    "            #print(\"OVERLAPPING TRANSITS\")\n",
    "            tts[i] = np.nan\n",
    "            err[i] = np.nan\n",
    "        \n",
    "    slide_transit_times[npl] = np.copy(tts)\n",
    "    slide_error[npl] = np.copy(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for npl, p in enumerate(planets):\n",
    "    bad = np.isnan(slide_transit_times[npl]) + np.isnan(slide_error[npl])\n",
    "    bad += slide_error[npl] > 8*np.nanmedian(slide_error[npl])\n",
    "    \n",
    "    slide_transit_times[npl][bad] = shape_transit_times[npl][bad]\n",
    "    slide_error[npl][bad] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab transit times and ephemeris\n",
    "# plot the OMC TTVs\n",
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "if NPL == 1: axes = [axes]\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    ephem = poly.polyval(transit_inds[npl], poly.polyfit(transit_inds[npl], slide_transit_times[npl], 1))\n",
    "    \n",
    "    xtime = slide_transit_times[npl]\n",
    "    yomc  = (slide_transit_times[npl] - ephem)*24*60\n",
    "    yerr  = slide_error[npl]*24*60\n",
    "    \n",
    "    good = ~np.isnan(slide_error[npl])\n",
    "    \n",
    "    axes[npl].plot(xtime[~good], yomc[~good], \"d\", color=\"lightgrey\")\n",
    "    axes[npl].errorbar(xtime[good], yomc[good], yerr=yerr[good], fmt='.', color='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "axes[0].set_title(TARGET, fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + '_ttvs_slide.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Fit MAP independent TTVs\n",
    "\n",
    "### Only refit transit times for which the slide method failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n(3) Fitting INDEPENDENT TTVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sc is not None:\n",
    "    sc_map_mask = np.zeros((NPL,len(sc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        tts = slide_transit_times[npl][np.isnan(slide_error[npl])]\n",
    "        sc_map_mask[npl] = detrend.make_transitmask(sc.time, tts, np.max([2/24,2.5*p.duration]))\n",
    "        \n",
    "    sc_map_mask = sc_map_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    sc_map_mask = None\n",
    "\n",
    "    \n",
    "if lc is not None:\n",
    "    lc_map_mask = np.zeros((NPL,len(lc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        tts = slide_transit_times[npl][np.isnan(slide_error[npl])]\n",
    "        lc_map_mask[npl] = detrend.make_transitmask(lc.time, tts, np.max([2/24,2.5*p.duration]))\n",
    "        \n",
    "    lc_map_mask = lc_map_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    lc_map_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab data near transits for each quarter\n",
    "map_time = [None]*18\n",
    "map_flux = [None]*18\n",
    "map_dtype = [\"none\"]*18\n",
    "\n",
    "\n",
    "for q in range(18):\n",
    "    if sc is not None:\n",
    "        if np.isin(q, sc.quarter):\n",
    "            use = (sc_map_mask)*(sc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 45:\n",
    "                map_time[q] = sc.time[use]\n",
    "                map_flux[q] = sc.flux[use]\n",
    "                map_dtype[q] = \"short\"\n",
    "                \n",
    "            else:\n",
    "                map_dtype[q] = \"short_no_transits\"\n",
    "\n",
    "    \n",
    "    if lc is not None:\n",
    "        if np.isin(q, lc.quarter):\n",
    "            use = (lc_map_mask)*(lc.quarter == q)\n",
    "\n",
    "            if np.sum(use) > 5:\n",
    "                map_time[q] = lc.time[use]\n",
    "                map_flux[q] = lc.flux[use]\n",
    "                map_dtype[q] = \"long\"\n",
    "                \n",
    "            else:\n",
    "                map_dtype[q] = \"long_no_transits\"\n",
    "                \n",
    "map_quarters = np.arange(18)[(np.array(map_dtype) == \"short\") + (np.array(map_dtype) == \"long\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as indep_model:\n",
    "    # transit times\n",
    "    tt_offset = []\n",
    "    map_tts  = []\n",
    "    map_inds = []\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        use = np.isnan(slide_error[npl])\n",
    "        \n",
    "        tt_offset.append(pm.Normal(\"tt_offset_{0}\".format(npl), mu=0, sd=1, shape=np.sum(use)))\n",
    "        \n",
    "        map_tts.append(pm.Deterministic(\"tts_{0}\".format(npl),\n",
    "                                        slide_transit_times[npl][use] + tt_offset[npl]*durations[npl]/3))\n",
    "        \n",
    "        map_inds.append(transit_inds[npl][use])\n",
    "    \n",
    "    # set up stellar model and planetary orbit\n",
    "    starrystar = exo.LimbDarkLightCurve(UCOEFFS)\n",
    "    orbit  = exo.orbits.TTVOrbit(transit_times=map_tts, transit_inds=map_inds, \n",
    "                                 b=b, r_star=RSTAR, m_star=Mstar)\n",
    "    \n",
    "    # track period and epoch\n",
    "    T0 = pm.Deterministic('T0', orbit.t0)\n",
    "    P  = pm.Deterministic('P', orbit.period)\n",
    "    \n",
    "    \n",
    "    # nuissance parameters\n",
    "    flux0 = pm.Normal('flux0', mu=np.mean(good_flux), sd=np.std(good_flux), shape=len(map_quarters))\n",
    "    logjit = pm.Normal('logjit', mu=np.var(good_flux), sd=10, shape=len(map_quarters))\n",
    "\n",
    "    \n",
    "    # now evaluate the model for each quarter\n",
    "    light_curves = [None]*len(map_quarters)\n",
    "    model_flux = [None]*len(map_quarters)\n",
    "    obs = [None]*len(map_quarters)\n",
    "    \n",
    "    for j, q in enumerate(map_quarters):\n",
    "        # calculate light curves\n",
    "        light_curves[j] = starrystar.get_light_curve(orbit=orbit, r=rp, t=map_time[q], \n",
    "                                                     oversample=oversample[j], texp=texp[j])\n",
    "        \n",
    "        model_flux[j] = pm.math.sum(light_curves[j], axis=-1) + flux0[j]*T.ones(len(map_time[q]))\n",
    "        pm.Deterministic('model_flux_{0}'.format(j), model_flux[j])\n",
    "        \n",
    "        obs[j] = pm.Normal(\"obs_{0}\".format(j), \n",
    "                           mu=model_flux[j], \n",
    "                           sd=T.sqrt(T.exp(logjit[j])), \n",
    "                           observed=map_flux[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with indep_model:\n",
    "    indep_map = indep_model.test_point\n",
    "    indep_map = pmx.optimize(start=indep_map, vars=[flux0, logjit])\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        indep_map = pmx.optimize(start=indep_map, vars=[tt_offset[npl]])\n",
    "        \n",
    "    indep_map = pmx.optimize(start=indep_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_transit_times = []\n",
    "indep_error = []\n",
    "indep_ephemeris = []\n",
    "full_indep_ephemeris = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    indep_transit_times.append(np.copy(slide_transit_times[npl]))\n",
    "    indep_error.append(np.copy(slide_error[npl]))\n",
    "    \n",
    "    replace = np.isnan(slide_error[npl])\n",
    "    \n",
    "    indep_transit_times[npl][replace] = indep_map[\"tts_{0}\".format(npl)]\n",
    "    \n",
    "    pfit = poly.polyfit(transit_inds[npl], indep_transit_times[npl], 1)\n",
    "    \n",
    "    indep_ephemeris.append(poly.polyval(transit_inds[npl], pfit))\n",
    "    full_indep_ephemeris.append(poly.polyval(p.index, pfit))\n",
    "\n",
    "    indep_error[npl][replace] = np.std(indep_transit_times[npl] - indep_ephemeris[npl])\n",
    "\n",
    "    \n",
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "if NPL == 1: axes = [axes]\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    xtime = indep_transit_times[npl]\n",
    "    yomc  = (indep_transit_times[npl] - indep_ephemeris[npl])*24*60\n",
    "    yerr  = (indep_error[npl])*24*60\n",
    "    \n",
    "    axes[npl].errorbar(xtime, yomc, yerr=yerr, fmt='.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "axes[0].set_title(TARGET, fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + '_ttvs_indep.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for periodic signals in the OMC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching for periodic signals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_freqs = []\n",
    "indep_faps = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    # grab data\n",
    "    xtime = indep_ephemeris[npl]\n",
    "    yomc  = indep_transit_times[npl] - indep_ephemeris[npl]\n",
    "\n",
    "    ymed = boxcar_smooth(ndimage.median_filter(yomc, size=5, mode=\"mirror\"), winsize=5)\n",
    "    out  = np.abs(yomc-ymed)/astropy.stats.mad_std(yomc-ymed) > 5.0\n",
    "    \n",
    "    \n",
    "    # search for a periodic component\n",
    "    peakfreq = np.nan\n",
    "    peakfap = 1.0\n",
    "    \n",
    "    if NPL == 1: fap = 0.1\n",
    "    elif NPL > 1: fap = 0.99\n",
    "    \n",
    "    if np.sum(~out) > 8:\n",
    "        try:\n",
    "            xf, yf, freqs, faps = LS_estimator(xtime[~out], yomc[~out], fap=fap)\n",
    "\n",
    "            if len(freqs) > 0:\n",
    "                if freqs[0] > xf.min():\n",
    "                    peakfreq = freqs[0]\n",
    "                    peakfap = faps[0]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    indep_freqs.append(peakfreq)\n",
    "    indep_faps.append(peakfap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omc_freqs = []\n",
    "omc_faps = []\n",
    "\n",
    "# for single planet systems, use the direct LS output\n",
    "if NPL == 1:\n",
    "    if np.isnan(indep_freqs[0]):\n",
    "        omc_freqs.append(None)\n",
    "        omc_faps.append(None)\n",
    "    else:\n",
    "        omc_freqs.append(indep_freqs[0])\n",
    "        omc_faps.append(indep_faps[0])\n",
    "    \n",
    "\n",
    "# for multiplanet systems, check if any statistically marginal frequencies match between planets\n",
    "elif NPL > 1:\n",
    "    \n",
    "    for i in range(NPL):\n",
    "        # save any low FAP frequencies\n",
    "        if indep_faps[i] < 0.1:\n",
    "            omc_freqs.append(indep_freqs[i])\n",
    "            omc_faps.append(indep_faps[i])\n",
    "            \n",
    "        # check if the LS frequency is close to that of any other planet\n",
    "        else:\n",
    "            close = False\n",
    "            \n",
    "            df_min = 1/(indep_ephemeris[i].max() - indep_ephemeris[i].min())\n",
    "            \n",
    "            for j in range(i+1, NPL):\n",
    "                # delta-freq (LS) between two planets\n",
    "                df_ij = np.abs(indep_freqs[i]-indep_freqs[j])\n",
    "                \n",
    "                if df_ij < df_min:\n",
    "                    close = True\n",
    "                    \n",
    "            if close:\n",
    "                omc_freqs.append(indep_freqs[i])\n",
    "                omc_faps.append(indep_faps[i])\n",
    "                \n",
    "            else:\n",
    "                omc_freqs.append(None)\n",
    "                omc_faps.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omc_pers = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    print(\"\\nPLANET\", npl)\n",
    "    \n",
    "    # roughly model OMC based on single frequency sinusoid (if found)\n",
    "    if omc_freqs[npl] is not None:\n",
    "        print(\"periodic component found at P =\", int(1/omc_freqs[npl]), \"d\")\n",
    "        \n",
    "        # store frequency\n",
    "        omc_pers.append(1/omc_freqs[npl])\n",
    "        \n",
    "        # grab data and plot\n",
    "        xtime = indep_ephemeris[npl]\n",
    "        yomc  = indep_transit_times[npl] - indep_ephemeris[npl]\n",
    "        LS = LombScargle(xtime, yomc)\n",
    "        \n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.plot(xtime, yomc*24*60, \"o\", c=\"grey\")\n",
    "        plt.plot(xtime, LS.model(xtime, omc_freqs[npl])*24*60, c=\"C{0}\".format(npl))\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        omc_pers.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine best OMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Determining best OMC model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_omc_model(xtime, yomc, polyorder=1, period=None, use_gp=False, xt_predict=None):\n",
    "    \"\"\"\n",
    "    Build a PyMC3 model to fit TTV observed-minus-calculated data\n",
    "    Assumes a functional from with a polynomial (up to cubic) plus a (optional) single-frequency sinusoid\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        xtime : ndarray\n",
    "            time values (e.g. linear ephemeris)\n",
    "        yomc : ndarray\n",
    "            observed-minus-caculated TTVs\n",
    "        polyorder : int\n",
    "            polynomial order (default=1)\n",
    "        period : float (optional)\n",
    "            if provided, the model will include a sinusoid component with tight priors on this period\n",
    "        use_gp : bool\n",
    "            True to include a Matern-3/2 covariance GP in addition to the parametric polynomial + sinusoid\n",
    "        xt_predict : ndarray\n",
    "            time values to predict OMC model; if not provided xtime will be used\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        model : pm.Model()\n",
    "    \"\"\"    \n",
    "    with pm.Model() as model:\n",
    "        # periodic component\n",
    "        if period is None:\n",
    "            f  = 1/(xtime.max()-xtime.min())\n",
    "            Ah = 0.0\n",
    "            Bk = 0.0\n",
    "            \n",
    "        else:\n",
    "            df = 1/(xtime.max()-xtime.min())\n",
    "            f  = pm.Normal(\"f\", mu=1/period, sd=df)\n",
    "            Ah = pm.Normal(\"Ah\", mu=0, sd=5*np.std(yomc))\n",
    "            Bk = pm.Normal(\"Bk\", mu=0, sd=5*np.std(yomc))\n",
    "            \n",
    "            \n",
    "        def sin_fxn(A, B, f, xt):\n",
    "            return A*T.sin(2*pi*f*xt) + B*T.cos(2*pi*f*xt)\n",
    "            \n",
    "        sin_trend = pm.Deterministic(\"sin_trend\", sin_fxn(Ah, Bk, f, xtime))\n",
    "        \n",
    "        \n",
    "        # polynomical component\n",
    "        C0 = pm.Normal(\"C0\", mu=0, sd=10)\n",
    "        C1 = 0.0\n",
    "        C2 = 0.0\n",
    "        C3 = 0.0\n",
    "\n",
    "        if polyorder >= 1: C1 = pm.Normal(\"C1\", mu=0, sd=10)\n",
    "        if polyorder >= 2: C2 = pm.Normal(\"C2\", mu=0, sd=10)\n",
    "        if polyorder >= 3: C3 = pm.Normal(\"C3\", mu=0, sd=10)\n",
    "        if polyorder >= 4: raise ValueError(\"only configured for 3rd order polynomials\")\n",
    "        \n",
    "        def poly_fxn(c0, c1, c2, c3, xt):\n",
    "            return c0 + c1*xt + c2*xt**2 + c3*xt**3\n",
    "        \n",
    "        poly_trend = pm.Deterministic(\"poly_trend\", poly_fxn(C0, C1, C2, C3, xtime))\n",
    "        \n",
    "        \n",
    "        # mean and jitter\n",
    "        mean = pm.Deterministic(\"mean\", sin_trend + poly_trend)\n",
    "        logjit = pm.Normal(\"logjit\", mu=np.log(np.var(yomc-sig.medfilt(yomc,kernel_size=5))), sd=10)\n",
    "        \n",
    "        \n",
    "        # times where trend will be predicted\n",
    "        if xt_predict is None:\n",
    "            xt_predict = xtime\n",
    "\n",
    "        \n",
    "        if use_gp:\n",
    "            # build the kernel and gp\n",
    "            dx = np.mean(np.diff(xtime))\n",
    "            \n",
    "            log_sigma = pm.Normal(\"log_sigma\", mu=np.log(np.std(yomc)), sd=5)\n",
    "            log_rho_off = pm.Normal(\"log_rho_off\", mu=np.log(4*dx), sd=5)\n",
    "            rho = pm.Deterministic(\"rho\", 4*dx + T.exp(log_rho_off))\n",
    "            \n",
    "            kernel = GPterms.Matern32Term(sigma=T.exp(log_sigma), rho=rho)\n",
    "\n",
    "            gp = GaussianProcess(kernel, t=xtime, diag=T.exp(logjit)*T.ones(len(xtime)), mean=mean)\n",
    "\n",
    "            gp.marginal(\"gp\", observed=yomc)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # here's the likelihood\n",
    "            pm.Normal(\"obs\", mu=mean, sd=T.sqrt(T.exp(logjit)*T.ones(len(xtime))), observed=yomc)\n",
    "            \n",
    "        \n",
    "        # track GP prediction and trend\n",
    "        full_mean_pred = pm.Deterministic(\"full_mean_pred\", \n",
    "                                          sin_fxn(Ah,Bk,f,xt_predict) + \n",
    "                                          poly_fxn(C0,C1,C2,C3,xt_predict))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mix_model(x):\n",
    "    \"\"\"\n",
    "    Build a 1D PyMC3 mixture model\n",
    "    The model is composed of two Normal distributions with the same mean but different variances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        x : ndarray\n",
    "            vector of data values\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        model : pm.Model()    \n",
    "    \"\"\"\n",
    "    \n",
    "    xnorm = x / np.std(x)\n",
    "    xnorm -= np.mean(xnorm)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        \n",
    "        # mixture parameters\n",
    "        w = pm.Dirichlet(\"w\", np.array([1.,1.]))\n",
    "        mu = pm.Normal(\"mu\", mu=0.0, sd=1.0, shape=1)\n",
    "        tau = pm.Gamma(\"tau\", 1.0, 1.0, shape=2)\n",
    "        \n",
    "        # here's the potential\n",
    "        obs = pm.NormalMixture(\"obs\", w, mu=mu*T.ones(2), tau=tau, observed=xnorm)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_res_outliers(res, loc, scales):\n",
    "    \"\"\"\n",
    "    Flag outliers in a residuals vector using a mixture model\n",
    "    Assigns residuals to either a narrow foreground distribution or a wide background distribution\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        res : ndarray (N)\n",
    "            vector of residuals\n",
    "        loc : float\n",
    "            normal mean inferred from build_mix_model()\n",
    "        scales : tuple\n",
    "            normal standard deviations inferred from build_mix_model()\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        fg_prob : ndarray (N)\n",
    "            probability the each item in res belongs to the foreground distribution\n",
    "        bad : bool (N)\n",
    "            binary classification of each item in res into foreground/background distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    resnorm = residuals/np.std(residuals)\n",
    "    resnorm -= np.mean(resnorm)\n",
    "    \n",
    "    order = np.argsort(scales)\n",
    "    scales = scales[order]\n",
    "    \n",
    "    z_fg = stats.norm(loc=loc, scale=scales[0]).pdf(resnorm)\n",
    "    z_bg = stats.norm(loc=loc, scale=scales[1]).pdf(resnorm)\n",
    "\n",
    "    fg_prob = z_fg/(z_fg+z_bg)\n",
    "    fg_prob = (fg_prob - fg_prob.min())/(fg_prob.max()-fg_prob.min())\n",
    "\n",
    "\n",
    "    # use KMeans clustering to assign each point to the foreground or background\n",
    "    km = KMeans(n_clusters=2)\n",
    "    group = km.fit_predict(fg_prob.reshape(-1,1))\n",
    "    centroids = np.array([np.mean(fg_prob[group==0]), np.mean(fg_prob[group==1])])\n",
    "\n",
    "    bad = group == np.argmin(centroids)\n",
    "        \n",
    "    return fg_prob, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matern_ephemeris = []\n",
    "matern_transit_times = []\n",
    "\n",
    "outlier_prob = []\n",
    "outlier_class = []\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    print(\"\\nPLANET\", npl)\n",
    "    \n",
    "    # grab data\n",
    "    xtime = indep_ephemeris[npl]\n",
    "    yomc  = indep_transit_times[npl] - indep_ephemeris[npl]\n",
    "\n",
    "    ymed = boxcar_smooth(ndimage.median_filter(yomc, size=5, mode=\"mirror\"), winsize=5)\n",
    "    out  = np.abs(yomc-ymed)/astropy.stats.mad_std(yomc-ymed) > 5.0\n",
    "    \n",
    "    \n",
    "    # compare various models\n",
    "    aiclist = []\n",
    "    biclist = []\n",
    "    outlist = []\n",
    "    fgplist = []\n",
    "    \n",
    "    if np.sum(~out) >= 16: max_polyorder = 3\n",
    "    elif np.sum(~out) >= 8: max_polyorder = 2\n",
    "    else: max_polyorder = 1\n",
    "    \n",
    "    for polyorder in range(1, max_polyorder+1):\n",
    "        \n",
    "        # build the OMC model\n",
    "        omc_model = build_omc_model(xtime[~out], yomc[~out], polyorder=polyorder, \n",
    "                                    period=omc_pers[npl], use_gp=False, xt_predict=xtime)\n",
    "\n",
    "        with omc_model:\n",
    "            omc_map = omc_model.test_point\n",
    "            \n",
    "            if omc_pers[npl] is not None:\n",
    "                omc_map = pmx.optimize(start=omc_map, vars=[omc_model.f, omc_model.Ah, omc_model.Bk])\n",
    "            if polyorder >= 1:\n",
    "                omc_map = pmx.optimize(start=omc_map, vars=[omc_model.C0, omc_model.C1])\n",
    "            if polyorder >= 2:\n",
    "                omc_map = pmx.optimize(start=omc_map, vars=[omc_model.C0, omc_model.C1, omc_model.C2])\n",
    "            if polyorder >= 3:\n",
    "                omc_map = pmx.optimize(start=omc_map, vars=[omc_model.C0, omc_model.C1, omc_model.C2, omc_model.C3])\n",
    "                \n",
    "            omc_map = pmx.optimize(start=omc_map)\n",
    "            omc_trace = pmx.sample(tune=5000, draws=1000, start=omc_map, chains=2, target_accept=0.9)\n",
    "\n",
    "        \n",
    "        # flag outliers via mixture model of the residuals\n",
    "        omc_trend = np.nanmedian(omc_trace[\"full_mean_pred\"], 0)\n",
    "        residuals = yomc - omc_trend\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.plot(xtime, yomc, \"k.\")\n",
    "        plt.plot(xtime, omc_trend, \"r\", lw=2)\n",
    "        plt.show()\n",
    "        \n",
    "        mix_model = build_mix_model(residuals)\n",
    "        \n",
    "        with mix_model:\n",
    "            mix_trace = pmx.sample(tune=3000, draws=1000, chains=1, target_accept=0.9)\n",
    "\n",
    "        loc = np.nanmedian(mix_trace[\"mu\"], axis=0)\n",
    "        scales = np.nanmedian(1/np.sqrt(mix_trace[\"tau\"]), axis=0)\n",
    "\n",
    "        fg_prob, bad = flag_res_outliers(residuals, loc, scales)\n",
    "        \n",
    "        outlist.append(bad)\n",
    "        fgplist.append(fg_prob)\n",
    "        \n",
    "        print(\"polyorder:\", polyorder)\n",
    "        print(\"loc:\", np.round(loc,3))\n",
    "        print(\"scale:\", np.round(scales,3))\n",
    "        print(\"{0} outliers found out of {1} transit times ({2}%)\".format(np.sum(bad), len(bad), \n",
    "                                                                          np.round(100.*np.sum(bad)/len(bad),1)))\n",
    "        \n",
    "        # calculate AIC & BIC\n",
    "        n = len(yomc)\n",
    "        k = polyorder + 3\n",
    "        \n",
    "        AIC = n*np.log(np.sum(residuals[~bad]**2)/np.sum(~bad)) + 2*k\n",
    "        BIC = n*np.log(np.sum(residuals[~bad]**2)/np.sum(~bad)) + k*np.log(n)\n",
    "        \n",
    "        aiclist.append(AIC)\n",
    "        biclist.append(BIC)\n",
    "        \n",
    "        print(\"AIC:\", np.round(AIC,1))\n",
    "        print(\"BIC:\", np.round(BIC,1))\n",
    "      \n",
    "    \n",
    "    # choose the best model and recompute\n",
    "    bad = outlist[np.argmin(aiclist)]\n",
    "    polyorder = 1 + np.argmin(aiclist)\n",
    "    \n",
    "    \n",
    "    omc_model = build_omc_model(xtime[~bad], yomc[~bad], polyorder=polyorder, period=omc_pers[npl], \n",
    "                                use_gp=False, xt_predict = full_indep_ephemeris[npl])\n",
    "\n",
    "    with omc_model:\n",
    "        omc_map = omc_model.test_point\n",
    "        if omc_pers[npl] is not None:\n",
    "            omc_map = pmx.optimize(start=omc_map, vars=[omc_model.f, omc_model.Ah, omc_model.Bk])\n",
    "        omc_map = pmx.optimize(start=omc_map)\n",
    "        omc_trace = pmx.sample(tune=10000, draws=1000, start=omc_map, chains=2, target_accept=0.95)\n",
    "    \n",
    "\n",
    "    omc_trend = np.nanmean(omc_trace[\"full_mean_pred\"], 0)\n",
    "    fg_prob = fgplist[np.argmin(aiclist)]\n",
    "\n",
    "    # save the final results\n",
    "    mtts = full_indep_ephemeris[npl] + omc_trend\n",
    "    mephem = poly.polyval(p.index, poly.polyfit(p.index, mtts, 1))\n",
    "                                                                          \n",
    "    matern_transit_times.append(mtts)\n",
    "    matern_ephemeris.append(mephem) \n",
    "    \n",
    "    outlier_prob.append(1-fg_prob)\n",
    "    outlier_class.append(bad)\n",
    "    \n",
    "    # plot the final trend and outliers\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.scatter(xtime, yomc*24*60, c=1-fg_prob, cmap=\"viridis\", label=\"MAP TTVs\")\n",
    "    plt.plot(xtime[bad], yomc[bad]*24*60, \"rx\")\n",
    "    plt.plot(full_indep_ephemeris[npl], omc_trend*24*60, \"k\", label=\"Quick model\")\n",
    "    plt.xlabel(\"Time [BJKD]\", fontsize=20)\n",
    "    plt.ylabel(\"O-C [min]\", fontsize=20)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend(fontsize=14, loc=\"upper right\")\n",
    "    plt.title(TARGET, fontsize=20)\n",
    "    plt.savefig(FIGURE_DIR + TARGET + '_omc_model_{0}.png'.format(npl), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(NPL, figsize=(12,3*NPL))\n",
    "if NPL == 1: axes = [axes]\n",
    "\n",
    "for npl, p in enumerate(planets):\n",
    "    xtime = matern_transit_times[npl]\n",
    "    yomc  = (matern_transit_times[npl] - matern_ephemeris[npl])*24*60\n",
    "    \n",
    "    axes[npl].plot(xtime, yomc, '.', c='C{0}'.format(npl))\n",
    "    axes[npl].set_ylabel('O-C [min]', fontsize=20)\n",
    "axes[NPL-1].set_xlabel('Time [BJKD]', fontsize=20)\n",
    "axes[0].set_title(TARGET, fontsize=20)\n",
    "plt.savefig(FIGURE_DIR + TARGET + '_ttvs_quick.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate TTV amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get estimate of ttv amplitude and a reasonable buffer\n",
    "ttv_amps   = np.zeros(NPL)\n",
    "ttv_buffer = np.zeros(NPL)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    # estimate TTV amplitude\n",
    "    ttv_amps[npl] = astropy.stats.mad_std(indep_transit_times[npl] - indep_ephemeris[npl])\n",
    "\n",
    "    # based on scatter in independent times, set threshold so not even one outlier is expected\n",
    "    N   = len(transit_inds[npl])\n",
    "    eta = np.max([3., stats.norm.interval((N-1)/N)[1]])\n",
    "\n",
    "    ttv_buffer[npl] = eta*ttv_amps[npl] + lcit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update and save TTVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quick_transit_times = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    p.tts = np.copy(matern_transit_times[npl])\n",
    "    quick_transit_times.append(p.tts[transit_inds[npl]])\n",
    "    \n",
    "\n",
    "for npl in range(NPL):\n",
    "    # update transit times in planet objects\n",
    "    p.tts = np.copy(matern_transit_times[npl])\n",
    "    \n",
    "    # Save Quick TTVs\n",
    "    data_out  = np.vstack([transit_inds[npl], quick_transit_times[npl]]).swapaxes(0,1)\n",
    "    fname_out = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_quick_ttvs.txt'\n",
    "    \n",
    "    np.savetxt(fname_out, data_out, fmt=('%1d', '%.8f'), delimiter='\\t')\n",
    "    \n",
    "    # Save MAP TTVs\n",
    "    data_out  = np.vstack([transit_inds[npl], indep_transit_times[npl]]).swapaxes(0,1)\n",
    "    fname_out = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_map_ttvs.txt'\n",
    "    \n",
    "    np.savetxt(fname_out, data_out, fmt=('%1d', '%.8f'), delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    # Save outlier probabilities\n",
    "    data_out  = np.vstack([transit_inds[npl], outlier_prob[npl], outlier_class[npl]]).swapaxes(0,1)\n",
    "    fname_out = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_omc_outlier_info.txt'\n",
    "    \n",
    "    np.savetxt(fname_out, data_out, fmt=('%1d', '%.8f', '%1d'), delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    # save everything all together\n",
    "    data_out  = np.vstack([transit_inds[npl],\n",
    "                           indep_transit_times[npl],\n",
    "                           quick_transit_times[npl],\n",
    "                           outlier_prob[npl], \n",
    "                           outlier_class[npl]]).swapaxes(0,1)\n",
    "    fname_out = QUICK_TTV_DIR + TARGET + '_{:02d}'.format(npl) + '_quick.ttvs'\n",
    "    \n",
    "    np.savetxt(fname_out, data_out, fmt=('%1d', '%.8f', '%.8f', '%.8f', '%1d'), delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag outliers based on transit model\n",
    "#### Cadences must be flagged as outliers from BOTH the Matern model and the Independent model to be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flagging remaining outliers\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_i = []\n",
    "res_m = []\n",
    "\n",
    "for j, q in enumerate(quarters):\n",
    "    print(\"QUARTER\", q)\n",
    "    \n",
    "    # grab time and flux data\n",
    "    if all_dtype[q] == \"long\":\n",
    "        use = lc.quarter == q\n",
    "        t_ = lc.time[use]\n",
    "        f_ = lc.flux[use]\n",
    "        \n",
    "    elif all_dtype[q] == \"short\":\n",
    "        use = sc.quarter == q\n",
    "        t_ = sc.time[use]\n",
    "        f_ = sc.flux[use]\n",
    "        \n",
    "    \n",
    "    # grab transit times for each planet\n",
    "    wp_i = []\n",
    "    tts_i = []\n",
    "    inds_i = []\n",
    "    \n",
    "    wp_m = []\n",
    "    tts_m = []\n",
    "    inds_m = []\n",
    "    \n",
    "    for npl in range(NPL):\n",
    "        itt = indep_transit_times[npl]\n",
    "        mtt = matern_transit_times[npl][transit_inds[npl]]\n",
    "        \n",
    "        use_i = (itt > t_.min())*(itt < t_.max())\n",
    "        use_m = (mtt > t_.min())*(mtt < t_.max())\n",
    "        \n",
    "        if np.sum(use_i) > 0:\n",
    "            wp_i.append(npl)\n",
    "            tts_i.append(itt[use_i])\n",
    "            inds_i.append(transit_inds[npl][use_i] - transit_inds[npl][use_i][0])\n",
    "            \n",
    "        if np.sum(use_m) > 0:\n",
    "            wp_m.append(npl)\n",
    "            tts_m.append(itt[use_m])\n",
    "            inds_m.append(transit_inds[npl][use_m] - transit_inds[npl][use_m][0])\n",
    "            \n",
    "\n",
    "    \n",
    "    # first check independent transit times\n",
    "    if len(tts) > 0:\n",
    "        # set up model\n",
    "        starrystar = exo.LimbDarkLightCurve(UCOEFFS)\n",
    "        orbit  = exo.orbits.TTVOrbit(transit_times=tts_i, transit_inds=inds_i, period=list(periods[wp_i]), \n",
    "                                     b=b[wp_i], r_star=RSTAR, m_star=Mstar)\n",
    "\n",
    "        # set oversampling factor\n",
    "        if all_dtype[q] == 'short':\n",
    "            oversample = 1\n",
    "            texp = scit\n",
    "        elif all_dtype[q] == 'long':\n",
    "            oversample = 15\n",
    "            texp = lcit\n",
    "\n",
    "        # calculate light curves\n",
    "        light_curves = starrystar.get_light_curve(orbit=orbit, r=rp[wp_i], t=t_, oversample=oversample, texp=texp)\n",
    "        model_flux = 1.0 + pm.math.sum(light_curves, axis=-1).eval()\n",
    "\n",
    "    else:\n",
    "        model_flux = np.ones_like(f_)*np.mean(f_)\n",
    "    \n",
    "    # calculate residuals\n",
    "    res_i.append(f_ - model_flux)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # then check matern transit times\n",
    "    if len(tts) > 0:\n",
    "        # set up model\n",
    "        starrystar = exo.LimbDarkLightCurve(UCOEFFS)\n",
    "        orbit  = exo.orbits.TTVOrbit(transit_times=tts_m, transit_inds=inds_m, period=list(periods[wp_m]), \n",
    "                                     b=b[wp_m], r_star=RSTAR, m_star=Mstar)\n",
    "\n",
    "        # set oversampling factor\n",
    "        if all_dtype[q] == 'short':\n",
    "            oversample = 1\n",
    "            texp = scit*1.0\n",
    "        elif all_dtype[q] == 'long':\n",
    "            oversample = 15\n",
    "            texp = lcit*1.0\n",
    "\n",
    "        # calculate light curves\n",
    "        light_curves = starrystar.get_light_curve(orbit=orbit, r=rp[wp_m], t=t_, oversample=oversample, texp=texp)\n",
    "        model_flux = 1.0 + pm.math.sum(light_curves, axis=-1).eval()\n",
    "\n",
    "    else:\n",
    "        model_flux = np.ones_like(f_)*np.mean(f_)\n",
    "    \n",
    "    # calculate residuals\n",
    "    res_m.append(f_ - model_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j, q in enumerate(quarters):\n",
    "    print(\"\\nQUARTER\", q)\n",
    "    res = 0.5*(res_i[j] + res_m[j])\n",
    "    x_ = np.arange(len(res))\n",
    "    \n",
    "    bad_i = np.abs(res_i[j] - np.mean(res_i[j]))/astropy.stats.mad_std(res_i[j]) > 5.0\n",
    "    bad_m = np.abs(res_m[j] - np.mean(res_m[j]))/astropy.stats.mad_std(res_m[j]) > 5.0\n",
    "    \n",
    "    bad = bad_i * bad_m\n",
    "    \n",
    "    print(\"   outliers rejected:\", np.sum(bad))\n",
    "    print(\"   marginal outliers:\", np.sum(bad_i*~bad_m)+np.sum(~bad_i*bad_m))\n",
    "    \n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.plot(x_, res, \"k\", lw=0.5)\n",
    "    plt.plot(x_[bad], res[bad], \"rx\")\n",
    "    plt.xlim(x_.min(), x_.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lc = []\n",
    "bad_sc = []\n",
    "\n",
    "for q in range(18):\n",
    "    if all_dtype[q] == \"long_no_transits\":\n",
    "        bad = np.ones(np.sum(lc.quarter == q), dtype=\"bool\")\n",
    "        bad_lc = np.hstack([bad_lc, bad])\n",
    "        \n",
    "        \n",
    "    if all_dtype[q] == \"short_no_transits\":\n",
    "        bad = np.ones(np.sum(sc.quarter == q), dtype=\"bool\")\n",
    "        bad_sc = np.hstack([bad_sc, bad])    \n",
    "    \n",
    "    \n",
    "    if (all_dtype[q] == \"short\") + (all_dtype[q] == \"long\"):\n",
    "        j = np.where(quarters == q)[0][0]\n",
    "\n",
    "        res = 0.5*(res_i[j] + res_m[j])\n",
    "        x_ = np.arange(len(res))\n",
    "\n",
    "        bad_i = np.abs(res_i[j] - np.mean(res_i[j]))/astropy.stats.mad_std(res_i[j]) > 5.0\n",
    "        bad_m = np.abs(res_m[j] - np.mean(res_m[j]))/astropy.stats.mad_std(res_m[j]) > 5.0\n",
    "\n",
    "        bad = bad_i * bad_m\n",
    "\n",
    "        if all_dtype[q] == \"short\":\n",
    "            bad_sc = np.hstack([bad_sc, bad])\n",
    "\n",
    "        if all_dtype[q] == \"long\":\n",
    "            bad_lc = np.hstack([bad_lc, bad])\n",
    "        \n",
    "        \n",
    "bad_lc = np.array(bad_lc, dtype=\"bool\")\n",
    "bad_sc = np.array(bad_sc, dtype=\"bool\")\n",
    "\n",
    "\n",
    "if sc is not None:\n",
    "    good_cadno_sc = sc.cadno[~bad_sc]\n",
    "    \n",
    "if lc is not None:\n",
    "    good_cadno_lc = lc.cadno[~bad_lc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detrend again with better estimates of transit timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset LONG CADENCE to raw MAST downloads\n",
    "if lc is not None:\n",
    "    lc_data = detrend.cleanup_lkfc(lc_raw_collection, KIC)\n",
    "    \n",
    "# make sure there is at least one transit in the long cadence data\n",
    "# this shouldn't be an issue for real KOIs, but can happen for simulated data\n",
    "if np.sum(np.array(all_dtype) == \"long\") == 0:\n",
    "    lc_data = []\n",
    "    \n",
    "    \n",
    "lc_quarters = []\n",
    "for i, lcd in enumerate(lc_data):\n",
    "    lc_quarters.append(lcd.quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset SHORT CADENCE to raw MAST downloads\n",
    "if sc is not None:\n",
    "    sc_data = detrend.cleanup_lkfc(sc_raw_collection, KIC)\n",
    "\n",
    "# make sure there is at least one transit in the short cadence data\n",
    "# this shouldn't be an issue for real KOIs, but can happen for simulated data\n",
    "if np.sum(np.array(all_dtype) == \"short\") == 0:\n",
    "    sc_data = []\n",
    "    \n",
    "    \n",
    "sc_quarters = []\n",
    "for i, scd in enumerate(sc_data):\n",
    "    sc_quarters.append(scd.quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert LightKurves to LiteCurves\n",
    "sc_lite = []\n",
    "lc_lite = []\n",
    "\n",
    "for i, scd in enumerate(sc_data):\n",
    "    sc_lite.append(LightKurve_to_LiteCurve(scd))\n",
    "    \n",
    "for i, lcd in enumerate(lc_data):\n",
    "    lc_lite.append(LightKurve_to_LiteCurve(lcd))\n",
    "    \n",
    "    \n",
    "# renaming here is an artifact of updates to my pipeline and dependencies\n",
    "# this is easier than going through and fixing all variable names\n",
    "sc_data = sc_lite\n",
    "lc_data = lc_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set break tolerance and nominal minimum oscillation period\n",
    "break_tolerance = np.max([int(DURS.min()/(LCIT/60/24)*5/2), 13])\n",
    "min_period = 1.0\n",
    "\n",
    "\n",
    "for i, lcd in enumerate(lc_data):\n",
    "    print(\"QUARTER {}\".format(lcd.quarter[0]))\n",
    "    \n",
    "    qmask = np.isin(lcd.cadno, good_cadno_lc)\n",
    "    lcd.remove_flagged_cadences(qmask)\n",
    "    \n",
    "    # make transit mask\n",
    "    lcd.mask = np.zeros(len(lcd.time), dtype=\"bool\")\n",
    "    for npl, p in enumerate(planets):\n",
    "        masksize = np.max([1/24, 0.5*p.duration + ttv_buffer[npl]])\n",
    "        lcd.mask += detrend.make_transitmask(lcd.time, p.tts, masksize)\n",
    "    \n",
    "    try:\n",
    "        lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period)\n",
    "    except:\n",
    "        warnings.warn(\"Initial detrending model failed...attempting to refit without exponential ramp component\")\n",
    "        try:\n",
    "            lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period, correct_ramp=False)\n",
    "        except:\n",
    "            warnings.warn(\"Detrending with RotationTerm failed...attempting to detrend with SHOTerm\")\n",
    "            lcd = detrend.flatten_with_gp(lcd, break_tolerance, min_period, kterm=\"SHOTerm\", correct_ramp=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lc_data) > 0:\n",
    "    lc = detrend.stitch(lc_data)\n",
    "else:\n",
    "    lc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set break tolerance and nominal minimum oscillation period\n",
    "break_tolerance = np.max([int(DURS.min()/(SCIT/3600/24)*5/2), 91])\n",
    "min_period = 1.0\n",
    "\n",
    "\n",
    "for i, scd in enumerate(sc_data):\n",
    "    print(\"QUARTER {}\".format(scd.quarter[0]))\n",
    "    \n",
    "    qmask = np.isin(scd.cadno, good_cadno_sc)\n",
    "    scd.remove_flagged_cadences(qmask)\n",
    "    \n",
    "    # make transit mask\n",
    "    scd.mask = np.zeros(len(scd.time), dtype=\"bool\")\n",
    "    for npl, p in enumerate(planets):\n",
    "        masksize = np.max([1/24, 0.5*p.duration + ttv_buffer[npl]])\n",
    "        scd.mask += detrend.make_transitmask(scd.time, p.tts, masksize)\n",
    "    \n",
    "    try:\n",
    "        scd = detrend.flatten_with_gp(scd, break_tolerance, min_period)\n",
    "    except:\n",
    "        warnings.warn(\"Initial detrending model failed...attempting to refit without exponential ramp component\")\n",
    "        try:\n",
    "            scd = detrend.flatten_with_gp(scd, break_tolerance, min_period, correct_ramp=False)\n",
    "        except:\n",
    "            warnings.warn(\"Detrending with RotationTerm failed...attempting to detrend with SHOTerm\")\n",
    "            scd = detrend.flatten_with_gp(scd, break_tolerance, min_period, kterm=\"SHOTerm\", correct_ramp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sc_data) > 0:\n",
    "    sc = detrend.stitch(sc_data)\n",
    "else:\n",
    "    sc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make individual mask for where each planet transits\n",
    "### These masks have width 1.5 transit durations, which may be wider than the masks used for detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if sc is not None:\n",
    "    sc_mask = np.zeros((NPL,len(sc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        sc_mask[npl] = detrend.make_transitmask(sc.time, p.tts, np.max([3/24,1.5*p.duration]))\n",
    "        \n",
    "    sc.mask = sc_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    sc_mask = None\n",
    "\n",
    "    \n",
    "if lc is not None:\n",
    "    lc_mask = np.zeros((NPL,len(lc.time)),dtype='bool')\n",
    "    for npl, p in enumerate(planets):\n",
    "        lc_mask[npl] = detrend.make_transitmask(lc.time, p.tts, np.max([3/24,1.5*p.duration]))\n",
    "        \n",
    "    lc.mask = lc_mask.sum(axis=0) > 0\n",
    "\n",
    "else:\n",
    "    lc_mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag high quality transits (quality = 1)\n",
    "\n",
    "### Good transits must have  at least 50% photometry coverage in/near transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl, p in enumerate(planets):\n",
    "    count_expect_lc = int(np.ceil(p.duration/lcit))\n",
    "    count_expect_sc = int(np.ceil(p.duration/scit))\n",
    "        \n",
    "    quality = np.zeros(len(p.tts), dtype=\"bool\")\n",
    "    \n",
    "    for i, t0 in enumerate(p.tts):\n",
    "        \n",
    "        if sc is not None:\n",
    "            in_sc = np.abs(sc.time - t0)/p.duration < 0.5\n",
    "            near_sc = np.abs(sc.time - t0)/p.duration < 1.5\n",
    "            \n",
    "            qual_in = np.sum(in_sc) > 0.5*count_expect_sc\n",
    "            qual_near = np.sum(near_sc) > 1.5*count_expect_sc\n",
    "            \n",
    "            quality[i] += qual_in*qual_near\n",
    "        \n",
    "        \n",
    "        if lc is not None:\n",
    "            in_lc = np.abs(lc.time - t0)/p.duration < 0.5\n",
    "            near_lc = np.abs(lc.time - t0)/p.duration < 1.5\n",
    "            \n",
    "            qual_in = np.sum(in_lc) > 0.5*count_expect_lc\n",
    "            qual_near = np.sum(near_lc) > 1.5*count_expect_lc\n",
    "            \n",
    "            quality[i] += qual_in*qual_near\n",
    "            \n",
    "    \n",
    "    p.quality = np.copy(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag which transits overlap (overlap = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify overlapping transits\n",
    "dur_max = np.max(DURS)\n",
    "overlap = []\n",
    "\n",
    "for i in range(NPL):\n",
    "    overlap.append(np.zeros(len(planets[i].tts), dtype='bool'))\n",
    "    \n",
    "    for j in range(NPL):\n",
    "        if i != j:\n",
    "            for ttj in planets[j].tts:\n",
    "                overlap[i] += np.abs(planets[i].tts - ttj)/dur_max < 1.5\n",
    "                \n",
    "    planets[i].overlap = np.copy(overlap[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('')\n",
    "print('cumulative runtime = ', int(timer() - global_start_time), 's')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make phase-folded transit plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for npl, p in enumerate(planets):\n",
    "    tts = p.tts[p.quality*~p.overlap]\n",
    "    \n",
    "    if len(tts) == 0:\n",
    "        print(\"No non-overlapping high quality transits found for planet {0} (P = {1} d)\".format(npl, p.period))\n",
    "    \n",
    "    else:\n",
    "        t_folded = []\n",
    "        f_folded = []\n",
    "\n",
    "        # grab the data\n",
    "        for t0 in tts:\n",
    "            if sc is not None:\n",
    "                use = np.abs(sc.time-t0)/p.duration < 1.5\n",
    "                \n",
    "                if np.sum(use) > 0:\n",
    "                    t_folded.append(sc.time[use]-t0)\n",
    "                    f_folded.append(sc.flux[use])\n",
    "                    \n",
    "            if lc is not None:\n",
    "                use = np.abs(lc.time-t0)/p.duration < 1.5\n",
    "                \n",
    "                if np.sum(use) > 0:\n",
    "                    t_folded.append(lc.time[use]-t0)\n",
    "                    f_folded.append(lc.flux[use])\n",
    "        \n",
    "        # sort the data\n",
    "        t_folded = np.hstack(t_folded)\n",
    "        f_folded = np.hstack(f_folded)\n",
    "\n",
    "        order = np.argsort(t_folded)\n",
    "        t_folded = t_folded[order]\n",
    "        f_folded = f_folded[order]\n",
    "        \n",
    "        # bin the data\n",
    "        t_binned, f_binned = bin_data(t_folded, f_folded, p.duration/11)\n",
    "        \n",
    "        # set undersampling factor and plotting limits\n",
    "        inds = np.arange(len(t_folded), dtype=\"int\")\n",
    "        inds = np.random.choice(inds, size=np.min([3000,len(inds)]), replace=False)\n",
    "        \n",
    "        ymin = 1 - 3*np.std(f_folded) - p.depth\n",
    "        ymax = 1 + 3*np.std(f_folded)\n",
    "        \n",
    "        # plot the data\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(t_folded[inds]*24, f_folded[inds], \".\", c=\"lightgrey\")\n",
    "        plt.plot(t_binned*24, f_binned, \"o\", ms=8, color=\"C{0}\".format(npl), label=\"{0}-{1}\".format(TARGET, npl))\n",
    "        plt.xlim(t_folded.min()*24, t_folded.max()*24)\n",
    "        plt.ylim(ymin, ymax)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.xlabel(\"Time from mid-transit [hrs]\", fontsize=20)\n",
    "        plt.ylabel(\"Flux\", fontsize=20)\n",
    "        plt.savefig(FIGURE_DIR + TARGET + '_{0:02d}_folded_transit_.png'.format(npl), bbox_inches='tight')\n",
    "        plt.legend(fontsize=20, loc=\"upper right\", framealpha=1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and model empirical autocorrelation function (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generating figures inside imported modules creates issues with UChicago Midway RCC cluster\n",
    "# it's easier to just define the function here in the main script\n",
    "\n",
    "def plot_acf(xcor, acf_emp, acf_mod, xf, yf, freqs, target_name, season):\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "\n",
    "    ax = plt.subplot2grid(shape=(5,10), loc=(0,0), rowspan=3, colspan=7)\n",
    "    ax.plot(xcor*24, acf_emp, color='lightgrey')\n",
    "    ax.plot(xcor*24, acf_mod, c='red')\n",
    "    ax.set_xlim(xcor.min()*24,xcor.max()*24)\n",
    "    ax.set_xticks(np.arange(0,xcor.max()*24,2))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_ylim(acf_emp.min()*1.1, acf_emp.max()*1.1)\n",
    "    ax.set_ylabel('ACF', fontsize=20)\n",
    "    ax.text(xcor.max()*24-0.15, acf_emp.max(), '%s, SEASON %d' %(target_name, season), va='top', ha='right', fontsize=20)\n",
    "\n",
    "\n",
    "    ax = plt.subplot2grid(shape=(5,10), loc=(0,7), rowspan=5, colspan=3)\n",
    "    ax.plot(xf/24/3600*1e3, yf, color='k', lw=0.5)\n",
    "    for f in freqs:\n",
    "        ax.axvline(f/24/3600*1e3, color='red', zorder=0, lw=3, alpha=0.3) \n",
    "    ax.set_xlim(xf.min()/24/3600*1e3, xf.max()/24/3600*1e3)\n",
    "    ax.set_ylim(yf.min(),1.2*yf.max())\n",
    "    ax.set_ylabel('Power', fontsize=20)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Frequency [mHz]', fontsize=20)\n",
    "\n",
    "    for i, sf in enumerate(np.sort(freqs)[::-1]):\n",
    "        ax.text(xf.min()/24/3600*1e3+0.1, yf.max()*(1.1-0.1*i), '%.2f min' %(24*60/sf), fontsize=16)\n",
    "\n",
    "\n",
    "    ax = plt.subplot2grid(shape=(5,10), loc=(3,0), rowspan=2, colspan=7)\n",
    "    ax.plot(xcor*24, acf_emp-acf_mod, c='lightgrey')\n",
    "    ax.set_xlim(xcor.min()*24,xcor.max()*24)\n",
    "    ax.set_xticks(np.arange(0,xcor.max()*24,2))\n",
    "    ax.set_xlabel('Lag time [hours]', fontsize=20)\n",
    "    ax.set_ylabel('Residuals', fontsize=20)    \n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what data type each season has\n",
    "season_dtype = []\n",
    "\n",
    "if sc is not None:\n",
    "    sc_seasons = np.unique(sc.season)\n",
    "else:\n",
    "    sc_seasons = np.array([])\n",
    "\n",
    "if lc is not None:\n",
    "    lc_seasons = np.unique(lc.season)\n",
    "else:\n",
    "    lc_seasons = np.array([])\n",
    "\n",
    "for z in range(4):\n",
    "    if np.isin(z, sc_seasons):\n",
    "        season_dtype.append(\"short\")\n",
    "    elif np.isin(z, lc_seasons):\n",
    "        season_dtype.append(\"long\")\n",
    "    else:\n",
    "        season_dtype.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Generating autocorrelation function\")\n",
    "print(\"Season data types:\", season_dtype, \"\\n\")\n",
    "\n",
    "# grab transit durations\n",
    "durations = np.zeros(NPL)\n",
    "for npl, p in enumerate(planets):\n",
    "    durations[npl] = p.duration\n",
    "\n",
    "    \n",
    "# set cutoff between low and high frequency signals\n",
    "fcut = 2/lcit\n",
    "fmin = 2/(5*durations.max())\n",
    "\n",
    "\n",
    "# now estimate the ACF\n",
    "acf_lag = []\n",
    "acf_emp = []\n",
    "acf_mod = []\n",
    "acf_freqs = []\n",
    "\n",
    "\n",
    "for z in range(4):\n",
    "    if season_dtype[z] == \"none\":\n",
    "        acf_lag.append(None)\n",
    "        acf_emp.append(None)\n",
    "        acf_mod.append(None)\n",
    "        acf_freqs.append(None)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        if season_dtype[z] == \"short\":\n",
    "            Npts = int(np.min([5*(1/24+durations.max()),2/3*periods.min()])/scit)\n",
    "            use = sc.season == z\n",
    "            m_ = sc.mask[use]\n",
    "\n",
    "            if np.sum(use) > 0:\n",
    "                t_ = sc.time[use][~m_]\n",
    "                f_ = sc.flux[use][~m_]\n",
    "                c_ = sc.cadno[use][~m_]\n",
    "                \n",
    "        if season_dtype[z] == \"long\":\n",
    "            Npts = int(np.min([5*(1/24+durations.max()),2/3*periods.min()])/lcit)\n",
    "            use = lc.season == z\n",
    "            m_ = lc.mask[use]\n",
    "\n",
    "            if np.sum(use) > 0:\n",
    "                t_ = lc.time[use][~m_]\n",
    "                f_ = lc.flux[use][~m_]\n",
    "                c_ = lc.cadno[use][~m_]\n",
    "\n",
    "                \n",
    "        if np.sum(use) > 0:\n",
    "            # generate the empirical acf (if generate_acf fails, use very low amplitude white noise)\n",
    "            try:\n",
    "                xcor, acor = noise.generate_acf(t_, f_, c_, Npts)\n",
    "            except:\n",
    "                try:\n",
    "                    Npts = int(2/3*Npts)\n",
    "                    xcor, acor = noise.generate_acf(t_, f_, c_, Npts)\n",
    "                except:\n",
    "                    xcor = 1 + np.arange(Npts, dtype=\"float\")\n",
    "                    acor = np.random.normal(size=len(xcor))*np.std(f_)*np.finfo(float).eps\n",
    "            \n",
    "            if season_dtype[z] == \"long\":\n",
    "                xcor = xcor*lcit\n",
    "                method = \"smooth\"\n",
    "                window_length = 3\n",
    "            \n",
    "            if season_dtype[z] == \"short\":\n",
    "                xcor = xcor*scit\n",
    "                method = \"savgol\"\n",
    "                window_length = None\n",
    "                \n",
    "\n",
    "            # model the acf\n",
    "            acor_emp, acor_mod, xf, yf, freqs = noise.model_acf(xcor, acor, fcut, fmin=fmin, \n",
    "                                                                method=method, window_length=window_length)\n",
    "\n",
    "            # make some plots\n",
    "            fig = plot_acf(xcor, acor_emp, acor_mod, xf, yf, freqs, TARGET, z)\n",
    "            fig.savefig(FIGURE_DIR + TARGET + \"_ACF_season_{0}.png\".format(z), bbox_inches=\"tight\")\n",
    "           \n",
    "            \n",
    "            # filter out high-frequency components in short cadence data\n",
    "            if season_dtype[z] == \"short\":\n",
    "                fring = list(freqs[freqs > fcut])\n",
    "                bw = 1/(lcit-scit) - 1/(lcit+scit)\n",
    "                \n",
    "                if len(fring) > 0:\n",
    "                    # apply the notch filter\n",
    "                    flux_filtered = detrend.filter_ringing(sc, break_tolerance, fring, bw)\n",
    "                    \n",
    "\n",
    "                    # search for addtional ringing frequencies\n",
    "                    try:\n",
    "                        xcor, acor = noise.generate_acf(t_, flux_filtered[use][~m_], c_, Npts)\n",
    "                        xcor = xcor*scit\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                                        \n",
    "                    new_freqs = noise.model_acf(xcor, acor, fcut, fmin=fmin, method=\"savgol\")[4]\n",
    "                    new_fring = new_freqs[new_freqs > fcut]\n",
    "                    \n",
    "                    for nf in new_fring:\n",
    "                        if np.sum(np.abs(fring-nf) < bw) == 0:\n",
    "                            fring.append(nf)\n",
    "                    \n",
    "                    # re-apply the notch filter with the new list of ringing frequencies\n",
    "                    flux_filtered = detrend.filter_ringing(sc, break_tolerance, fring, bw)\n",
    "                    \n",
    "                    \n",
    "                    # update the LiteCurve\n",
    "                    sc.flux[use] = flux_filtered[use]\n",
    "                    f_ = sc.flux[use][~m_]\n",
    "\n",
    "                \n",
    "                # re-run the ACF modeling on the filtered lightcurve\n",
    "                try:\n",
    "                    xcor, acor = noise.generate_acf(t_, f_, c_, Npts)\n",
    "                    xcor = xcor*scit \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "                acor_emp, acor_mod, xf, yf, freqs = noise.model_acf(xcor, acor, fcut, fmin=fmin, method=\"savgol\")\n",
    "\n",
    "                fig = plot_acf(xcor, acor_emp, acor_mod, xf, yf, freqs, TARGET, z)\n",
    "                fig.savefig(FIGURE_DIR + TARGET + \"_ACF_season_{0}_filtered.png\".format(z), bbox_inches=\"tight\")\n",
    "                \n",
    "            \n",
    "            # add to list\n",
    "            acf_lag.append(xcor)\n",
    "            acf_emp.append(acor_emp)\n",
    "            acf_mod.append(acor_mod)\n",
    "            acf_freqs.append(freqs)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            acf_lag.append(None)\n",
    "            acf_emp.append(None)\n",
    "            acf_mod.append(None)\n",
    "            acf_freqs.append(None)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and model synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating synthetic noise\\n\")\n",
    "\n",
    "synth_time  = []\n",
    "synth_red   = []\n",
    "synth_white = []\n",
    "\n",
    "\n",
    "for z in range(4):\n",
    "    print(\"SEASON\")\n",
    "    print(season_dtype[z])\n",
    "    \n",
    "    if season_dtype[z] == \"none\":\n",
    "        synth_time.append(None)\n",
    "        synth_red.append(None)\n",
    "        synth_white.append(None)\n",
    "       \n",
    "        \n",
    "    else:\n",
    "        if season_dtype[z] == \"short\":\n",
    "            Npts = int(2*durations.max()/scit)\n",
    "            use = sc.season == z\n",
    "            m_ = sc.mask[use]\n",
    "\n",
    "            if np.sum(use) > 0:\n",
    "                t_ = sc.time[use][~m_]\n",
    "                f_ = sc.flux[use][~m_]\n",
    "                \n",
    "        \n",
    "        if season_dtype[z] == \"long\":\n",
    "            Npts = int(5*durations.max()/lcit)\n",
    "            use = lc.season == z\n",
    "            m_ = lc.mask[use]\n",
    "\n",
    "            if np.sum(use) > 0:\n",
    "                t_ = lc.time[use][~m_]\n",
    "                f_ = lc.flux[use][~m_]\n",
    "                \n",
    "                \n",
    "        if np.sum(use) > 0:\n",
    "            if season_dtype[z] == \"long\":\n",
    "                vector_length = 7*len(acf_lag[z])\n",
    "            if season_dtype[z] == \"short\":\n",
    "                vector_length = 3*len(acf_lag[z])\n",
    "            \n",
    "            \n",
    "            # pull and split high/low frequencies\n",
    "            freqs = np.copy(acf_freqs[z])\n",
    "\n",
    "            low_freqs  = freqs[freqs <= fcut]\n",
    "            high_freqs = freqs[freqs > fcut]\n",
    "\n",
    "\n",
    "            # generate some synthetic correlated noise\n",
    "            clipped_acf = (acf_mod[z][:Npts])*np.linspace(1,0,Npts)\n",
    "\n",
    "            x, red_noise, white_noise = noise.generate_synthetic_noise(acf_lag[z][:Npts], clipped_acf, \n",
    "                                                                       vector_length, np.std(f_))\n",
    "\n",
    "            # add to list\n",
    "            synth_time.append(x)\n",
    "            synth_red.append(red_noise)\n",
    "            synth_white.append(white_noise)\n",
    "\n",
    "\n",
    "            # plot the noise\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.plot(x, white_noise + red_noise, \".\", c=\"lightgrey\")\n",
    "            plt.plot(x, red_noise, c=\"r\", lw=4, label=\"{0}, SEASON {1}\".format(TARGET, z))\n",
    "            plt.axhline(depths.max(), c=\"k\", ls=\":\", lw=2)\n",
    "            plt.axhline(depths.min(), c=\"k\", ls=\"--\", lw=2)\n",
    "            plt.axhline(-depths.min(), c=\"k\", ls=\"--\", lw=2)\n",
    "            plt.axhline(-depths.max(), c=\"k\", ls=\":\", lw=2)\n",
    "            plt.xlim(x.min(),x.max())\n",
    "            plt.ylim(np.percentile(white_noise,1), np.percentile(white_noise,99))\n",
    "            plt.xlabel(\"Time [days]\", fontsize=24)\n",
    "            plt.ylabel(\"Flux\", fontsize=24)\n",
    "            plt.xticks(fontsize=14)\n",
    "            plt.yticks(fontsize=14)\n",
    "            plt.legend(fontsize=20, loc=\"upper right\", framealpha=1)\n",
    "            plt.savefig(FIGURE_DIR + TARGET + \"_synthetic_noise_season_{0}.png\".format(z), bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "        else:\n",
    "            synth_time.append(None)\n",
    "            synth_red.append(None)\n",
    "            synth_white.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting a GP to synthetic noise\\n\")\n",
    "\n",
    "gp_priors = []\n",
    "\n",
    "\n",
    "for z in range(4):\n",
    "    if season_dtype[z] == \"none\":\n",
    "        gp_priors.append(None)\n",
    "       \n",
    "    else:\n",
    "        srz = synth_red[z]\n",
    "        \n",
    "        # pull and split high/low frequencies\n",
    "        freqs = np.copy(acf_freqs[z])\n",
    "        \n",
    "        if freqs is not None:\n",
    "\n",
    "            low_freqs  = freqs[freqs <= fcut]\n",
    "            high_freqs = freqs[freqs > fcut]\n",
    "\n",
    "            if len(low_freqs) > 0:\n",
    "                lf = low_freqs[0]\n",
    "            else:\n",
    "                lf = None\n",
    "                \n",
    "            if len(high_freqs) > 0:\n",
    "                warnings.warn(\"there are remaining high-frequency noise components\")\n",
    "                \n",
    "        else:\n",
    "            lf = None\n",
    "\n",
    "\n",
    "        # fit a GP model to the synthetic noise\n",
    "        try:            \n",
    "            gp_model = noise.build_sho_model(synth_time[z], \n",
    "                                             srz + np.random.normal(srz)*np.std(srz)*0.1, \n",
    "                                             var_method = \"fit\", \n",
    "                                             fmax = 2/lcit, \n",
    "                                             f0 = lf)\n",
    "        \n",
    "            with gp_model:\n",
    "                gp_map = gp_model.test_point\n",
    "\n",
    "                for mv in gp_model.vars:\n",
    "                    gp_map = pmx.optimize(start=gp_map, vars=[mv])\n",
    "\n",
    "                gp_map = pmx.optimize(start=gp_map)\n",
    "                \n",
    "                try:\n",
    "                    gp_trace = pmx.sample(tune=6000, draws=1500, start=gp_map, chains=2, target_accept=0.9)\n",
    "                except:\n",
    "                    gp_trace = pmx.sample(tune=12000, draws=1500, start=gp_map, chains=2, target_accept=0.95)\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            gp_model = noise.build_sho_model(synth_time[z], \n",
    "                                             srz + np.random.normal(srz)*np.std(srz)*0.1,\n",
    "                                             var_method = \"local\",\n",
    "                                             fmax = 2/lcit,\n",
    "                                             f0 = lf, \n",
    "                                             Q0 = 1/np.sqrt(2))\n",
    "        \n",
    "            with gp_model:\n",
    "                gp_map = gp_model.test_point\n",
    "\n",
    "                for mv in gp_model.vars:\n",
    "                    gp_map = pmx.optimize(start=gp_map, vars=[mv])\n",
    "\n",
    "                gp_map = pmx.optimize(start=gp_map)\n",
    "                \n",
    "                try:\n",
    "                    gp_trace = pmx.sample(tune=12000, draws=1500, start=gp_map, chains=2, target_accept=0.95)\n",
    "                except:\n",
    "                    gp_trace = gp_map\n",
    "            \n",
    "        \n",
    "        # save the priors\n",
    "        gp_priors.append(noise.make_gp_prior_dict(gp_trace))\n",
    "\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.plot(synth_time[z], synth_red[z], c=\"pink\", lw=4, label=\"{0}, SEASON {1}\".format(TARGET, z))\n",
    "        plt.plot(synth_time[z], np.nanmedian(gp_trace[\"pred\"], axis=0), c=\"red\")\n",
    "        plt.xlim(synth_time[z].min(), synth_time[z].max())\n",
    "        plt.xlabel(\"Time [days]\", fontsize=24)\n",
    "        plt.ylabel(\"Flux\", fontsize=24)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.legend(loc=\"upper right\", fontsize=20)\n",
    "        plt.savefig(FIGURE_DIR + TARGET + \"_GP_noise_model_{0}.png\".format(z), bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of all quarters with data\n",
    "quarters = []\n",
    "\n",
    "if lc is not None:\n",
    "    quarters = np.hstack([quarters, np.unique(lc.quarter)])\n",
    "if sc is not None:\n",
    "    quarters = np.hstack([quarters, np.unique(sc.quarter)])\n",
    "\n",
    "quarters = np.array(np.sort(quarters), dtype=\"int\")\n",
    "\n",
    "# get variance of each quarter\n",
    "var_by_quarter = []\n",
    "\n",
    "for i, q in enumerate(quarters):\n",
    "    if (sc is not None) and (np.sum(sc.quarter==q) > 0):\n",
    "        var_by_quarter.append(np.var(sc.flux[sc.mask*(sc.quarter==q)]))\n",
    "    elif (lc is not None) and (np.sum(lc.quarter==q) > 0):\n",
    "        var_by_quarter.append(np.var(lc.flux[lc.mask*lc.quarter==q]))\n",
    "    else:\n",
    "        var_by_quarter.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save detrended lightcurves and estimates of the noise properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save detrended lightcurves as .fits files\n",
    "try:\n",
    "    lc.to_fits(TARGET, DLC_DIR + TARGET + \"_lc_detrended.fits\")\n",
    "except:\n",
    "    print(\"No long cadence data\")\n",
    "\n",
    "try:\n",
    "    sc.to_fits(TARGET, DLC_DIR + TARGET + \"_sc_detrended.fits\")\n",
    "except:\n",
    "    print(\"No short cadence data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save var_by_quarter\n",
    "data_out  = np.vstack([quarters, var_by_quarter]).swapaxes(0,1)\n",
    "fname_out = NOISE_DIR + TARGET + \"_var_by_quarter.txt\"\n",
    "    \n",
    "np.savetxt(fname_out, data_out, fmt=(\"%1d', '%.15f\"), delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "# save gp_priors\n",
    "for z in range(4):\n",
    "    try:\n",
    "        for k in gp_priors[z].keys():\n",
    "            gp_priors[z][k] = list(gp_priors[z][k])\n",
    "\n",
    "        fname_out = NOISE_DIR + TARGET + \"_shoterm_gp_priors_{0}.txt\".format(z)\n",
    "\n",
    "        with open(fname_out, 'w') as file:\n",
    "            json.dump(gp_priors[z], file)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TOTAL RUNTIME = %.2f min\" %((timer()-global_start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
