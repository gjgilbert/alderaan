{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import astropy.stats\n",
    "from   astropy.io import fits as pyfits\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from   timeit import default_timer as timer\n",
    "import warnings\n",
    "import corner\n",
    "\n",
    "from alderaan.constants import *\n",
    "import alderaan.io as io\n",
    "\n",
    "# flush buffer to avoid mixed outputs from progressbar\n",
    "sys.stdout.flush()\n",
    "\n",
    "# turn off FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select mission, target, and paths\n",
    "MISSION = \"Kepler\"\n",
    "TARGET  = \"K02150\"\n",
    "PRIMARY_DIR = '/Users/research/projects/alderaan/'\n",
    "TRACE_FILE  = '/Users/research/projects/alderaan/Traces/' + TARGET + '/' + TARGET + '_transit_shape.fits'\n",
    "\n",
    "if MISSION == \"Simulated\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/simulated_catalog.csv\"\n",
    "    TRUE_TTV_DIR = PRIMARY_DIR + \"Simulations/TTVs/\"\n",
    "    \n",
    "if MISSION == \"Kepler\":\n",
    "    CSV_FILE = PRIMARY_DIR + \"Catalogs/cumulative_koi_catalog.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the necessary paths exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which to find lightcurve data\n",
    "if MISSION == 'Kepler': DOWNLOAD_DIR = PRIMARY_DIR + 'MAST_downloads/'\n",
    "if MISSION == 'Simulated': DOWNLOAD_DIR = PRIMARY_DIR + 'Simulations/'\n",
    "\n",
    "# directories in which to place pipeline outputs    \n",
    "FIGURE_DIR    = PRIMARY_DIR + 'Figures/' + TARGET + '/'\n",
    "TRACE_DIR     = PRIMARY_DIR + 'Traces/' + TARGET + '/'\n",
    "QUICK_TTV_DIR = PRIMARY_DIR + 'QuickTTVs/' + TARGET + '/'\n",
    "DLC_DIR       = PRIMARY_DIR + 'Detrended_lightcurves/' + TARGET + '/'\n",
    "NOISE_DIR     = PRIMARY_DIR + 'Noise_models/' + TARGET + '/'\n",
    "\n",
    "# check if all the paths exist and create them if not\n",
    "if os.path.exists(FIGURE_DIR) == False:\n",
    "    os.mkdir(FIGURE_DIR)\n",
    "    \n",
    "if os.path.exists(TRACE_DIR) == False:\n",
    "    os.mkdir(TRACE_DIR)\n",
    "    \n",
    "if os.path.exists(QUICK_TTV_DIR) == False:\n",
    "    os.mkdir(QUICK_TTV_DIR)\n",
    "    \n",
    "if os.path.exists(DLC_DIR) == False:\n",
    "    os.mkdir(DLC_DIR)\n",
    "    \n",
    "if os.path.exists(NOISE_DIR) == False:\n",
    "    os.mkdir(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in pre-constrained stellar parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from csv file\n",
    "print('Reading in data from csv file')\n",
    "\n",
    "# read in a csv file containing info on targets\n",
    "csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "# put these csv data into a dictionary\n",
    "target_dict = {}\n",
    "for k in csv_keys: \n",
    "    target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "    \n",
    "if MISSION == 'Kepler':\n",
    "    KOI_ID = TARGET\n",
    "    \n",
    "elif MISSION == 'Simulated':\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"MISSION must be 'Kepler' or 'Simulated'\")\n",
    "    \n",
    "    \n",
    "# pull relevant quantities and establish GLOBAL variables\n",
    "use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "\n",
    "KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "NPL = np.array(target_dict['npl'], dtype='int')[use]\n",
    "\n",
    "RSTAR_TRUE = np.array(target_dict['rstar'],  dtype='float')[use]\n",
    "\n",
    "LOGRHO_TRUE = np.array(target_dict['logrho'], dtype='float')[use]\n",
    "LOGRHO_ERR1_TRUE = np.array(target_dict['logrho_err1'], dtype='float')[use]\n",
    "LOGRHO_ERR2_TRUE = np.array(target_dict['logrho_err2'], dtype='float')[use]\n",
    "\n",
    "U1_TRUE = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "U2_TRUE = np.array(target_dict['limbdark_2'], dtype='float')[use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some consistency checks\n",
    "if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "if all(n == NPL[0] for n in NPL): NPL = NPL[0]\n",
    "else: raise ValueError('There are inconsistencies with NPL in the csv input file')\n",
    "\n",
    "if all(r == RSTAR_TRUE[0] for r in RSTAR_TRUE): RSTAR_TRUE = RSTAR_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with RSTAR in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_TRUE[0] for r in LOGRHO_TRUE): LOGRHO_TRUE = LOGRHO_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR1_TRUE[0] for r in LOGRHO_ERR1_TRUE): LOGRHO_ERR1_TRUE = LOGRHO_ERR1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR1 in the csv input file')\n",
    "\n",
    "if all(r == LOGRHO_ERR2_TRUE[0] for r in LOGRHO_ERR2_TRUE): LOGRHO_ERR2_TRUE = LOGRHO_ERR2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with LOGRHO_ERR2 in the csv input file')\n",
    "\n",
    "if all(u == U1_TRUE[0] for u in U1_TRUE): U1_TRUE = U1_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "if all(u == U2_TRUE[0] for u in U2_TRUE): U2_TRUE = U2_TRUE[0]\n",
    "else: raise ValueError('There are inconsistencies with U2 in the csv input file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_TRUE = 10**(LOGRHO_TRUE)\n",
    "RHO_ERR1_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR1_TRUE) - RHO_TRUE\n",
    "RHO_ERR2_TRUE = 10**(LOGRHO_TRUE + LOGRHO_ERR2_TRUE) - RHO_TRUE\n",
    "\n",
    "RHO_ERR_TRUE = np.sqrt(RHO_ERR1_TRUE**2 + RHO_ERR2_TRUE**2)/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get shape model posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyfits.open(TRACE_FILE) as trace:\n",
    "    print(trace.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in the fits file with saved traces\n",
    "\n",
    "with pyfits.open(TRACE_FILE) as trace:\n",
    "    header  = trace[0].header\n",
    "    hdulist = pyfits.HDUList(trace)\n",
    "    \n",
    "    NDRAWS, NPL = trace['P'].shape\n",
    "    \n",
    "    # limb darkening parameters\n",
    "    U = trace['U'].data\n",
    "    U1, U2 = U[:,0], U[:,1]\n",
    "    \n",
    "    # basis parameters\n",
    "    C0 = trace['C0'].data\n",
    "    C1 = trace['C1'].data\n",
    "    B  = trace['B'].data\n",
    "    LOG_R = trace['LOG_R'].data/np.log(10)\n",
    "    LOG_DUR = trace['LOG_DUR'].data/np.log(10)\n",
    "    \n",
    "    # physical parameters\n",
    "    P   = trace['P'].data\n",
    "    T0  = trace['T0'].data\n",
    "    R   = trace['R'].data\n",
    "    DUR = trace['DUR'].data\n",
    "    RHO = trace['RHO'].data\n",
    "    \n",
    "    # grazing coordinate\n",
    "    GAMMA = trace[\"GAMMA\"].data\n",
    "    \n",
    "    # TTV parameters\n",
    "    TTS = [None]*NPL\n",
    "\n",
    "    for npl in range(NPL):    \n",
    "        TTS[npl] = trace['TTS_{0}'.format(npl)].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyfits.open(TRACE_FILE) as trace:\n",
    "    header  = trace[0].header\n",
    "    hdulist = pyfits.HDUList(trace)\n",
    "    \n",
    "    NDRAWS, NPL = trace['R'].shape\n",
    "    \n",
    "    \n",
    "    # GP parameters\n",
    "    LOGSW4 = np.zeros((NDRAWS,4))\n",
    "    LOGW0  = np.zeros((NDRAWS,4))\n",
    "    LOGQ   = np.zeros((NDRAWS,4))\n",
    "    \n",
    "    for z in range(4):\n",
    "        try: LOGSW4[:,z] = trace['LOGSW4_{0}'.format(z)].data\n",
    "        except: pass\n",
    "        \n",
    "        try: LOGW0[:,z] = trace['LOGW0_{0}'.format(z)].data\n",
    "        except: pass\n",
    "        \n",
    "        try: LOGQ[:,z] = trace['LOGQ_{0}'.format(z)].data\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    per = (np.median(P[:,npl]),  np.std(P[:,npl]))\n",
    "    r   = (np.median(R[:,npl]), astropy.stats.mad_std(R[:,npl]))\n",
    "    b   = (np.median(B[:,npl]),  astropy.stats.mad_std(B[:,npl]))\n",
    "        \n",
    "    print(\"\\nPLANET {0}\".format(npl))\n",
    "    print(\"  period = {:.3f} +/- {:.3f}\\t[days]\".format(per[0],per[1]))\n",
    "    print(\"  rp/Rs  = {:.3f} +/- {:.3f}\".format(r[0],r[1]))\n",
    "    print(\"  impact = {:.3f} +/- {:.3f}\".format(b[0],b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For simulated data, read in ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MISSION == \"Simulated\":\n",
    "    # Read in the data from csv file\n",
    "    print('Reading in simulated \"ground truth\" data from csv file\\n')\n",
    "\n",
    "    # read in a csv file containing info on targets\n",
    "    csv_keys, csv_values = io.read_csv_file(CSV_FILE)\n",
    "\n",
    "    # put these csv data into a dictionary\n",
    "    target_dict = {}\n",
    "    for k in csv_keys: \n",
    "        target_dict[k] = io.get_csv_data(k, csv_keys, csv_values)\n",
    "\n",
    "\n",
    "    # pull relevant quantities and establish GLOBAL variables\n",
    "    KOI_ID = \"K\" + TARGET[1:]\n",
    "\n",
    "    use = np.array(target_dict['koi_id']) == KOI_ID\n",
    "    KIC = np.array(target_dict['kic_id'], dtype='int')[use]\n",
    "\n",
    "    u1_true = np.array(target_dict['limbdark_1'], dtype='float')[use]\n",
    "    u2_true = np.array(target_dict['limbdark_2'], dtype='float')[use]\n",
    "\n",
    "    P_true  = np.array(target_dict['period'], dtype='float')[use]\n",
    "    T0_true = np.array(target_dict['epoch'],  dtype='float')[use]\n",
    "    rp_true = np.array(target_dict['prad'], dtype='float')[use]\n",
    "    b_true  = np.array(target_dict['impact'], dtype='float')[use]\n",
    "    \n",
    "    \n",
    "    # do some consistency checks\n",
    "    if all(k == KIC[0] for k in KIC): KIC = KIC[0]\n",
    "    else: raise ValueError('There are inconsistencies with KIC in the csv input file')\n",
    "\n",
    "    if all(u == u1_true[0] for u in u1_true): u1_true = u1_true[0]\n",
    "    else: raise ValueError('There are inconsistencies with U1 in the csv input file')\n",
    "\n",
    "    if all(u == u2_true[0] for u in u2_true): u2_true = u2_true[0]\n",
    "    else: raise ValueError('There are inconsistencies with U2 in the csv input file')\n",
    "        \n",
    "        \n",
    "    # sort planet truths by period\n",
    "    order = np.argsort(P_true)\n",
    "\n",
    "    P_true  = P_true[order]\n",
    "    T0_true = T0_true[order]\n",
    "    rp_true = rp_true[order]\n",
    "    b_true  = b_true[order]\n",
    "    \n",
    "    \n",
    "    print(\"true radii:\", rp_true)\n",
    "    print(\"true impact:\", b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limb darkening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([U1, U2]).swapaxes(0,1)\n",
    "labels = ['U1', 'U2']\n",
    "\n",
    "if MISSION == \"Simulated\":\n",
    "    truths = [u1_true, u2_true]\n",
    "else:\n",
    "    truths = None\n",
    "\n",
    "fig = corner.corner(data, labels=labels, truths=truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    data = np.stack([C0[:,npl], C1[:,npl], B[:,npl], LOG_R[:,npl], LOG_DUR[:,npl]]).swapaxes(0,1)\n",
    "    labels = [r\"$C_0$\", r\"$C_1$\", r\"$b$\", r\"$\\log r$\", r\"$\\log T$\"]\n",
    "\n",
    "    fig = corner.corner(data, labels=labels, color='C{0}'.format(npl), truth_color=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    data = np.stack([P[:,npl], T0[:,npl], R[:,npl], B[:,npl], RHO[:,npl], DUR[:,npl]]).swapaxes(0,1)\n",
    "    labels = [\"P\", r\"$t_0$\", \"r\", \"b\", r\"$\\rho_{circ}$\", \"T\"]\n",
    "    \n",
    "    \n",
    "    if MISSION == \"Simulated\":\n",
    "        truths = [None, None, rp_true[npl], b_true[npl], None, None]\n",
    "    else:\n",
    "        truths = None\n",
    "\n",
    "\n",
    "    fig = corner.corner(data, labels=labels, truths=truths, color='C{0}'.format(npl), truth_color=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closer investigation of near-grazing transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    data = np.stack([B[:,npl], R[:,npl], DUR[:,npl]]).swapaxes(0,1)\n",
    "    labels = [\"b\", \"r\", \"T\"]\n",
    "\n",
    "    fig = corner.corner(data, labels=labels, color='C{0}'.format(npl));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for npl in range(NPL):\n",
    "    g = GAMMA[:,npl]\n",
    "    bins = np.linspace(-1,np.percentile(g,99.9),55)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(g, bins=bins, histtype=\"step\", density=True, lw=2, color=\"C{0}\".format(npl))\n",
    "    plt.axvline(1, color=\"k\", ls=\":\")\n",
    "    plt.title(r\"$\\gamma$\", fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the distribution of $g \\equiv \\rho_{\\rm circ}/\\rho_{\\rm obs}$ multimodal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for npl in range(NPL):\n",
    "    data.append((np.log10(RHO[:,npl]/RHO_TRUE))/3)\n",
    "    labels.append(r\"$\\log(g_{0})$\".format(npl+1))\n",
    "\n",
    "data = np.stack(data).swapaxes(0,1)\n",
    "\n",
    "fig = corner.corner(data, labels=labels, color=\"k\", truth_color=\"C0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for z in range(4):\n",
    "    data  = []\n",
    "    labels = []\n",
    "\n",
    "    if np.sum(LOGSW4[:,z] != 0):\n",
    "        data.append(LOGSW4[:,z])\n",
    "        labels.append('LogSw4')\n",
    "        \n",
    "    if np.sum(LOGW0[:,z] != 0):\n",
    "        data.append(LOGW0[:,z])\n",
    "        labels.append('Logw0')    \n",
    "    \n",
    "    if np.sum(LOGQ[:,z] != 0):\n",
    "        data.append(LOGQ[:,z])\n",
    "        labels.append('LogQ')\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        data = np.stack(data).swapaxes(0,1)\n",
    "        fig = corner.corner(data, labels=labels)\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try reweighting for eccentricity posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "# For some reason, ChainConsumer can't find Latex unless I make a matplotlib plot first\n",
    "# This is a workaround -- I'll find a real solution later\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.plot(np.linspace(0,1,2), \"k:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate chains by this amount\n",
    "upsample = 1000 \n",
    "\n",
    "# preconstrained density (e.g. from GaiaDR2/Berger+2020 isochrones)\n",
    "rho_obs = (RHO_TRUE, RHO_ERR_TRUE)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    \n",
    "    # draw samples in (e,w) and calculate log(weight)\n",
    "    rho_circ = np.repeat(RHO[:,npl], upsample)\n",
    "    ecc = np.random.uniform(0, 1, len(rho_circ))\n",
    "    omega = np.random.uniform(-0.5*np.pi, 1.5*np.pi, len(rho_circ))\n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_circ / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "    weight = np.exp(log_weight - np.max(log_weight))\n",
    "    \n",
    "    \n",
    "    # now plot it\n",
    "    c = ChainConsumer()\n",
    "    c.add_chain(np.vstack((ecc,omega*180/pi)).T, weights=weight,parameters=[r'$e$',r'$\\omega$'])\n",
    "    fig = c.plotter.plot()\n",
    "    fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we set priors on eccentricity to recover inclination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate chains by this amount\n",
    "upsample = 1000 \n",
    "\n",
    "# preconstrained density (e.g. from GaiaDR2/Berger+2020 isochrones)\n",
    "rho_obs = (RHO_TRUE, RHO_ERR_TRUE)\n",
    "\n",
    "for npl in range(NPL):\n",
    "    \n",
    "    # draw samples in (e,w) and calculate log(weight) for a given Rayleigh scale\n",
    "    rho_circ = np.repeat(RHO[:,npl], upsample)\n",
    "    \n",
    "    sig_e = 0.0355\n",
    "    esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*len(rho_circ)).reshape(2,-1)\n",
    "    ecc = np.sqrt(esinw**2 + ecosw**2)\n",
    "    omega = np.arctan2(esinw, ecosw)\n",
    "    \n",
    "    while np.any(ecc >= 1):\n",
    "        print(\"redrawing\", np.sum(ecc>=1))\n",
    "        esinw, ecosw = np.random.normal(loc=0, scale=sig_e, size=2*np.sum(ecc>=1)).reshape(2,-1)\n",
    "        omega[ecc>=1] = np.arctan2(esinw, ecosw)\n",
    "        ecc[ecc>=1] = np.sqrt(esinw**2 + ecosw**2)\n",
    "    \n",
    "    \n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_circ / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "    weight = np.exp(log_weight - np.max(log_weight))\n",
    "\n",
    "    # upsample impact parameter\n",
    "    b = np.repeat(B[:,npl], upsample)\n",
    "    \n",
    "    # now plot it\n",
    "    c = ChainConsumer()\n",
    "    c.add_chain(np.vstack((ecc,b)).T, weights=weight,parameters=[r'$e$',r'$b$'])\n",
    "    fig = c.plotter.plot()\n",
    "    fig.set_size_inches(3 + fig.get_size_inches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
