{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior_dict(sho_trace, low_freqs, high_freqs):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of (median, std) for each hyperparameter from a GP noise model\n",
    "    The expected sho_trace should be the output of a PyMC3/Exoplanet model built with noise.build_sho_model()\n",
    "    \n",
    "    Assumes a specific set of input variable names from sho_trace:\n",
    "      - ['logw0', 'logSw4', 'logQ', 'logw0_x', 'logSw4_x', 'logQ_x', 'logS1', 'logS2', 'logQ1']\n",
    "      - cannot have, e.g. both logw0 & logw0_x; both will be mapped to logw0\n",
    "      - can have any other subset of inputs\n",
    "      - maps logQ1 to logQh\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    sho_trace : PyMC3 multitrace\n",
    "        trace output of a PyMC3/Exoplanet model built with noise.build_sho_model()\n",
    "    low_freqs : list\n",
    "        fixed (regular, not angular) frequency corresponding to 'logSw4'\n",
    "    high_freqs : list\n",
    "        fixed (regular, not angular) frequencies corresponding to 'logS1' and 'logS2'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    priors : dict\n",
    "        Dictionary keys can be any combination of ['logw0', 'logSw4', 'logQ', 'logS1', 'logS2', 'logQh', 'f1', 'f2']\n",
    "        Each entry is a tuple of (median, std, percentile_01, percentile_99)\n",
    "    \"\"\"\n",
    "    priors = {}\n",
    "    \n",
    "    varnames = sho_trace.varnames\n",
    "    \n",
    "    # check for redundancies\n",
    "    if np.isin('logw0', varnames) & np.isin('logw0_x', varnames):\n",
    "        raise ValueError('Expected only one of logw0 or logw0_x')\n",
    "    if np.isin('logSw4', varnames) & np.isin('logSw4_x', varnames):\n",
    "        raise ValueError('Expected only one of logSw4 or logSw4_x')\n",
    "    if np.isin('logQ', varnames) & np.isin('logQ_x', varnames):\n",
    "        raise ValueError('Expected only one of logQ or logQ_x')\n",
    "    \n",
    "    # assign low-frequency term hyperparameters to dictionary\n",
    "    if np.isin('logw0', varnames):\n",
    "        priors['logw0'] = (np.median(sho_trace['logw0']), np.std(sho_trace['logw0']), \\\n",
    "                           np.percentile(sho_trace['logw0'], 1), np.percentile(sho_trace['logw0'], 99))\n",
    "    if np.isin('logSw4', varnames):\n",
    "        priors['logSw4'] = (np.median(sho_trace['logSw4']), np.std(sho_trace['logSw4']), \\\n",
    "                            np.percentile(sho_trace['logSw4'], 1), np.percentile(sho_trace['logSw4'], 99))\n",
    "    if np.isin('logQ', varnames):\n",
    "        priors['logQ'] = (np.median(sho_trace['logQ']), np.std(sho_trace['logQ']), \\\n",
    "                          np.percentile(sho_trace['logQ'], 1), np.percentile(sho_trace['logQ'], 99))\n",
    "\n",
    "    if np.isin('logw0_x', varnames):\n",
    "        priors['logw0'] = (np.median(sho_trace['logw0_x']), np.std(sho_trace['logw0_x']), \\\n",
    "                           np.percentile(sho_trace['logw0_x'], 1), np.percentile(sho_trace['logw0_x'], 99))\n",
    "    if np.isin('logSw4_x', varnames):\n",
    "        priors['logSw4'] = (np.median(sho_trace['logSw4_x']), np.std(sho_trace['logSw4_x']), \\\n",
    "                            np.percentile(sho_trace['logSw4_x'], 1), np.percentile(sho_trace['logSw4_x'], 99))\n",
    "    if np.isin('logQ_x', varnames):\n",
    "        priors['logQ'] = (np.median(sho_trace['logQ_x']), np.std(sho_trace['logQ_x']), \\\n",
    "                          np.percentile(sho_trace['logQ_x'], 1), np.percentile(sho_trace['logQ_x'], 99))\n",
    "            \n",
    "    # assign high-frequency term hyperparameters to dictionary\n",
    "    if np.isin('logS1', varnames):\n",
    "        priors['logS1'] = (np.median(sho_trace['logS1']), np.std(sho_trace['logS1']), \\\n",
    "                           np.percentile(sho_trace['logS1'], 1), np.percentile(sho_trace['logS1'], 99))\n",
    "    if np.isin('logS2', varnames):\n",
    "        priors['logS2'] = (np.median(sho_trace['logS2']), np.std(sho_trace['logS2']), \\\n",
    "                           np.percentile(sho_trace['logS2'], 1), np.percentile(sho_trace['logS2'], 99))\n",
    "    if np.isin('logQ1', varnames):\n",
    "        priors['logQh'] = (np.median(sho_trace['logQ1']), np.std(sho_trace['logQ1']), \\\n",
    "                           np.percentile(sho_trace['logQ1'], 1), np.percentile(sho_trace['logQ1'], 99))\n",
    "    \n",
    "    # add fixed frequencies to dictionary\n",
    "    if len(low_freqs) > 0:\n",
    "        priors['f0'] = (low_freqs[0], 0., 0., 0.)\n",
    "    \n",
    "    for i, f in enumerate(high_freqs):\n",
    "        priors['f'+str(i+1)] = (f, 0., 0., 0.)\n",
    "        \n",
    "     # fill in anything that is missing\n",
    "    if ~np.isin('logw0', list(priors.keys())):\n",
    "        if np.isin('f0', list(priors.keys())) & np.isin('logQ', list(priors.keys())):\n",
    "            w0 = convert_frequency(2*pi*priors['f0'][0], T.exp(priors['logQ'][0]))\n",
    "            priors['logw0'] = (np.log(float(w0.eval())), 0., 0., 0.)\n",
    "        else:\n",
    "            raise ValueError('Cannot determine logw0')\n",
    "        \n",
    "    if ~np.isin('logQ', list(priors.keys())):\n",
    "        priors['logQ'] = (np.log(1/np.sqrt(2)), 0., 0., 0.)\n",
    "    \n",
    "    \n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_stamps(time, data, tts, dur, dtype, stampsize=1.5):\n",
    "    \"\"\"\n",
    "    Cut out a stamp centered on each transit time from a full Kepler lightcurve\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array-like\n",
    "        time values at each cadence\n",
    "    data : array-like\n",
    "        corresponding data (e.g. flux, error, cadence number, etc.)\n",
    "    dtype : string\n",
    "        ndarray datatype to assign to output stamps\n",
    "    tts : array-like\n",
    "        list of transit times\n",
    "    dur : float\n",
    "        transit duration\n",
    "    stampsize : float\n",
    "        distance from each transit center to cut, in transit durations (default=1.5)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    stamps : list\n",
    "        list of stamps centered on each transit\n",
    "    \"\"\"\n",
    "    stamps = []\n",
    "\n",
    "    # cut out the stamps\n",
    "    for t0 in tts:\n",
    "        neartransit = np.abs(time - t0)/dur < stampsize\n",
    "        stamps.append(np.array(data[neartransit], dtype=dtype))\n",
    "        \n",
    "    return stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stamps(sc_stamps, lc_stamps):\n",
    "    '''\n",
    "    Combine short and long cadence stamps, using SC wherever available\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sc_stamps : list\n",
    "        list of short cadence stamps\n",
    "    lc_stamps: list\n",
    "        list of long cadence stamps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stamps_out : list\n",
    "        list of stamps\n",
    "    stamp_cadence: array-like\n",
    "        array of len(stamps_out) specifying cadence of each stamp as 'short', 'long', or 'none'\n",
    "    '''\n",
    "    # check lengths\n",
    "    if len(sc_stamps) != len(lc_stamps):\n",
    "        raise ValueError('inconsistent number of stamps')\n",
    "        \n",
    "    Nstamps = len(sc_stamps)\n",
    "    \n",
    "    # add stamps to list, prioritizing short cadence\n",
    "    stamps_out = []\n",
    "    stamp_cadence = []\n",
    "    for i in range(Nstamps):\n",
    "        if len(sc_stamps[i]) > 0:\n",
    "            stamps_out.append(sc_stamps[i])\n",
    "            stamp_cadence.append('short')\n",
    "        elif len(lc_stamps[i]) > 0:\n",
    "            stamps_out.append(lc_stamps[i])\n",
    "            stamp_cadence.append('long')\n",
    "        else:\n",
    "            stamps_out.append([])\n",
    "            stamp_cadence.append('none')\n",
    "            \n",
    "    stamp_cadence = np.array(stamp_cadence)\n",
    "\n",
    "    return stamps_out, stamp_cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_lombscargle_analysis(tts, ephemeris, period):\n",
    "    \"\"\"\n",
    "    Generates a Lomb-Scargle periodogram from a series of measured transit times and a linear ephemeris\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tts : array-like\n",
    "        vector of transit times\n",
    "    ephemeris : array-like\n",
    "        vector of times corresponding to the linear ephemeris\n",
    "    period: float\n",
    "        orbital period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    LS : dict\n",
    "        {freq , power, peak_freq, peak_fap}\n",
    "        \n",
    "    \"\"\"\n",
    "    # observed minus calculated\n",
    "    omc = tts-ephemeris\n",
    "    \n",
    "    # Hann window to reduce ringing\n",
    "    hann = sig.windows.hann(len(omc))\n",
    "    hann /= np.sum(hann)\n",
    "    \n",
    "    # identify any egregious outliers\n",
    "    local_trend = sig.medfilt(omc, kernel_size=7)\n",
    "    \n",
    "    out = np.zeros_like(tts)\n",
    "    out = np.abs(omc - local_trend)/astropy.stats.mad_std(omc) > 5.0\n",
    "    \n",
    "            \n",
    "    # compute a Lomb-Scargle periodogram; find 1st and 2nd peaks\n",
    "    lombscargle = astropy.stats.LombScargle(tts[~out], omc[~out]*hann[~out])\n",
    "    freq, power = lombscargle.autopower(minimum_frequency=2.0/(tts.max()-tts.min()), \\\n",
    "                                        maximum_frequency=0.25/period, \\\n",
    "                                        samples_per_peak=11)\n",
    "    \n",
    "    peak_freq  = freq[np.argmax(power)]\n",
    "    peak_fap   = lombscargle.false_alarm_probability(power.max(), method='bootstrap')\n",
    "\n",
    "    mask = np.abs(freq - peak_freq) < 11*np.median(freq[1:]-freq[:-1])\n",
    "    second_peak = freq[~mask][np.argmax(power[~mask])]\n",
    "    \n",
    "    ls = {}\n",
    "    ls['freq'] = freq\n",
    "    ls['power'] = power\n",
    "    ls['peak_freq'] = peak_freq\n",
    "    ls['peak_fap'] = peak_fap\n",
    "    ls['2nd_peak'] = second_peak\n",
    "        \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "import scipy.signal as sig\n",
    "from   scipy import stats\n",
    "import astropy\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from .constants import *\n",
    "\n",
    "__all__ = ['Planet']\n",
    "\n",
    "class Planet:\n",
    "    def __init__(self, epoch=None, period=None, depth=None, duration=None, index=None, tts=None, tts_err=None, quality=None, \\\n",
    "                 pttv=None, time_stamps=None, flux_stamps=None, error_stamps=None, mask_stamps=None, model_stamps=None, \\\n",
    "                 stamp_cadence=None, stamp_coverage=None, stamp_chisq=None, icov=None):\n",
    "\n",
    "\n",
    "        self.epoch            = epoch            # reference transit time in range (0, period)\n",
    "        self.period           = period           # orbital period\n",
    "        self.depth            = depth            # transit depth\n",
    "        self.duration         = duration         # transit duration\n",
    "\n",
    "        self.index            = index            # index of each transit in range (0,1600) -- Kepler baseline\n",
    "        self.tts              = tts              # all midtransit times in range (0,1600) -- Kepler baseline\n",
    "        self.tts_err          = tts_err          # corresponding 1-sigma error bars on transit times\n",
    "        self.quality          = quality          # boolean flag per transit; True=good\n",
    "\n",
    "        self.pttv             = pttv             # [Amp, Pttv, phi, C0, C1, C2, C3]\n",
    "\n",
    "        self.time_stamps      = time_stamps      # list of time stamps (one per transit) centered on midtransit\n",
    "        self.flux_stamps      = flux_stamps      # list of flux stamps\n",
    "        self.error_stamps     = error_stamps     # list of error stamps\n",
    "        self.mask_stamps      = mask_stamps      # list of mask stamps, (mask=1 where OTHER planets transit)\n",
    "        self.model_stamps     = model_stamps     # list of model stamps\n",
    "\n",
    "        self.stamp_cadence    = stamp_cadence    # 'short', 'long', or 'none'\n",
    "        self.stamp_coverage   = stamp_coverage   # fraction of transit/baseline covered by useable cadences\n",
    "        self.stamp_chisq      = stamp_chisq      # chi-sq per transit\n",
    "\n",
    "        self.icov             = icov             # inverse covariance matrix\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "        \n",
    "    def mask_overlapping_transits(self):\n",
    "        '''\n",
    "        Remove cadences from stamps where other planets transit\n",
    "\n",
    "        -- automatically updates time_, flux_, error_, mask_, and cadno_stamps\n",
    "        '''\n",
    "        for i, m in enumerate(self.mask_stamps):\n",
    "            if len(m) > 0:\n",
    "                self.time_stamps[i]  = self.time_stamps[i][~m]\n",
    "                self.flux_stamps[i]  = self.flux_stamps[i][~m]\n",
    "                self.error_stamps[i] = self.error_stamps[i][~m]\n",
    "                self.mask_stamps[i]  = self.mask_stamps[i][~m]\n",
    "                self.cadno_stamps[i] = self.cadno_stamps[i][~m]\n",
    "\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def clip_outlier_cadences(self, sigma=5.0, kernel_size=7):\n",
    "        '''\n",
    "        Do some iterative sigma rejection on each stamp\n",
    "\n",
    "        sigma: rejection threshold for clipping (default=5.0)\n",
    "        kernel_size: size of window for median filter (default=7)\n",
    "\n",
    "        -- automatically updates time_, flux_, error_, mask_, and cadno_stamps\n",
    "        '''\n",
    "        for i, f in enumerate(self.flux_stamps):\n",
    "            if len(f) > 0:\n",
    "                loop = True\n",
    "                while loop:\n",
    "                    smoothed = sig.medfilt(self.flux_stamps[i], kernel_size=kernel_size)\n",
    "                    outliers = np.abs(self.flux_stamps[i]-smoothed)/self.error_stamps[i] > sigma\n",
    "\n",
    "                    if np.sum(outliers) > 0:\n",
    "                        self.time_stamps[i]  = self.time_stamps[i][~outliers]\n",
    "                        self.flux_stamps[i]  = self.flux_stamps[i][~outliers]\n",
    "                        self.error_stamps[i] = self.error_stamps[i][~outliers]\n",
    "                        self.cadno_stamps[i] = self.cadno_stamps[i][~outliers]\n",
    "                    else:\n",
    "                        loop = False\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def flatten_stamps(self, jitter=0.1):\n",
    "        '''\n",
    "        Fit a linear polynomial to out-of-transit flux to flatten data flux stamps\n",
    "\n",
    "        jitter: fudge factor to avoid fitting in-transit flux if there are unresolved TTVs (default=0.1)\n",
    "\n",
    "        -- automatically updates flux_stamps on Planet object\n",
    "        '''\n",
    "        for i, flux in enumerate(self.flux_stamps):\n",
    "            if len(flux) > 0:\n",
    "                time = self.time_stamps[i]\n",
    "\n",
    "                intransit = np.abs(time-self.tts[i])/self.duration < 0.5+jitter\n",
    "\n",
    "                if np.sum(~intransit) > 0:\n",
    "                    coeffs = np.polyfit(time[~intransit],flux[~intransit],1)\n",
    "                    linfit = np.polyval(coeffs, time)\n",
    "                else:\n",
    "                    linfit = 1.0\n",
    "\n",
    "                self.flux_stamps[i] = flux/linfit\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_stamp_coverage(self, stampsize=1.5):\n",
    "        '''\n",
    "        Flag stamps with insufficient in-transit points\n",
    "\n",
    "        stampsize: distance from each transit center to consider, in transit durations (default=1.5)\n",
    "        '''\n",
    "        # determine locations of SC and LC data\n",
    "        sc_loc = self.stamp_cadence == 'short'\n",
    "        lc_loc = self.stamp_cadence == 'long'\n",
    "\n",
    "        # expected number of points in stamp if none are missing\n",
    "        expected_sc_pts = 2*stampsize*self.duration/(SCIT/3600/24)\n",
    "        expected_lc_pts = 2*stampsize*self.duration/(LCIT/60/24)\n",
    "\n",
    "        # count up points per stamp overall\n",
    "        pts_overall = []\n",
    "        for t in self.time_stamps:\n",
    "            pts_overall.append(len(t))\n",
    "        pts_overall = np.array(pts_overall)\n",
    "\n",
    "        # count up points per stamp in transit\n",
    "        pts_in_transit = []\n",
    "        for i, t0 in enumerate(self.tts):\n",
    "            pts_in_transit.append(np.sum(np.abs(self.time_stamps[i]-t0) < self.duration/2))\n",
    "        pts_in_transit = np.array(pts_in_transit)\n",
    "\n",
    "        # calculate cover fraction        \n",
    "        overall_fraction = np.zeros_like(self.tts)\n",
    "        overall_fraction[sc_loc] = pts_overall[sc_loc]/expected_sc_pts\n",
    "        overall_fraction[lc_loc] = pts_overall[lc_loc]/expected_lc_pts\n",
    "\n",
    "        in_transit_fraction = np.zeros_like(self.tts)\n",
    "        in_transit_fraction[sc_loc] = pts_in_transit[sc_loc]/(expected_sc_pts/2/stampsize)\n",
    "        in_transit_fraction[lc_loc] = pts_in_transit[lc_loc]/(expected_lc_pts/2/stampsize)\n",
    "\n",
    "        # use the smaller value as the coverage\n",
    "        self.stamp_coverage = np.minimum(overall_fraction, in_transit_fraction)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def calculate_stamp_chisq(self):\n",
    "        '''\n",
    "        Compare model_stamps, flux_stamps, and error_stamps to calcualte chisq for each transit\n",
    "        '''\n",
    "        mstamps = self.grab_stamps('model')\n",
    "        fstamps = self.grab_stamps('flux')\n",
    "        icov    = self.grab_icov()\n",
    "\n",
    "        stamp_chisq = []\n",
    "        j = 0\n",
    "        for i, good in enumerate(self.quality):\n",
    "            if good:\n",
    "                y = mstamps[j]-fstamps[j]\n",
    "                stamp_chisq.append(np.dot(y.T,np.dot(icov[j],y)))\n",
    "                j += 1\n",
    "            else:\n",
    "                stamp_chisq.append(np.inf)\n",
    "\n",
    "        self.stamp_chisq = np.array(stamp_chisq)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def identify_good_transits(self, cover_fraction=0.7, chisq_sigma=5.0, verbose=True):\n",
    "        '''\n",
    "        Identify transits with sufficient coverage and non-outlier chisq\n",
    "\n",
    "        cover_fraction: coverage threshold; eg. 0.7 will reject stamps with more than 70% of cadences missing (default=0.7)\n",
    "        chisq_sigma: sigma threshold to reject stamps as poorly fit (default=5.0)\n",
    "        verbose: boolean flag; 'True' to print results\n",
    "        '''\n",
    "        # determine locations of SC and LC data\n",
    "        sc_loc = self.stamp_cadence == 'short'\n",
    "        lc_loc = self.stamp_cadence == 'long'\n",
    "\n",
    "        # flag stamps with sufficient coverage\n",
    "        self.calculate_stamp_coverage()\n",
    "        enough_pts = self.stamp_coverage > cover_fraction\n",
    "        nonempty   = self.stamp_coverage > 0\n",
    "\n",
    "        # count up points per stamp\n",
    "        pts_per_stamp = []\n",
    "        for t in self.time_stamps:\n",
    "            pts_per_stamp.append(len(t))\n",
    "        pts_per_stamp = np.array(pts_per_stamp)\n",
    "\n",
    "        # flag stamps with unusually high chisq values (use pseudo-reduced-chisq)\n",
    "        reject_chisq = np.zeros_like(self.tts, dtype='bool')\n",
    "\n",
    "        if self.stamp_chisq is not None:\n",
    "            X2u = self.stamp_chisq[~np.isinf(self.stamp_chisq)] / (pts_per_stamp[~np.isinf(self.stamp_chisq)])\n",
    "            mad = astropy.stats.mad_std(X2u)\n",
    "            med = np.median(X2u)\n",
    "            reject_chisq[~np.isinf(self.stamp_chisq)] = np.abs(X2u-med)/mad > chisq_sigma\n",
    "            reject_chisq[np.isinf(self.stamp_chisq)]  = True\n",
    "\n",
    "        # print out results\n",
    "        if verbose:\n",
    "            print('%d out of %d transits rejected with high chisq' \\\n",
    "                  %(np.sum(reject_chisq[enough_pts]), np.sum(nonempty)))\n",
    "            print('%d out of %d transits rejected with insufficient coverage' \\\n",
    "                  %(np.sum(~enough_pts[nonempty]), np.sum(nonempty)))\n",
    "\n",
    "        # save the results\n",
    "        self.quality = enough_pts * ~reject_chisq\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def grab_stamps(self, stamptype, cadence='any'):\n",
    "        '''\n",
    "        stamptype: 'time', 'flux', 'error', 'mask', 'model', or 'cadno'\n",
    "        cadence: 'short', 'long', or 'any'\n",
    "        '''\n",
    "        if stamptype == 'time':  stamps = self.time_stamps\n",
    "        if stamptype == 'flux':  stamps = self.flux_stamps\n",
    "        if stamptype == 'error': stamps = self.error_stamps\n",
    "        if stamptype == 'mask':  stamps = self.mask_stamps\n",
    "        if stamptype == 'model': stamps = self.model_stamps\n",
    "        if stamptype == 'cadno': stamps = self.cadno_stamps\n",
    "      \n",
    "        if cadence == 'any':\n",
    "            use = self.quality * ~(self.stamp_cadence=='none')\n",
    "        elif cadence == 'short':\n",
    "            use = self.quality * (self.stamp_cadence=='short')\n",
    "        elif cadence == 'long':\n",
    "            use = self.quality * (self.stamp_cadence=='long')\n",
    "        else:\n",
    "            raise ValueError('cadence must be \"short\", \"long\", or \"any\"')\n",
    "\n",
    "        stamps_out = []\n",
    "        for i, s in enumerate(stamps):\n",
    "            if use[i]: stamps_out.append(s)\n",
    "\n",
    "        return stamps_out\n",
    "    \n",
    "    \n",
    "    \n",
    "    def grab_icov(self, cadence='any'):\n",
    "        '''\n",
    "        cadence: 'short', 'long', or 'any'\n",
    "        '''\n",
    "        if cadence == 'any':\n",
    "            use = self.quality * ~(self.stamp_cadence=='none')\n",
    "        elif cadence == 'short':\n",
    "            use = self.quality * (self.stamp_cadence=='short')\n",
    "        elif cadence == 'long':\n",
    "            use = self.quality * (self.stamp_cadence=='long')\n",
    "        else:\n",
    "            raise ValueError('cadence must be \"short\", \"long\", or \"any\"')\n",
    "\n",
    "        icov_out = []\n",
    "        for i, c in enumerate(self.icov):\n",
    "            if use[i]: icov_out.append(c)\n",
    "\n",
    "        return icov_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_filter(data, f0, fsamp, Q):\n",
    "    \"\"\"\n",
    "    Apply a 2nd-order notch filter (i.e. a narrow stopband filter) to a data array\n",
    "    See scipy.signal.iirnotch & scipy.signal.filtfilt for details of implementation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like\n",
    "        data to be filtered\n",
    "    f0 : float\n",
    "        center frequency of stopband\n",
    "    fsamp: float\n",
    "        sampling frequency, same units as f0\n",
    "    Q : float\n",
    "        quality factor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data_filtered: array-like\n",
    "        data array with selcted frequency filtered out\n",
    "    \"\"\"\n",
    "    w0 = f0/(fsamp/2)\n",
    "    \n",
    "    b, a = sig.iirnotch(w0, Q)\n",
    "    \n",
    "    data_filtered = sig.filtfilt(b, a, data)\n",
    "    \n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "\n",
    "def FFT_estimator(x, y, sigma=5.0):\n",
    "    \"\"\"\n",
    "    Identify significant frequencies in a (uniformly sampled) data series\n",
    "    Fits a Lorentzian around each peak\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        1D array of x data values; should be monotonically increasing\n",
    "    y : array-like\n",
    "        1D array of corresponding y data values, len(x)\n",
    "    sigma : float\n",
    "        sigma threshold for selecting significant frequencies (default=5.0)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xf : ndarray\n",
    "        1D array of frequency values\n",
    "    yf : ndarray\n",
    "        1D array of response values, len(xf)\n",
    "    freqs : ndarray\n",
    "        array of significant frequencies\n",
    "    \"\"\"\n",
    "    # min/max testable time deltas (conservative low-freq cutoff)\n",
    "    Tmin = 2*(x[1]-x[0])\n",
    "    Tmax = (x.max()-x.min())/4\n",
    "\n",
    "    N = len(x)//2\n",
    "\n",
    "    # FFT convolved with a hann windown (to reduce spectral leakage)\n",
    "    window = sig.hann(len(x))\n",
    "\n",
    "    xf = np.linspace(0, 1/Tmin, N)\n",
    "    yf = np.abs(fftpack.fft(window*y)[:N])\n",
    "    \n",
    "    yf -= np.median(yf)\n",
    "        \n",
    "    keep = xf > 1/Tmax\n",
    "    \n",
    "    xf = xf[keep]\n",
    "    yf = yf[keep]\n",
    "    \n",
    "    yf = boxcar_smooth(yf, 3, 1)\n",
    "    \n",
    "    # make a copy of raw xf and yf data\n",
    "    xf_all = xf.copy()\n",
    "    yf_all = yf.copy()\n",
    "\n",
    "    \n",
    "    # now search for significant frequencies\n",
    "    freqs = []\n",
    "\n",
    "    loop = True\n",
    "    while loop:\n",
    "        yf_noise = astropy.stats.mad_std(yf)\n",
    "        peakfreq = xf[np.argmax(yf)]\n",
    "        \n",
    "        if (yf[xf==peakfreq]/yf_noise > sigma) and (yf[xf==peakfreq] > 1/xf[xf==peakfreq]):\n",
    "            res_fxn = lambda theta, x, y: y - lorentzian(theta, x)\n",
    "            \n",
    "            theta_in = np.array([peakfreq, 1/Tmax, yf.max(), np.median(yf)])\n",
    "            theta_out, success = op.leastsq(res_fxn, theta_in, args=(xf, yf))\n",
    "\n",
    "            width = np.max(5*[theta_out[1], 3*(xf[1]-xf[0])])\n",
    "            mask = np.abs(xf-theta_out[0])/width < 1\n",
    "\n",
    "            yf[mask] = theta_out[3]\n",
    "\n",
    "            freqs.append(theta_out[0])\n",
    "\n",
    "        else:\n",
    "            loop = False\n",
    "\n",
    "        \n",
    "    freqs = np.array(freqs)    \n",
    "    \n",
    "    return xf_all, yf_all, freqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
